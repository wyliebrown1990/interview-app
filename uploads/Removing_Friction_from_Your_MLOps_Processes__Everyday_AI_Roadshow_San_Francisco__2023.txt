 Welcome to removing frictions from your ML ops processes. My name is Chris Helmus. I'm a senior solutions engineer here at DataIQ, which means I get to work with organizations, understand where they're at and the analytic maturity, and help them figure out where DataIQ can add value. And one of the things I love most about my job is how many different types of organizations I get to work with, you know, verticals from telecoms, to manufacturing, L.O.B.'s, from HR, marketing, finance. And one of the most common things I see across all of these different organizations is challenges with what we're going to talk about today, which is machine learning operations. And hopefully, since you're all at every TAI, you also agree that we can get a lot of value out of machine learning and analytics. You know, maybe you're at the beginning of your journey with your teams or maybe you're helping your organizations lead the charge. But regardless, you've probably realized that doing this right isn't so easy, especially when you want to scale to real value. And that's why I'm so passionate about MLOps because without a framework for the, you know, almost alchemy of turning models into value, it's easy to get things wrong. And we want to talk about today about how you can get things right. So what are we going to talk about? First, we're going to do some levels setting, right? A little more about what MLOps is, why it's important, and how it can be so challenging. Then we're going to pivot and talk about, you know, what are those key ingredients for success? What are the kind of universal frameworks and features that can support this? And then we're going to spend a little time on what I'm calling MLOps in action. You know, how do we take those universal frameworks, put them in the lens of a story through DataIQ and make them a little bit more real? So let's start with the challenges. And I like to kind of start with this central tension of that every or most orgs want to scale with AI, but it's not so easy, right? They struggle to keep control. So why do we want to scale, right? Well, we want better results, right? We want better customer experience. We want to improve employee productivity. We want to do things faster, accelerate faster. And fundamentally, I'm not a huge quote guy, but I do love this one from McKinsey. About how if you take ML, you'll work it into processes. You get more efficient and you get more revenue, which I think is the holy grail for us. So why don't we do it? Well, a couple of facts that I like to bring up. One, you know, only one out of 10 data scientists working days end up producing something valuable for their company. And when I talk to data scientists in the field, it can be because they're drowning in ad hoc requests or they spend all their time on maintenance, just a few reasons. And then 11% are able to get three quarters of more of their POCs into production, right? So even if you do make something valuable, actually getting it into a place where it can be consumed to make an impact, also really difficult. And finally, it can take around nine months to go from, you know, we have an idea, we have a prototype, let's get it into production. So hopefully you see these things and you realize starting to realize this idea of scaling and doing this right is not so easy. So let's peel back, you know, the onion, one more layer. And let's look at it through this lens of loops, right? Where does this go wrong? And the loop I'm talking about is this kind of classic analytic project lifecycle loop, right? Where you have to build your projects and you have to deploy them. And one of the first things I want to talk about is the fact that when I talk with organizations and we talk about MLOPS, a lot of times they're focused on the right side of this loop, right? How do we deploy things we've already built? What I want to work to convince you of today is that we need to look at the whole thing holistically to really get to scale. So let's do a little more level setting and let's start with the left side. We're talking about the bill. We're talking about business understanding of a problem, finding the right data, getting the right ingredients to your model, doing that experimenting and prototyping, selecting a model, and just that left side, right? Complex can iterate, can loop before you ever get to the right side, right? Actually getting your models into a place where they can provide value, where they're monitored. And then once you find problems, you go back to build and you build and you deploy and you build and you deploy. So this is complex enough, right? Just this idea of all the tooling we're using, how complex the analytics are. But what we believe at DataIQ and what we see is that there is something even more fundamental that a lot of orgs aren't taking into account. And it's the fact that it's not just one or two people that do this. There are a whole cast of characters involved in the analytics life cycle, right? Think data scientists who are struggling to find the right data, ML engineers who are struggling to validate retrained models, potentially in collaboration with the business, risk managers who are trying to apply the right compliance policies without limiting speed to value. There's quite a number of people. And this difference in skill sets in all these people, these are the gaps. These are where things start to fall through. And everything we've really talked about so far is in the context of one loop, right? One project. So what happens when you try to do more is the real question. And the kicker is that the more you try to scale, the worse it actually gets in our experience. So you know at first you start your first couple projects. It's just it's pretty hard, right? But you get them. You get them working. And your business stakeholders are smart. So they see the value. They want more. But what I see here many times is that our ops teams, they're focused on maintenance, they're focused on fixing errors. They're a little bit too slow to be able to deliver. And then even when frameworks get built, they're usually too specific. They don't involve that whole cast of characters. They don't focus on democratization. So what it really comes down to is we need a new approach, right? A fundamentally new approach so we can regain control and actually get the scale. So hopefully I've convinced you that it's not so easy, right? It's something we need to do. What I want to start talking about now is kind of the bright path to how we actually do solve these things. And we believe at DataIQ that it starts with something comprehensive and collaborative where we can actually involve all those people, all those personas and bring them into the right part of the life cycle. So being able to have people involved in the preparation, involved in the experimentation and the prototyping, building and selecting of those models, making sure that we get safe sign-off, get models into production, get them consumed, and then wrap that whole thing up, make it continuous, make sure all those personas at the bottom are involved. So if we want to get to this, right, this new approach, there really are three fundamental things, if we try to roll things up that help us get here. We need to make MLOps loops that are unified, operationalized and repeatable. So what we're going to do now is we're going to dive a little deeper into each of these, you know, crucial ingredients, getting you a little context of what I've been hearing in the field relating to these, and then actually go over some of the more detailed features and functionality that support these pillars. So building that unified loop. So out in the field, things that you may be experiencing these are things like, hey, how do we make sure these handoffs in our projects aren't so time consuming? How do we deal with all that tooling complexity? Right, our domain experts may be using Table of Power BI Excel, business stakeholders wanting results in PowerPoint, you know, engineers using all kinds of things, maybe airflow to orchestrate. And then also, you know, if we're going to invest the time to build a unified loop, how do we make sure we don't just throw away what we've already done, not just machine learning models, but also, you know, logical models, data pipelines. And so to kind of address those, this unified loop really is that crucial first step. All right, I think we've talked about analytics projects need that constant refinement and improvement. And in order to account for that, you have to make sure you can unify both steps and people. And the goal overall with the unified loop is to streamline things enough that it's easy and natural to follow the loop. Right, so starting with that end to end environment, we have to make sure that all the pieces of the puzzle that need to be done here can be in this unified environment. So think data discovery all the way through model monitoring. And ideally, we do this in a way that makes the handoffs easier. So think if I'm a data scientist and I'm modeling the way I want to experimenting, I want to have everything I do seamlessly tracked that makes it easy for me to pick the best model, but also makes it easy when it's time to package up the project and move to the engineer for production. Next, collaboration, as I'm sure you're starting to pick up, really is the key here. Right, if we can, we have expertise all over the organization. We need to make sure we can leverage that expertise from the right people together. So making sure we can take our domain experts who know the data, involve them in the future engineering, picking the right data for your project, in the same place where our modeling expertise experts or data scientists can experiment in prototype, in the same place that our engineers who have both the data and the production pipeline expertise can make those pipelines visible. And if we do streamline the steps and the people, well, the reality is that orgs are changing all the time. New systems, new tech, we need to make sure we can manage and monitor models that are built across the enterprise. And ideally, we also need to make sure we future proof things, being able to make it easy to access the data and compute we have today and what we're going to have tomorrow. So let's say we get this unified loop down. Well, what's next? We need to make it easy to make that loop robust, tracked, automated. Right, so not only so we save time, but also so we can build confidence in that process, in that loop. And I hear a lot on this on things like how do we avoid recoding when we move to production, which is inherently manual. I hear a lot about, you know, we're good at data quality, but we're not so good at AI quality. How do we make sure we can apply the same guardrails, the hooks and the triggers we do for our current data processes? And then finally, a lot about automation, really crucial, but I think the lens I want people to see automation through is as much automation as makes sense for your use case and as much human the loop makes sense for your use case. So when we're thinking about operationalizing, right, it's taking that unified loop, trying to make sure we can minimize errors, inconsistencies, delays, get to this, you know, fundamental control at speed idea, make sure things don't fall through the cracks. So the first thing we're talking about, you want to try to make what you design as close as you can to what you operationalize. You know, think a process map of what you're working on that makes it visible and easy to understand, but also lets you control which pieces get automated when, you know, being able to make how you collect ground truth, separate from when you kick off your retrains. And then if we're going to automate more and more, right, it can be sort of a blessing in a curse if you don't do it right. If you don't have guardrails to build confidence into that automation, things can go haywire. So thinking things like standard templatize built-in AI quality checks, but also synops from other stakeholders so you can build confidence across the org that is being done right. And then if you have guardrails, that's great, but if they're not visible, that helps no one, right. So centralizing the types of monitoring and alerting you have. So the really where you want to get is an ML engineer, right, can get an AUC model drift alert in the same way that a business person can get a revenue decline alert. So if we build our Unified Loop, has all the steps, has all the people, make sure that it's robust, that's all great. We've put in a lot of work. So now we need to talk about how we can actually repeat this loop in really achieved scale. And when we get to this piece, this is a little bit more about the bigger picture, right. How do we make sure that with these loops we've built, we can actually enforce compliance. How can we see across all of our projects in models. And also something really crucial is how do we make sure people can reuse trusted pieces of the puzzle. So something that I really commonly hear is we have lots of smart people doing really smart stuff in our organization, but because we don't have reuse, as people start to build projects, they end up becoming the only ones that can maintain those projects. And then when you get to, let's say, three or four projects, well, your ability to do new ones kind of drop off a cliff, which really hurts, obviously, the ability to scale. So what do we need to be able to repeat the loop? Well, fundamentally, you want to make sure that similar projects can follow similar workflows, right, have a blueprint. And I think about this in the sense of for projects that have similar risk profiles, how do we make sure similar stakeholders get involved in the sign-off process? Or when we're moving a project from let's say a prototyping validation to investing more resources, what are the common things we have to have checked off along the way? And this middle one, if we kind of drill down from the high level projects into a use case, making sure that we can re leverage components is so important because think about an analytic project. You got a lot of artifacts. You have data, you have data pipelines, you have model train code, the models themselves, how you're orchestrating, all of the interpretability metrics you hopefully are have around the process. If you don't tag those things and have them cataloged, they're not going to be discoverable. And even more important, when they're discovered, those users aren't going to be able to find the context to know how they should be used safely. So think a use case marketing data scientist wants to work on a natural language processing project related to customer reviews. In this scenario, they can easily search, find something that let's say finance data scientist has created, understand how it was used, really easily leverage that template and also have someone to reach out to if they need help. And then finally, we need to make sure we can see things across the initiatives. So here we're talking about how not just model registries, right, but project registries. So let's think about a risk manager being able to log in, see their full portfolio of 25 projects, see where each one is at in the life cycle and know exactly where they need to invest some time. All right, so unified, operationalized, repeatable. What I want to do now right before we get into actually MLOPS and action, the story is give you a quick customer story that brings a lot of this together. So this story has to do with a large US electronics manufacturing company. And they had a really common problem that lots of manufacturers have, which is how do we find anomalies before they cause problems in the manufacturing lines. And the solution they ended up working on with data IQ was let's set up this reliable pipeline that takes video from the manufacturing floor, processes it, scores it against a live API model endpoint, gets those results back to the manufacturing line so action can be taken. Right, and if you then wrap that up and health checks and monitoring so that anytime something starts to drift, someone can be brought in, they can validate retrain the model right back into production to line keeps humming. And fundamentally, this did save the customer a ton of money, which is great. But the other thing I want to highlight is this was built in a way that let them be able to work to deploy this not in just one factory line, but dozens. So when you think about those pieces, you know, unified, right, the full analytics life cycle from grabbing the data, doing the ground truth collection feedback loop all the way through, operationalized, automated alerts and an easy path to production when a model needs to be retrained. And then repeatable, right, built in a way so that we don't have to redo this for every use case. Okay, so we've talked a lot about these universal frameworks. And I think they're really important, they can be a little abstract for the first time you're hearing them. So what I wanted to do is show a little story through the lens update IQ. So actually having the platform up and kind of talking us through a use case related to something I think we can all relate to, which is credit card fraud. So think, you know, you go swipe your card, right, that hits a live model endpoint. And you either get to client maybe if they think it's fraud or maybe you get that text message that asks if it's really you. So we're going to be investigating that story where that model is already in production, providing value. And we're going to tell the story through three different personas, right, a business stakeholder domain expert persona and ML engineer and a risk manager. So all right, let's dive in. We're going to start with that business stakeholder. And I want you to notice is this, this user is getting alerts right where they're used to getting them in Slack, right. And it's not some model drift alert. It's a fraud missed alert, where we can see green check marks in the past for things we're looking good. And then we got more in alert related to the amount of fraud being missed below. So to give that user some context. Now when the business stakeholder wants to find out more, well, they get a one quick click link into data. I could they have a dashboard explaining the context linked to that credit card fraud project where they can understand, you know, who they need to reach out to from the data expert side, how those fraud mismetrics have been trending over time, right, and kind of where the threshold isn't where it was not met this time. And in this case, that gives our stakeholder the ability to contact their ML engineer, tell them they want to move this up the priority list. Now fortunately, this ML engineer already has their own automated alerts and their own dashboard where they're seeing things like a UC data drift precision, how that's been trending downwards over time as ground truth has been collected and making it really easy to compare to the performance from that model was initially trained. Now that ML engineer can also head to data, whose model evaluation store, where everything has these ground truth pipelines are happening, happening, everything is collected and tracked automatically. And importantly, these metrics are standardized across their use cases, making life a lot easier. Now if that ML engineer wants to do some extra validation, they can click and drill in and get information on all types of drift. In this case, we're going to take a look at input data drift to see the difference between what we did at train time for our model and what's coming in now with our newest transactions with customers. And if we look field by field, we can actually see in this case that card age has really drifted. We have a bunch of new customers, which could be driving the performance issues in our model. So our ML engineer now is looking at the save model repository, seeing that version two that says active, that's what was already in production. Version three that was automatically that reach train was kicked off by that AI quality alert is sitting right below, making it super easy to dive in and start doing some validation. Now data I could give this engineer a lot of standardized tools to do this, things like what if analysis, individual explanations to start exploring the boundaries of the model and its performance. But it also provides some of those guardrails, right? These metrics that were built with the business so that this ML engineer can feel confident that this model is valid not just from, you know, an AUC perspective from a business perspective. Data I could also even layers in some out of the box diagnostics. So our ML engineer can feel good about things like overfit leakage, making things standardized quick, really easy to scale. So engineer is feeling good about the validation. And that version three of the model is now sitting in a lower environment on the left. And we can see our production version two over on the right. The employer is where our ML engineer can do various checks hitting that API endpoint, making sure the new one is functioning as expected. But if our ML engineer decides to try to move that version three into that higher environment into production, we are going to hit a governance blocker here. This is then going to trigger in alert to the third character in our story, that risk manager. Now our risk manager typically lives here in data I could govern, right? This centralized watch tower that helps us oversee not just models, but our analytics projects across the enterprise. So our risk manager here is able to see a whole lot of context around things like sponsors of the project, what business initiative it's part of, its description, even things around the scope and the risk level of our project. And if you look over on the left side, we're able to see in our blueprint where our project lives, even get things like auto-generated model documentation to get some understanding of what's going on. Now we're able to come here and look at our version three of the models, see in this models blueprint, we're actually sitting at the review stage. And we can see this require sign-on from IT and operations, risk, business, each of those stakeholders can come into here review, lead feedback, and give their approval or not. If we get the right approval, automatic kick of that model into our higher environment, getting things into production. Now this use case, pretty risky customer facing, we had a lot of human in the loop in this demonstration, but also just want to note that every piece of the puzzle here can be fully, absolutely automated through data-accus scenarios, doing things like building the train data, doing the AI quality checks, kicking off those retrains, making sure no validations are missing, getting the API service out into production. So I know a little bit of a whirlwind, trying to fit in a lot in these 30 minutes, but hopefully this starts to make a little bit of this multi-persona collaborative approach to ML ops real within data-accus. And really again, this is what we're trying to get to. We're trying to get to a fundamentally new approach to ML ops, where first you have all of the right stakeholders involved at the right pieces of the analytics lifecycle, right, that unified loop that has everything we need. We want to make sure that loop is operationalized, right, so accelerated automation, simplified path to deployment, and then we really want things to be able to repeat, right, we have to get that one-and-done approach out of the way and get to something that's more standardized, so we can really scale. And when we look at this, you know, nice flowing loop here, it's always good to keep in mind, you know, why do we care about this? Why are we going to invest so much effort in trying to get this new approach to ML ops going? And it's because we don't just want to add value for mayi machine learning analytics. We want to be able to multiply the impact across our organization while making sure we regain control. Thank you.