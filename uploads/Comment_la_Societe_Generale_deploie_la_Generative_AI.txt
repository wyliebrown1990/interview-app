 ... Bonjour à tous. Je serai vite intervenir auprès de vous aujourd'hui. Ce matin, Cristina Poisson, la TVO du groupe Claire Calme-Jane, la chief innovation officer du groupe vous a parlé de la démocratisation de la donnée. Je vous propose maintenant de vous parler du sujet dont vous le parle cette année. Depuis un an, depuis l'arrivée de Chad Gipiti, c'est la généritive AI. Et donc, comment nous, à la société générale, on explore ce sujet. Peut-être avant de parler de généritive AI, je voudrais vous rappeler qu'on faisait de l'AI avant la généritive AI. Et donc, en fait, c'est un long chemin. À la société générale, on a commencé à déployer des cas d'usage de machine learning en production il y a un petit peu moins de 10 ans. Et puis, progressivement, on a industrialisé et on était la première banque revaine, je crois, à piloter nos initiatives de machine learning et DIA par la valeur. Donc, on est passé d'un stock de cas d'usage à une valeur attendue de cas d'usage. Aujourd'hui, notre ambition c'est d'atteindre 500 millions d'euros de valeur généré par nos cas d'usage, il y a data à horizon 2026. Et donc, notre stratégie business, c'est d'intégrer l'AI au coeur de la transformation de nos différents métiers. Et donc, dans notre portefeuille de cas d'usage, on a des cas qui vont traiter plutôt du risque opérationnel et de l'efficacité, des cas plutôt défensifs et également des cas qui vont traiter de la méruration de la relation cliente et également de la génération du nouveau business, jusque là, je pense qu'on a tous à peu près les mêmes cas d'usage. Comme exemple que je peux vous donner plus spécifiquement sur la banque, ça peut être des initiatives pour identifier des cas de fraude sur les paiements en temps réel, grâce à du machine learning et identifier des patterns ou des émissions obligataires d'arriver à ajuster les différents paramètres pour optimiser ces émissions, là aussi en se basant sur les données du passé. Et en fait, par rapport à ce que je viens de vous dire, la généale, c'est plus une nouvelle étape pour aller chercher plus de valeur plus qu'en réalité, un shift technologique. Alors, ce que je voulais dire par rapport à la généale, il y a plusieurs marqueurs importants. Je pense que vous les avez en tête, mais c'est malgré tout bien de les préciser. Le premier, c'est que ce sont des modèles qui sont plus ou moins à la plupart déjà entraînés et déjà utilisables. Deuxième chose, ça traite des données qui sont non structurées, des documents. Vous savez que la banque, elle s'appuie, c'est une industrie qui est par un nature dématérialisé. Elle s'appuie sur beaucoup de données et des documents. On faisait des choses avant la généale, mais la généale nous permet d'aller à un niveau plus loin et d'accélérer sur le traitement de cette donnée non structurée. On la tous vécu avec la GPD, l'expérience utilisateur, elle est redoutable. Le côté conversational fait qu'on adopte la technologie. Et puis, il y a le côté générique. On peut couvrir des sujets quand on a envie de faire une synthèse de documents. Le modèle fonctionne pour tout un... différentes natures de documents. Donc, c'est impressionnant. Et donc, utile. Dernière chose, à vous mentionner, nous, on considère que le généal va apporter beaucoup sur l'aspect efficacité opérationnelle. La valeur, on attendre, elle se situe plutôt au niveau de l'efficacité opérationnelle. La question qu'on peut se poser, c'est la part de valeur réellement mobilisable sur cette efficacité opérationnelle. Donc, peut-être maintenant, là, je vous ai dressé le décor YAH, j'ai parlé des marqueurs de généailles. Et peut-être détailler un peu les types de cas d'usage qu'on voit, il y a cette technologie. Je le synthétise en 4C. Le premier C, c'est la considion. C'est-à-dire les experts virtuels des agents qui sont capables d'accompagner le collaborateur et pour faire des travaux d'experts. Elle est cherchée des informations dans un document, dans un corps plus de documents, faire des synthèses. Voilà. Deuxième famille de cas, la génération de contenu. Donc, la génération de production de contenu en général, du marketing, ça peut être intéressant, des contrôts en due de réunion. Troisième C, le client, l'engagement client. La conversation, elle, d'avoir des interactions. Et d'ailleurs, ça peut être le client externe. Mais ça peut aussi être nos collaborateurs. Et donc, interagir, apporter des réponses à des questions qui pourraient poser. Et le 4C, c'est le code. Le code informatique. La génération de code. La génération de documentation. Également, la traduction de code. Et donc, le corollaire de ces 4 catégories, c'est aussi, en fait, les fonctions dans l'entreprise, qui vont être transformées avec cette technologie. Donc, assez naturellement, le code, on comprend bien que c'est toute la population de software ingénieur, de développeurs au global. Le conversationnel, les agents, les employés qui sont en relation avec les clients, la génération de contenu, le marketing, l'avent, on sent qu'il y a un impact. Et puis, pour les experts virtuels, les directs, ce sont toutes les directions centrales de la banque, risques, conformités juridiques, tout cet ensemble de fonctions. Maintenant, je vais aborder avec vous comment, au niveau de la valeur business, des usages, comment ce structure à la société générale pour amener en production la technologie. La première chose qui est fondamental, c'est l'aculture, on en a beaucoup parlé aujourd'hui, mais cette faire toucher du doigt à l'ensemble des staff, y compris les comités exécutives, le potentiel de la technologie, le potentiel est également les limites de la technologie dans un environnement bancaire, donc très régulé. Mais faire en sorte que les employés se projettent dans ce que peut la rapporter, cette technologie, l'idée, cette faire émaner des cas d'usages, aujourd'hui au niveau de la banque sur les quatre catégories que j'ai vocées, on en a identifié plus d'une centaine. Et après, par un jeu, après, on fait une priorisation des cas d'usages qui existent depuis déjà des années dans la banque et qui a assez traditionnel, on essaie d'identifier la valeur qu'on va tirer des cas d'usages et de se projeter à l'échelle, non pas uniquement pour une entité mais également sur un potentiel global. Donc on va identifier la valeur, identifier aussi les freins en termes de faisabilité. Qu'est-ce qui aujourd'hui nous empêche d'avancer ? Est-ce qu'on a les bonnes données ? Est-ce que ces données sont de qualité ? On va aussi bien, et là c'est peut-être une spécificité supplémentaire par rapport à notre cadre traditionnel, c'est on va sur ces aspects qui sont assez nouveaux, cette technologie elle est nouvelle. Donc, tous les sujets liés au risque, la responsabilité, la transparence, les risques spécifiques induits par la GNI, risques juridiques, propriétés intellectuels, les hallucinations, on va intégrer, on va prendre en compte ces éléments et puis par un jeu de tamis finalement, on va prendre notre centaine de cas d'usage et puis on va arriver sur 40 cas d'usage et sur ces 40 cas d'usage. En fait, on va se dire pour chaque catégorie que je vous ai présenté tout à l'heure, qu'elle est le cas d'usage qui est le plus intéressant à développer dans un premier temps parce qu'on est prêt, parce que le cas il est vraiment représentatif d'un cluster beaucoup plus global. Donc, on a identifié pour chaque une des catégories, ce qu'on va appeler un ou plusieurs cas précurseurs, on en a un peu moins d'une dizaine et ce sont ces cas-là, avec lesquels on va commencer à travailler pour tester la capacité de la technologie à répondre aux besoins dans une optique en fait de créer ce que nous, on appelle un bloup printe, c'est à dire une capacité qui permette d'accélérer et d'être mise à disposition des différentes entités qui auraient un cas relativement similar. Voilà pour la partie, voilà pour nos stratégies et si ça vous convient, je vous propose de basculer et d'entrer plus dans le détail sur les aspects techniques liés à ces capacités LLM que l'on développe à la société générale. Merci beaucoup. On est, j'imagine que beaucoup d'entre vous ont pleins de questions à poser et pour l'occasion, nous avons le plaisir d'accrire Sébastien Brasser qui travaillait aussi sur ces sujets à la société générale et Keurth Muemel qui lui lide les sujets génératifs à laïs à la société générale. Donc, bienvenue à tous les deux. Merci. Merci Athienne d' nous avoir exposé la stratégie, la toute ses réflexions de la société générale sur le généré des VI. Maintenant, je devrais me dire à Never Sébastien, c'est très bien que vous êtes en côté, c'est très pratique. Je voudrais en tant que Distinguished Engineer chez la société générale, est-ce que tu peux nous parler un peu de la dimension de technologie qu'associer avec ces sujets, ses priorités, et surtout la question de la mise à disposition des outils pour que le réalisation de tous ces cas d'usage, et surtout est-ce qu'il y a des principes d'architecture que vous avez pu tirer de cette expérimentation ? Absolument. Absolument, en cœur. Je suis de mon cœur, donc, déjà merci pour l'amitation. En plus, moi, j'adore le l'ouvre, donc c'est le cas de faire d'une pierre de coups très clairement. Alors, je dirais que le premier point qui est super important, c'est de bien faire la dissociation entre les cas d'usage, la plateforme sous-jacente, ce que évidemment, quand on dit « Il y a génératif », tout le monde pense, chage et pété, tout le monde pense, c'est nervio-conversationnel, mais comment on a vu justement à Kétienne, en vérité, ça représente une partie seulement des cas d'usage qui peuvent exploiter « Il y a génératif » et « la la la la la ». On vraiment, bien dissocié, ce cas d'usage spécifique et la construction d'une plateforme sous-jacente, il doit servir des cas d'usage beaucoup plus large que les simples cas d'usage conversationnel. Deuxièmement, cette plateforme, faut qu'elle soit escalable, qu'elle sert les cas d'usage de manière que soit costé fichante, et surtout, faut qu'elle soit mise à disposition des équipes de datatient en tissu, développeurs métiers, qui, croyez-moi, ils ont pas envie de consoir ces plateformes, ils ont pas envie de l'opérer, parce qu'on a réalité un impoversement de manière transecque, de valeur métier en tant que tel, et donc, c'est important de venir centraliser ce type de capacité et de le faire aussi avec les meilleures solutions, les meilleures enaybles, heureux technologiques du marché, qui soient open source, ou qui soient commerciaux et épeillants. Donc, ça, c'est le deuxième principe, la centralisation. Le troisième, c'est qu'on est dans un univers qui évolue très vite. Il y a des nouveaux LLM qui sortent toutes les semaines, six nées tous les jours, si je reprends juste la dernière semaine. On a vu les nouveaux modèles, GPT4, annoncé par OpenAI. On a vu anthropique, qui est encore sécurisé, je crois, 2 milliards ou 3 milliards auprès de Google. Hier, il y a Satiana Dela, de Microsoft, qui a annoncé un nouveau modèle open source de Microsoft. Donc, on voit que c'est très effervescent et qu'on va devoir, on va dire, avoir une architecture qui est suffisamment décoplie, près de capables d'intégrer des nouveaux modèles aux fils de l'eau, et pour venir brancher des pranchées sans que ce soit trop painful à opérer dans le temps. Et justement, sur le côté un peu painful, les comments choisit de quelle LLM vient brancher des pranchées. C'est là où la télémetrie applicative est déterminante. Puisque, en mesure, ma l'usage qui fait justement des différents modèles sur la plateforme, nous derrière, en tant que fournissant de cette plateforme, où on est capable, va déterminer, quel modèle faut encore du sens. Pour être qu'à d'usage métier, lesquels modèles ne font plus de sens, c'est lesquels on peut dévancher. Et dernier point, mais non démoindre. Évidemment, il y a beaucoup d'innovations dans la LLM. Il y a quand même des standards technologiques qui émergent, je ne saurais qu'il n'y a pas trop de doute. Je pense notamment à l'enchaîne, liemain indès, que ce certain pattern d'architecture, on va faire du rétrivoi le moment de titre génération. Donc ça, ça va être directement implementable et consommable dans la plateforme. Et dernier point qui est très lié aux métiers régulés et donc bancaires, c'est qu'on doit être responsable, by design, puisque l'arrivilation va nous imposer de fournir des notions de transparence de modèle, d'explicabilité des prômes, de traceabilité, d'autitabilité. Et ça, c'est pas forcément fonctionnel, mais c'est un moins cliché à prendre en compte dans la plateforme. Très bien. Donc clairement, vous avez certaines nombres de réussites dans le domaine. J'imagine que dans cette salle, il y a un certain nombre de personnes qui voudraient faire comme vous. Est-ce qu'il y a des écueils à éviter ou des enseignements que vous pouvez partager avec la salle ? Bien sûr, il y a toujours des écueils. C'est jamais simple de commencer sur des nouvelles technologies. Le premier entret, nous dirais, c'est bien prévoir son capacité planning. Parce qu'il y a évidemment qu'on ouvre le service pour la première fois et il va avoir beaucoup monde qui va se connecter. Donc là, c'est important d'avoir fait ces stress tests, ton annon, d'avoir du stroke-ling sur les appels que vous mettez à disposition. Ça, c'est absolument cliché. Il ne fait pas que la plateforme, mais c'est cool au bout de cinq minutes. Le deuxième point, c'est plutôt dans le ingénierie, en tant que tel. C'est ce que j'appelle l'ARC du 70-2010. Puisqu'on est en z'un monde à la fois qui évolue très vite dans le premier technologique, mais aussi beaucoup d'attentes des métiers. On peut encore en gros, ce que je recommande. C'est passé 70% du temps de vos équipes d'ingénierie. Sur vraiment, l'ingénierie de la plateforme et son évolution incrementale. Je dis à Ruffie de l'eau. 20% sont de la recherche et du développement. Donc plutôt des sujets qui sont un peu en avance de phase, qui sont pas tout à fait secs. Mais sur lequel vous voyez qu'il y a du potentiel que ça peut marcher et répond à d'aderver et problématique. Et enfin, 10% sur les sujets qui sont à beaucoup plus long terme que vous allez pas mettre en prode avant de t'être deux ans, trois ans. Donc par exemple, la médadada du moment, c'est tout ce qui est AI constitutionnel et Confidential Computing dans le domaine de intelligence artificielle générative. C'est une question. Il y a un point qui est aussi intéressant. C'est toujours ce dire quelles plus intéressants. Je vais parler des modèles entre une approche où je fais moi-même. Je vais me mécaire de modèles de fondation par exemple. Je vais prendre des modèles de fondation que je vais chéper, donc adapté ou alors je vais m'appuyer sur des entreprises, des expertises qui proposent et maintiennent des modèles au sein même de leur solution. Donc en fait, cette approche-là, il faut toujours se dire laquelle est la plus pertinente au regard de ce que, d'un point de vue bizzase, je veux développer. C'est toujours une histoire de compromis quand on fait l'architecture qu'il arrive. Avant d'un dernier point, il ne faut pas non plus surestimer. Je dirais, la capacité des utilisateurs à faire du pont un engineering la plupart découvernt, donc faut les accompagner dans ce contexte-là. Donc ça passe bien évidemment par l'autofornation, de la mise à disposition, de cours à cette fois-ci plutôt en présentiel et collectif. Mais j'ai réfléchi également à l'inclusion directement dans la plateforme et pour pas non expensueux des utilisateurs, de peut-être d'un coach, d'un advisor, de clé de capables de cotier justement un utilisateur pour les rendre meilleur. Donc c'était à faire des prontes au quotidien. Et ça vous donne même pour les développeurs, datatient en liste. Donc comme je dis précédemment, on va faire des architectures qui intègrent de l'illien générative. Ça ne coule pas de source. C'est quand il y a des nouvelles compétences, des nouveaux frameworks que va apprendre. Il faut vraiment travailler sur les enablements, justement des cognités de développeurs et datatient en liste. En plus de fournir l'infrastructure et les plateformes qui permettent de développer les cadusages. Très clair. Merci. Et aujourd'hui, comment collaborerez-vous avec datacou dans ce domaine? Moi là, je vois très clairement le rôle de datat IQ. Puisqu'on est bien évidemment déjà utilisateur et éclient d'atat IQ. C'est la capacité de faire le lLLMMesh. Vous allez avoir, finalement, à venir connecter les environnements de travail d'atat IQ de nos utilisateurs existants, de nos business analyses, d'atatient en liste. Vraiment, cette plateforme d'illegénérative, qu'on est en train de construire. Donc ça sert vraiment le premier point. Et le deuxième, là où je vous attend, c'est justement un capacité aussi à cotier directement dans datat IQ. Les business analyses et datatient en liste pour qu'ils fassent du bon trompe. Et qu'ils ne fassent pas des escalades, justement dans mon équipe, ça m'édera l'avenir. Très bien. On est toujours à l'écoute par des telles suggestions. Donc dans le promen de studios, vous avez déjà un petit parti de ça avec tes tempêtes. Mais effectivement, il n'y a jamais une fin de notre développement. Donc avant de terminer, j'aurais eu nous orienter un peu plus vers l'avenir. En vous en tronnant sur, en gros, les prochaines étapes et les jalon-clés pour la société générale. J'aurais aussi entendu le perspective à la fois de datatienne, mais aussi de toi, Sebastian. Ouais, bien sûr. Un point de vue métier, c'est d'arriver à déployer sur la base des cas précursors que j'ai vécu tout à l'heure d'arriver à déployer la technologie à l'échelle. Pour ça, c'est avancer groupé et s'appuyer sur l'expertise. Les expertises qu'on a au sein du groupe, l'ensemble des expertises, avec aussi les retours d'expérience qu'on peut faire. Donc, à arriver à accélérer. Évidemment, il y a un aspect de projection aussi de l'impact de ces technologies à moins à rien et long terme. Donc, à arriver à mettre finalement sa longue vue et à imaginer ce que sera la banque avec cette technologie qui sera déployée partout. Ouais, d'un point de vue métier, c'est les grands enjeux. Très bien. Très clair. Et Sebastian, sur un point de vue technologique. Point de vue technologique, point de challenge qui est personnel, c'est arrivé à toujours lire les papiers de recherche sur les génératifs parce qu'on se rend tous les jours et il s'est beaucoup trop inquiétés. C'est très actif. Je sais pas quand ils font pour tenir la charge, mais c'est bravo eux. C'est le point de challenge. Bon, le deuxième, évidemment, c'est la scalbité, la montée en charge, de notre platform au fur et à mesure. Avec elle, elle va passer de 4, 8, 12, 20 etc. 4 usages. Et puis le troisième, c'est plutôt un challenge de fond. Et il va se dire que ça va être, comme je disais, continuer les efforts d'aculturation et de scaling des développeurs d'adacentistes qui vont venir devoir consommer ces modèles génatifs à travers notre platform. Bon, c'est super. Je tiens à vous remercier tous les deux. Ce perspective, à le fois métier et aussi technologique de la Geneux d'AVI, chez Société de Génarelle. Merci beaucoup. Merci, coeur. Merci, coeur. Merci.