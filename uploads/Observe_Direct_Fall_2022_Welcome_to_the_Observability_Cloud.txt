 The focus of today's talk is quite simply the observability cloud. For the very first time, observe is bringing together all the capabilities you've been asking for into one single integrated product. Everything from log analytics to monitoring to dashboarding, distributed tracing and much, much more. We also build observe on top of a modern cloud native architecture, which delivers an order of magnitude improvement in economics. We believe that there's never been a better time to introduce the observability cloud. Recent research shows that 58% of businesses moving to the cloud have lost visibility into their operations. At the same time, they're seeing a 40% increase in data volume, pushing costs through the roof. Let's start today with our unique approach. Like anyone else, we ingest all event data into a data lake. Then also, unlike anyone else, we curate that event data into a data graph. Finally, we democratize access through data apps, enabling a broad range of users to get up and running in minutes. Let's drill into the data lake. Quite simply, we ingest anything and everything that looks like an event. That's log, metrics, traces for sure, but also zendex tickets, salesforce customer interactions, and Jenkins build information. And we don't use proprietary agents to collect your data. We rely 100% on open source. The data lake is based on AWS S3 and we compress data 10 X on average, making it insanely cheap. Next, we make the magic happen. We curate the event data into a graph of connected data sets. No one has anything like this. Simply put, data sets are things that users want to ask questions about that could be containers or S3 buckets, but it could also be customers or shopping carts. And then observe relates the data sets. This means that during an investigation, users can immediately navigate to related context. This is critical for investigating unknown issues, a common occurrence in today's world of microservice based apps and continuous delivery. Because data sets represent real things, we can track how the state of those things are changing over time. This is important because modern systems are ephemeral. They're constantly changing. And so it's essential to be able to reconstitute the state of the system at any point in time. Finally, let's talk about data apps, which promise to democratize access to event data. Everyone from customer success to support, to DevOps and SRE, to engineering should have access. Observe provides apps for the most common infrastructure components, such as Kubernetes, AWS, GCP, and Azure. And observe enables you to easily create equivalent apps to observe your own distributed applications. What's in an app? The short answer is everything you need to observe your distributed application, that means dashboards alert, search, and all of your data sets. Many of these components are available in competitive offerings, but in observe, they're much, much better. For example, almost everyone has dashboards. They pull together metrics to provide an overview of the health of an application. Observe can do that too. But because observe has better data, we deliver better dashboards. Observe dashboards are driven by a data set graph, which means that when we see a spike, we can drill down and retain context as we do it. Maybe we're only showing the customers that are experienced an error in the last hour. And if we suspect it's a problem with the infrastructure, we can drill into only see the health of the pods that emitted the error. And we can keep drilling to show maybe only the container logs emitted by those pods. Similarly, pretty much everyone has alerts. They fire when, for example, errors are detected in container logs. And observe has got alerts too. But because observe has better data, we're able to deliver better alerts. Instead of just showing a threshold condition has been exceeded, you can now show the impact of the blast radius of perhaps which customers have been affected. Observe can do this because observe alerts are based on the data set graph. Pretty much everyone has an ability to search events. This example, you've probably seen a hundred times before, diving it in millions of container logs to look for errors. Observe can do that too. But because observe has better data, we're able to deliver better search. We give container log search results, but we also give you relevant contextual information, such as the pod names. And for more information about those pods, like the restart count, you're just to click away. Observe can do this because search is based on the data set graph, and it contains all the relevant contextual information. Finally, the observability cloud is based on a modern architecture. Observe separates storage and compute, which changes the economics of observability. For ingest, you simply pay for AWS S3, and don't forget that the data is compressed 10x. And you also pay for the compute you consume when you query observe. Computers are elastic, so you only pay when you're using the system, and it's built to the near a second. Architecture matters. Now, only does observe scale, the more it scales, the more you save. In our benchmarking, observe is 10x lower cost than leading competitors at scale. This example is from a typical host monitoring environment, but this one is from a Kubernetes logging environment. As you can see, the shape of the chart is exactly the same. At scale, observe has an order of magnitude cost advantage. What we've seen and what businesses really care about is how price scales with data volume. Most incumbent offerings scale linearly. That means that as their data volume increases 10x, the price increases 10x. Incumbent offerings have a legacy architecture, and if you want proof of that, just look at the price list. We believe that the observability cloud is going to improve observability for you by a factor of three. Imagine lowering MTTR3X. But we're also going to give you a 10x improvement in cost depending on your use case. That's the observability cloud, and it's available today. Hello and welcome. My name is Kelsey and I work on our sales engineering team here at Observe. And today, I'm going to walk through a demo of the observability cloud. Data is ingested into Observe using open source collectors and does not require certain schemas or structures, reducing operational overhead. Here, you can see high level stats of the data you have stored. There's about 70 or so terabytes in data in total, which includes various log data, permethiast metrics, hotel traces, and even contextual data like GitHub and Jenkins events. Of the data that is an Observe, about 30% is accelerated. Unlike other tools, you can access all your raw data at any time. However, acceleration is the process of curating data into data sets to facilitate rapid optimized queries. Let's take a look at Observe's data set graph, where you can see all the data sets built on top of the data light and the connections between them. At a high level, you can start to see the different out of the box apps we have deployed. For example, Kubernetes and AWS. If you want an observable system that enables you to reduce troubleshooting time and increase efficiency, then you need to be able to go from point A to point B without having to know the route. I.e., you don't want your teams to have to have tribal knowledge to do investigations. Similar to a GPS. This is what Observe does. If I click on Pod, you can see all the related data sets and all related traces that went through a set of pods. Or if I want to understand the Jenkins builds related to the pods that are restarted. No matter what questions you're trying to ask, graph link or these relationships between these out of the box data sets enables you to start anywhere and navigate to various layers of your tech stack quickly and seamlessly without needing tribal knowledge. Let's show this in practice. Let's use the Kubernetes out of the box app as an example. Opening the app will drop us into the out of the box, Kubernetes dashboard. This is an overall view of the health of your cluster. You are able to see high-level stats, like cluster size and some other key metrics. If I scroll down here, I can see that there's some unhealthy pods. Unlike a lot of other dashboards and other tools, Observes dashboards are actionable. Meaning you can use Graph link to investigate related data sets. Let's drill down into the related pods. Now you can see all the metrics for those unhealthy pods and the outlier metrics like high CPU and memory become more apparent. Over on the top and the activity tab, by default, you can see notifications which are out of the box alerts that have fired to specific teams via different channels like Slack or PagerDuty. But let's take this farther. Since metrics only tell us part of the story, often you want to take a look at other data like related logs. You can quickly pull up other related data, not just what is provided on the dashboard. Let's look at container logs. Notice that we didn't have to filter to the same time frame or the pods we were looking at at the dashboard view. It automatically keeps that context as you look at related data. Furthermore, I could seamlessly navigate to all the traces that went through these pods. By using Graph on this dashboard, I see I have traces sorted by response time. And there are some slow ones in here. So let's pick one and take a look. On the waterfall, I can see we started this trace in NGINX, then called down to a microservice container in Kubernetes, which made some elastic cache and SQL calls, which then invoked a Lambda. Oh, we can see the problem here. That Lambda has made lots and lots of SQL calls. Pretty quickly, we have come to the conclusion that the car rating Lambda, that is the problem. We have some sort of SQL call loop. But what is that SQL call? Let's open one of these up. Here we have the span attributes, and I can see specific SQLs that are being called. Further down this page, I get the tabular view of all the spans, and then below that, all the logs. We have a across this environment for this specific trace. Thank you. Since our launch, we've had customers using Observe to store, analyze, and alert on their open telemetry data. The feedback we received from these early adopters is a dicomore prescriptive out of the box solution for troubleshooting the microservice applications instrument with open telemetry. Today, I'm happy to announce the new Observe app for open telemetry, which does just that. We're firm believers in open telemetry's vision of commodity vendor neutral data collection. As a result, getting open telemetry data into Observe is a breeze. Observe has an endpoint for ingesting OTOP data, so all you have to do is configure open telemetry collector to export data at it. If you're using our Kubernetes integration, we handle all of the setup for you. Once this data is in, you can use our out-of-the-box dashboards to understand the performance of all of the services your application depends on, and even drill into individual traces. With our new waterfall visualization, it's easy to diagnose problematic traces by seeing which spans per slow or had an error. The best part is that we treat tracing data like any other data to observe, which means that you can use our schema and demand and grappling capabilities to refine and link your open telemetry data to any other log metric or resource data in Observe. So what this mainstream aloevox experience is that your open telemetry data is linked to the Kubernetes AWS and host monitoring apps in Observe, so you can easily correlate application issues with infrastructure issues. Now, we ourselves use open telemetry to analyze the performance of Observe, and we're proud to now share this capability with you. We've been doubling down on providing out-of-bots use cases for customers with the Observe apps. First off, we made a bunch of the improvement to our existing apps. We expanded AWS app to support Amazon CloudWatch synthetics. This allows you to monitor your endpoints and API, so you can continually verify your customer experience even if you don't have any customer traffic on your applications. We also made it easier to set up host monitoring apps with the auto installation script. This eliminates the needs to manually install and configure three separate agents. I'd like to also highlight a few of our new apps. We introduce a GCP app. It comes with auto bots dataset dashboard and monitor for five popular GCP services, compute engine, cloud load balancing, cloud storage, class SQL, and cloud functions. Support for BigQuery and GKE are coming soon. If you're already using Prometheus Node Explorer for host monitoring, you can now integrate with Observe in minutes. All you have to do is install the Node Explorer apps and configure Prometheus to include Observe as a destination. You will get dashboard for CPU, memory, and network disk out-of-box. We're committed to making it easy to observe your entire stack. That's why we're continuing to invest in CI-CD and code-level integrations with Jenkins app and GitHub app. This will help our customer quickly identify which PR calls a service audit or long response time, reducing the meantime to recover dramatically. But wait, there's more, introducing the Open Telemetry app, a complete solution for tracing. It provides a prescriptive ingest guidance, visualizations, and dataset for tracing use case. Beyond these apps, we are working on Azure, GitLab, and many more in the future. Stay tuned. Since releasing our dashboarding and metric expression builder features in May, we've been hard at work getting feedback from our users and making improvements. Let's review a few of the major ones. First off, the metric expression builder is now available in worksheets and dashboards. That means users can visualize complex, multi-metric expressions without using query language. We've also added support for parameters in metric expressions. So users can easily make their dashboards configurable. Power users will be happy to know that the Opel for their query is always a click away in the Script tab. On the dashboarding front, we've taken a look at the things that users struggle with when creating dashboards. Textboxes and drop downs are great to put on dashboard when you know exactly the sort of filter a user might want. But sometimes you just can't know that ahead of time. To solve this, we've introduced a new type of parameter that we call a filtered dataset. The filtered dataset parameter makes every field in a dataset available for filtering. It supports many different kinds of filtering, like inclusion and exclusion, wild cards, numeric ranges, and more. This leverages all of the powerful context that observe extracts from your data. Lastly, we made a big improvement to our filter bar by adding first class JSON support. Previously, users had to extract JSON fields before filtering on them. With this latest improvement, users can access and filter any JSON field on the fly. This also extends to filtering and grouping in the metric expression builder. We're really looking forward to seeing the stories our users can now tell with these exciting new features. Thank you. One of the common concerns with usage-based pricing is how to keep spend within a desired budget. At Observe, we are obsessed with giving our customers full control of their tooling costs. I'm excited to tell you about three new features that are going to help with the same. First up, usage governor is a new feature that can help customers stay within a configurable budget. It works by tracking your average credit usage in a rolling time window. It will warn users when usage hits 80% of the configured budget or reject queries when usage exceeds the limit. The admin has the capability to define budgetary limits for both transforms and query to stay within their budget. Moving on to the next feature, Acceleration Consent, which is designed for when a user queries outside of the accelerator range of the data set. Observe will now ask users for their consent before kicking off an on-demand backwall. This will make sure user is aware and they're able to use their budget where they needed their most. Next up is our usage dashboard to which we've made a few improvements. Users can now drill down to understand their credit usage by data set by monitor, by package and by user. The dashboard will also have an overview of usage governor's behavior. This will include things like credits consumed against the configured limit analyst of bypass queries. We are confident that these features will empower our customer to be in the dry receipt and take advantage of a true usage based pricing model. I'm Tony Noons. I'm the North American and South American support director for Project 44. Project 44 is a logistics king offering a great software to compile all your data and make end-to-end visibility for your shipments. And observability is a huge part of that for us. So to take something like observability and being able to dive into data, dive into every piece of data that we have is really what our business is about. We wanted to make things simpler, clearer, easier, friendlier. Tracking shipments, I know it seems really simple from the front end. You look on the website and you find out where your bathroom is or your Wonder Woman cake pan. But you're ultimately stuck with all this data that's happening behind the scenes. Project 44 is an organization. Compiles that data normalizes it for customers, but for us looking at it and support, it's not normalizing customized. We have to kind of do that on our own. One of the biggest benefits that we see is turnaround time. And resolve time is everything in support. Being able to find access to the pin pointed data that you need to understand is this problem with us. Is it with a carrier? Is it with the API? Is there a communication problem? To be able to locate that quickly is the difference between a happy customer and a sad customer. One of the new rollouts of the dashboards that that observed put together really helps staff starting from square one. We're able to build dashboards that say, here are the pieces you're going to need to know about this shipment. Here's our API log, here's your server request, here's your client request, here's all the data that you need to assess the shipment for the customer. The possibilities for observer are really endless, and it's what your imagination can picture with that data.