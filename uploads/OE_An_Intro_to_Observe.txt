 Welcome to Observe EnableIt. Today's video will be an introduction to Observe for anyone brand new to the product or anyone looking to get a refresher on the fundamental concepts to observe. To introduce Observe today, we'll be going over three core topics that cover the majority of Observe at a high level. Those three topics are data ingests, data models, and data apps, which data apps are really how we put our data to use, and that includes dashboards, monitors, and data explorers. Let's start with data ingests. And our philosophy at Observe is that data ingests should be kept easy and simple, leaving the complexity to modeling and observe and leaving our source systems in their configuration as simple as possible. We'll talk more about storage considerations in respect to data ingests in a moment. Effectively, anything that can be sent via a network request can probably be ingested by Observe. This includes plain text, JSON, CSV data, or anything compressed in a zip file can be sent using our collection endpoint. To put data ingests in perspective, we can appreciate that there really are only four different methods of data ingests that apply to all the different sources that exist out there. And those four methods are at a high level, cloud services, collector agents, web hooks, and polar and cloud functions. Now, to go a little bit more detail on each of these different methods of ingests, to give some perspective on how you might ingest data from one of your sources, we'll look at a couple examples of each. Beginning with cloud services, this includes the main cloud providers like AWS, Azure, or GCP. And so with AWS, the most common example is a Kinesis fire hose that can be configured to send to the Observe collection URL. However, if your infrastructure is hosted on Microsoft Azure Cloud, then the more common example for you would be in Azure Event Hub. As you can see a few examples here, which are also configured to collect data from internal Azure services and send them on to observe. The next method of ingest is collector agents. Observe supports the open source collector community, and the three most prominently used at Observe are Fluentbit, Telegraph, and the Grafana agent. You can see the Fluentbit agent here, which is mainly used for collecting various logs. The Telegraph agent is used for collecting various metrics, making use of open source plugins for different sources of metrics. And then last but not least, the Grafana agent is commonly used because of Prometheus being such a common way of hosting and providing metrics in different environments. These common cloud services and collector agents comprise the majority of different sources that need to ingest data, especially the core infrastructure and application data. However, there are other sources of data like SAS providers, databases, and other APIs, which have data you would like to ingest to observe. However, they require a different means of extracting that data. Webhooks are a way of using the platform itself to export data automatically to a configured endpoint. And in this case, it would be observed. We can look to source control providers like GitHub, which provides Webhooks that can be created on their platform, configured to send certain events, which include merge requests, GitHub action events, and more, and then send that automatically to Observe. Another common example is with ticketing systems like JIRA. JIRA, of course, has a webhook mechanism that can new events, incidents, and tickets to observe automatically. And then last but not least, we have Polars and Functions, which are our last line of defense when trying to extract data from sources that don't support a nice interface for exporting that data. And so Polars can be thought of as simpler means of just using a Git request to extract data, often times without authentication needed. But if we do require secrets that should be stored and owned by the customer, or maybe we require additional logic to interface with the database or even an API, then we would implement a cloud function that could be run in AWS or Azure, periodically, and then extracts the data and sends it to Observe. A common example is using AWS Lambda to host a function like the one you see here called Observe Collection, which extracts data from S3 buckets and then sends it to Observe. And now we've appreciated the four different ways at a high level, at least, of ingesting data to observe, and so this should give you ideas on how you could ingest data from your different sources. Now, last but not least, we want to go over some ingest considerations. The two main ones that come to mind is storage concerns and also performance concerns with the load of ingest. However, you can imagine I'm going to say that there was really nothing to be concerned about, because within Observe, one of our core tenets of our architecture is separating compute from storage. And thus, you can store massive amounts of data for very cheap, and then only have to pay for the data you actually end up using in terms of queries. And so you can see in our Observe documentation, our concept of a data lake, where it provides a place to store all of your data, you can see that it's compressed ten times your data once stored, and then it's also stored ultimately in Amazon S3 for 13 months by default. Resulting in an inexpensive long-term storage. You can also see in further links, like blogs about using our storage in Snowflake, to get even more details on how economical it is to store massive amounts of data in Observe. And then going back to the other consideration in terms of performance, Observe has customers who ingest data at 100 terabytes per day, in addition to the many other customers, also ingesting terabytes of data per day. We absolutely boast supporting a scale of petabytes. And so now that we've talked about data ingest, let's start getting into what we can actually do with that data, and this all has to do with modeling data, because as it's ingested, it'll be at first in its raw form, and not as usable as we would like it to be. Before getting into modeling data itself, we should take a moment to appreciate the connection between ingesting data and data landing in Observe, and where data lands in Observe at first is a data stream. Data streams in Observe are the entry point for any and all data, before it gets modeled into any other forms. However, once the data has been made available in the data stream, we can filter it and model it into a subsequent data set. A data set is an important vocabulary term for Observe. It is a curated view of your data. We'll see in a moment what data sets look like in Observe and how that progresses from data streams. We'll also see how to create a data set at a very high level, but we'll leave it to later enablement videos to go into deeper detail. To create a data set, it requires data modeling, taking the raw data and ultimately applying filters and transformation, to produce some result. And this produces an additional data set that is a view, downstream from our data stream, and thus inherently creates a data pipeline. But in Observe terminology, we call this data lineage. Here you can see a snapshot of what data lineage actually looks like in Observe. The Kubernetes data stream accepts data, raw data from all the various different Kubernetes clusters, but from there we'll filter it out into specifically just API updates. And then one step further of all those different API updates, we can filter it down and model out API updates specific to deployments. And in this way, we've modemmed our data from its raw form, created two additional data sets, and also a data lineage. So now let's go through a very quick demo to appreciate how we can go from ingest to data streams, and then into data sets, and appreciate this data lineage. We can go back to our curl example that we showed earlier, and was used to send some basic data into an Observe instance. And so doing exactly this, I can jump from my terminal in which is the source that sent this data, and then go into Observe and see how it looks. Welcome to the Observe platform. This may be your first time seeing it. And it's so you may want to look into some additional Observe enablement content to help navigate the overall platform. However, for now we're going to stay focused on the data streams to appreciate ingest and basic data modeling. So here we can see we're looking at the list of data streams, which we can get to from the left navigation bar. And these are the logical entry points for data being ingested to Observe. We can create these that will and segment different sources of data ingest. And so I have my data stream here that I've created, and I can go in and see the data set for this data stream. Here we can see that this is a data set. And of course, going from a data stream to a data set is a very close connection because every data stream by default gets a data set created with it. And this data set has a default set of columns or fields to help encapsulate the data that wasn't just. So here I can see the data in its raw form. I can double click on each cell to expand it further, and I can also interface with this data. So I can begin manipulating and modeling this data right from the interface itself just by clicking on a field and then selecting extract. Now I can have this field available in my data set, changing its schema, but this also then gives me the ability to filter based on this field, something I couldn't have done before. In addition, if I want to look through metadata and filter to a specific value, I can also do that. And so now I've applied a filter and an extraction to my data. Now I might want to change the view of the columns by looking down here in the table controls. A somewhat hidden but very important feature and observe for helping visualize data exactly as you want. Here I can hide many different fields. I could also rearrange them if I'd like. This is also the opportunity where I can change the view of my data. And also how much data is being displayed in the current query window. Of course, there's not a lot of data in my present example, but now I can see only the data I really care about. And this represents a new view of my data. As we mentioned, a data set is a curated view of your data. And so that's exactly what I would do is I would publish this as its own data set. To do so, I wouldn't need to go in and create a worksheet. Worksheets are a temporary scratch pad within observe that expose you to all of the power of modeling data. And so already you can see glimpses of the opal, the underlying processing language that drives these data manipulations. But worksheets will be covered in more detail in the intro to modeling data. That enablement video should be linked to this one. For now, we're going to skip over this because we already used the UI to apply our filter and to extract our data. We can maybe do one more step here and instead of just hiding these fields, we could actually delete them. By right clicking, I can delete these columns. And then right here from the right menu, you can see publish new data set. And I'll call this my view big set. So if we navigate to our new data set that we just created, we can see the view that we had just modeled out based off of the data stream data set. And now we have this new data set that we can reference without having to recreate those same filters and data transformations. Now last but not least, we can go into this related tab here. And this is going to help us appreciate how this data set relates to others. Namely the lineage tab here can be used to appreciate the data lineage that we mentioned earlier. Now we have our data stream data set with the raw data flowing into our new data set with the new modifications. So this concludes a very brief introduction into data models and tying together the pieces of ingest, data streams and data sets. Now let's move on to data apps because so far we've been able to get data ingested into observe and then get some basic modeling to make that data more useful. But ultimately our goal is to be able to create these data models so that we can create dashboards monitors and be able to investigate that data further. And so to do that very quickly and easily within observe we have the concept of observe apps. Observe apps are a predefined collection of resources to help ingest the data but then also observe that data within observe. And these observed apps correlate to the observed integrations you'll find in our public documentation. Each of these comes with the following predefined dashboards monitors and then the data can be viewed through the log explorer and the metric explorer. And so these four mechanisms here are the main ways of using your data in observe. The one other one worth mentioning which we got a very brief glimpse of is worksheets. Of course we're going to cover that in more detail in future enablement videos. But for now we'll focus in on dashboards and monitors, the log explorer and metric explorer and how they relate to the content and data ingested through observe apps. So starting with dashboards and monitors those are going to be tightly coupled to your observe integration or app. Let's go back into observe and appreciate observe apps and how they relate to dashboards monitors. Now back into the observe platform you can see right underneath data streams the apps 10 and this displays all of the apps installed in your environment. You can see a link to install more apps you can configure apps or you can view the content related to this. And so just as I mentioned an app or an integration like in this case for AWS comes with predefined data sets to model all of the raw data coming in for you. And you can see with the extent of AWS services and different types of data that could be ingested there is a large number of different data sets. We'll take a look at how we'll actually want to find data by using the log explorer here at just a moment. But for reference you can see these are all of the underlying data sets which you could use to look at specific views of data. In addition we have monitors which underneath the templates tab for each individual app in this case the AWS app will provide template monitors that you can enable to cater to various different AWS monitoring use cases. And then last but not least we have dashboards from which you can see under the AWS package are various dashboards predefined to help monitor and observe the different AWS data being ingested. And this gives a clear idea of what is actually generated when an app is installed and observed. However to conclude today's introduction we can't leave without mentioning the log explorer and the metric explorer which will be your two main vehicles for investigating data from an integration or any other custom source. So we'll go into these next we'll go back into the observed platform. And then we can find the log explorer right under the investigate section in this logs tab. There will certainly be additional enabling content to help go into all of the different features offered by the log explorer and also the metric explorer for today's video will give a very brief introduction but will leave it to those subsequent videos for more detail. So here in the log explorer we can see the actual logs themselves which by clicking on different cells we can interact with. Up above we have a filter bar which can be used to filter these logs or we could use the filter bar here to the left menu. And then if I want to switch between different sources of logs here looking at the app logs I can just click on a different logs data set and then quickly switch between I can also use this option here to visualize this data on the fly. Very similar to this view is the metric explorer which in a similar way allows me to select a metric I could search for a CPU metric and maybe I only want to look at CPU metrics related to my EC2 instances. And so now I can quickly find that metric and then work with it here in the metric visualize here I can use the filter bar to narrow my results. I can use this bar here to configure and change how my data is being aggregated or grouped by. And then lastly I can configure different visualization options here. And that concludes our introduction to observe today we went over data ingest all the different sources and ways we can ingest data to observe. And then we went over the basics of modeling data and observe to appreciate how we go from a raw ingested form into something that can be made more usable. And the way of making use of that data is through data apps which provide us dashboards monitors and the metric and log explorer to investigate our data. Many of the features and concepts you saw today will be described in greater detail and subsequent observe enablement videos. Please refer to those for more information. Thank you.