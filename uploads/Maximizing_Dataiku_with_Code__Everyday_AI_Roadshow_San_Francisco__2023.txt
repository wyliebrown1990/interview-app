 So a little bit about me, I have a degree in computer science. And actually in college, I was taking a class on, I think it was like machine learning databases, something or rather, I had to write a ton of Spark code to do some k-means clustering and it took me ages. And then I interviewed with a actually alternative platform to data IQ for data science, got familiar with their platform and discovered that whoa, data science platforms exist and can automate all that code that I was writing, my life is over. And then I was working for that organization and it was kind of a reasonably proprietary platform where everybody that I was talking to wanted to bring the latest and greatest algorithms, packages, customize their entire pipeline. And it was a need that I felt like I was struggling to meet because it wasn't flexible enough. And so I am so excited to be here presenting about maximizing data IQ with code today because I think it gets to the heart of what we've heard all this morning where data science platforms can, your speed, your time to value, increase trust, decrease risk, and democratize data to entire organizations, which can really, you know, bring so much value to a team. But of course, and I think if you're sitting through this talk, hopefully I have plenty of coders in the room, there is always going to be a need for customization. And that's what I'm going to talk through today. So thank you, Chris, for the super concrete like walkthrough of a real kind of use case with ML ops and showing what a platform can do and how much can be automated so that you don't have to build it all of yourself. And what I'm going to be talking about is I will be doing some live demos. What I'm going to be talking about is all of the different options for how programmers can customize your data IQ environment and take the skills that you have and deploy those into reusable components and drive the value of your skills even further beyond your immediate data science team. So the questions that I'm hoping to answer, the goal of my presentation is to really give you a whole bunch of ideas so that you can be inspired to go back and think about all the different ways you can use your programming skills or your team's programming skills to maximize data IQ in countless different ways. All right, so the first question is where can I write code in data IQ? So if you're familiar with data IQ, you're probably familiar with all of the code recipes that we have in Python, R, SQL, Spark recipes. You're also probably familiar with the Jupyter notebooks. But what I'm hoping to do today is expose you to the countless other places that you can write code. So let's first take a quick tour of the platform. Let me make sure this is big enough. Okay, so here's that Jupyter notebook that if you've seen data IQ before, you're probably you've probably seen this before. And I just want to highlight a couple of the extra things that data IQ has added on top of the notebook. So the first is code samples. This gets back to that kind of reusability of best practices and making sure that collaboration on your team is as strong as possible. So if you if your team, you know, starts to develop code snippets that you think can be reused and reapplied to other use cases, you can create your own code samples. We have a bunch that come out of the box. They're all searchable and taggable so that other people on your team can easily copy those and plug them into your notebook. Now, the next question and part of the reason why a platform makes everything so helpful is let's say I've you know done some analysis in my notebook. I've built some visualizations. And now my question is, okay, how am I going to expose some of these insights to other people? You know, maybe they don't want to view a notebook with all of this code in it. So the first thing is if you know, I think people who are used to writing notebooks, you can write markdown, you can write all these descriptions, which is super helpful. And so you can publish your notebooks directly to dashboards within data IQ and then that can be shared linked to stakeholders. But we also as part of the data IQ API have something called saved insights. So I can take any of the visualizations, whether it be a map plot lib type of visualizations, plotly visualizations, and just with a single line of code, I can save those as figures, which then I can directly publish into my dashboard as kind of standalone figures. If the visualization package supports interactivity, that can be embedded within the dashboard as well. So that's just one a couple of kind of the extensions of the notebook environment on top of data IQ. And the other piece that I want to share is let's say your notebook is maybe a bit less exploratory oriented and instead you're iterating on a feature engineering script or you're iterating on a model training pipeline. And you're using the Jupyter interface because it's super interactive. And you can view the outputs, view the results and kind of move through that iterative process. But then the question becomes how do I want to operationalize this code that I was writing, whether it be you know feature engineering, psychic learn pipeline, something like that. So directly from the Jupyter notebook interface, you can transform the notebook into a Python recipe where it will be embedded within the flow as a script that can execute as part of a broader data IQ pipeline where we can then view the inputs, the outputs, the connections to data sources, whether it be snowflake databases, cloud storage, other data systems is abstracted away through the data IQ API. So that all I have to do is register the data, register the handle to the data set. And it basically allows you know a programmer who maybe is used to just working on Jupyter notebooks on their laptop to then you know still the same environment, still the same interface, make a really easy transition into deploying their pipelines into a production nisable flow within the data IQ platform. Now that's a workflow that I think a lot of people see in our demos all the time. You're probably reasonably familiar with it. But other places that you can write data IQ that you can write code in data IQ that you might not be familiar with are in those automation scenarios that Chris had showed where we have a whole bunch of out of the block, out of the box building blocks to define exactly how pipelines are going to get built metrics and checks on the models and data sets within your pipelines that you may want to monitor, automatically retraining models or creating API services. Those are all tons of the building blocks that we support without having to write code. But let's say you have some sort of complicated logic that you know is going to require some you know if then else statements or maybe a loop or even potentially calling out to an external API in order to determine exactly how the pipeline is going to get built. Well, we have a custom Python step in the automation scenarios that allows you to write whatever kind of logic you want so that you can make those automation pipelines as flexible as possible. Another example is on the data preparation visual recipes. So once again, we have a whole bunch of out of the box building blocks so that you can really move as fast as possible. But like I said at the beginning, you know, pointing click isn't always going to cover all of the bases. So as a programmer, you want that flexibility, you want to be able to customize it. Same type of thing. You can embed Python functions directly in the script of a data IQ visual preparation recipe as well. So that's just a couple of the kind of less common ways that I'm hoping to inspire you to take advantage of with the programming skills you have or your team members have. Okay, so as programmers, another question that frequently gets asked is, can I build code in my favorite IDE? And the answer is a resounding yes. So you'll see three main ways that you can leverage a familiar IDE like VS code, sublime, pie charm, whatever your IDE of choices. So the first is these data IQ APIs, the way to connect to data IQ data sets, manage data IQ assets like models, those APIs can be installed in any environment. So if you have a, you know, a fantastic local environment that you've really like procured for yourself, all you have to do is install the data IQ APIs and then you can continue to, you know, use data IQ as this back end platform, but within the local environment that you've built for yourself. Now the next piece is one of our newest features and that is called code studios and it's basically IDE's that have been embedded within the data IQ platform. So let me pull up my other project that has my code studio. Okay, here we go. It's still not started. Okay, we'll wait for that to start. We'll come back to it. But then the next piece is we also have integrations with local IDE's as well. So here I have my local VS code environment and what you'll see on the right side is the data IQ extension, which basically allows me to browse out to any of the projects on my data IQ environment, select any of those code recipes that then I might want to use within my IDE for, you know, if I want to use a debugger or have all the nice like syntax management that IDE's provide. And then if I execute any of these recipes, they continue to execute on the data IQ back end and I can just read the logs directly within the IDE. So for people who are frustrated with working with like the recipe interface directly in the flow, all you have to do is install the data IQ plugin for VS code or one of the other IDE's. And then you can use that familiar experience, but still leveraging those code recipes in the back end compute of data IQ. Now let's go ahead and take a look. Oh, that'll take a moment to start, but I have the screenshot here. So it's the embedded IDE of code studios is exactly that. It is a VS code, our studio embedded within the actual data IQ platforms that you don't have to go back and forth. And then the interaction with all of the assets like your recipes, your project libraries is very much the same as the local IDE integration. But instead it's just one click away from the data IQ platform. Okay, so speaking of project libraries, another question that frequently comes up is how can I integrate existing code bases into my platform? Or let's say I develop a really complex code base that then I want to wrap up in a modular reusable framework that, you know, maybe not all data scientists take advantage of, but it might be something that a traditional analyst who is in a programmer wants to use as well. And this is where project libraries and plugins come into play. So if we look at this screenshot here, you'll see in the in your data IQ project, you can directly import any custom library from Git that then is made available to all of your Python recipes, all of your Jupyter notebooks. And so if you've developed all of this modular code, you're tracking it in a Git repository. And now you want to leverage it inside of data IQ, all you have to do is pull it in from the Git repo. And it's immediately available for you to take advantage of in those recipes and those notebooks. Now what you'll also see on the other, on the right hand side are all of the different plugin components that you can build within data IQ. So you can create your own recipes. You can create your own versions of data set checks. You can create notebook templates for your team to spin up and reuse. And that is what the data IQ plugins are all about. So here is an example of one of our plugins called the feature factory events aggregator. And it's basically a way for you to automatically generate events from kind of like time series, like think about like web logs or something like that. And you'll see here, this is like, you know, a Python library that we've built. But what we have done is wrapped this, all this Python code up into a plugin so that now if I want to include this in my flow, all I have to do is select the plugin of interest, create the inputs and outputs. And then, you know, if I'm the data scientist or if I'm the data analyst who it doesn't want to concern myself with all of that code, I have a predefined feature engineering plugin that allows me to just input my configuration fields and I can generate those features that I might be using in reporting. And this is helpful not just, you know, if you're a data scientist who wants to enable other non programming data analysts, but it's also helpful if you have a team of data scientists who doesn't want to have to like always re-import the same library. And maybe you want kind of a little bit more of a clearer picture into exactly what your different feature engineering recipes are doing. You can wrap those up as plugins. And then it's really easy to incorporate those into the flow. Okay. So we've covered kind of like data exploration, feature engineering, and now about what about ML? So I think one of my favorite aspects of data IQ is auto ML, you know, I can run a whole bunch of different model tournaments without having to write any boilerplate code. But like the theme of this presentation is there's always the question of, okay, you know, the baseline was fine, but now I need to optimize it. Now I need to tweak it and there may be some packages or specific, you know, business rules logic that I need to implement using code. And how can I incorporate that into the data IQ platform? So there are two primary ways that we can do this. The first, which is probably the most kind of like traditional entry point is our integration with ML flow and our experiment tracking feature. So if you're not familiar with ML flow, it's an open source project that basically allows you to log all of the data about your machine learning experiments. It could be logging which hyper parameters you chose, it could be logging performance metrics, and of course logging the model objects themselves so that then you can deploy them to environments later on. And the data IQ platform comes with an ML flow server out of the box. So you don't have to worry about setting that up at all. All you need to do as the data scientist is learn how to use the ML flow APIs and you can embed those into your into your project. So if we take a look at my custom modeling notebook here, we can see I've built out a machine learning, psychic learn pipeline. But here I am leveraging that integration with ML flow so that I can start to track all of my experiments. So here it's just a couple of API calls and I log all of the metrics of interest and data IQ tracks all of that information in this experiment tracking interface where then I can see all of the different runs that I went through as I'm iterating on different hyper parameters search spaces or algorithms of my choice. And I can compare all of those metrics that I logged. And then if I dive into one of these runs, I can view the detailed metrics, view the detailed parameters. And here we can see that model that I wrote with my own two hands. I didn't use auto ML here. As packaged up as a pickle file, that then can be easily deployed. So if I want to incorporate this into the flow, I can click this deploy button, give my model a name, indicate the version, and it will land in my flow much the same way that this model did. So here is the model that I built. And it looks just like an auto ML model from data IQ. We have all of our versions automatically tracked each time I redeploy that model. And if I click into the model, we have out of the out of the box value add pieces of the platform. Like what if analyses model explainability reports, performance metrics. So I didn't have to you know, code all of these pieces by myself. This is all automated and makes my life easier and faster. But I was still able to use a you know, custom scikit-learn pipeline that I wrote, you know, because I needed control over that process. And all I had to do, all I had to do was write a few lines of ML flow code to import that into data IQ as this saved model object. Now the other piece of leveraging code within data IQ for ML purposes is directly within the ML interface. So if we take a look at our feature handling and we choose like one of our numerical features here is where I can you know, X abstract all of that boilerplate code for automatically doing rescaling. Or if it's a categorical column automatically do that, tell me encoding. But if I do need to do some custom pre-processing, that's available to me as well within this interface. And one of the things I like about this is both the feature pre-processing themselves from a categorical standpoint or a numerical standpoint combined with algorithms where I can you know, author my own custom estimators as well. All of that gets packaged up so that the pre-processing of the data and the actual estimator itself are always together in one place so that then when I need to ship it to production environments, I don't have to worry about re-duplicating exactly how my data was you know, the pre-processing steps or anything like that. And those those those customized AutoML models can then be deployed into the flow just like my models that I imported via MLflow. And you can then you know, using the whole framework that Chris explained, deploy those out to production environments, create an alert and monitoring loop so that you can track those models in production and understand you know, do they need to be retrained? What does my data drift look like? But you still have that flexibility of building your own custom model. Okay, so another question, what about custom front end? I think a lot of data scientists are picking up a lot of these new frameworks like Dash and Streamlit in order to build data applications without having to know like a whole bunch of HTML and JavaScript and all of that kind of stuff. I personally love these new packages. And once again, data IQ being the platform that is designed to enable any aspect of the data lifecycle has integrated those into our platform as well. So if I want to create a new web app out of the box, we have support for our shiny web apps, dash apps, bouquet apps, using code studios, you can also launch and build Streamlit apps as well. And this allows you to basically author, you know, those apps in the kind of like framework that you that you want to use embedded within the same screen, view the preview, and then data IQ, the platform will take care of building the Docker container underneath the hood that then you can deploy out into the Kubernetes environment that data IQ manages. So as the data scientists, I don't have to worry about it. I can let the platform handle it, but I still can build my own custom app. Okay, next, I've been talking a lot about all of these different packages, right? So one question is, how can I manage all of my code environments without crying? Okay. And my, this is like one of my favorite things about data IQ is if I want to build a new custom code environment, all I have to do is list out the packages that I'm interested in. And then I click build for the containers that I want to build them for. And much like the web apps behind the scenes, data IQ will build that Docker container and take care of deploying it out to your Kubernetes environments so that as the data scientists, all I have to worry about is what packages I want to use, but I don't have to write a single line of a Docker file. Thank goodness. Okay, and this is kind of one of the last pieces that I've been alluding to is that behind the scenes data IQ will do that SQL push down. So if I am writing a SQL query, data IQ will, you know, just directly push that into the SQL environment of my choice. If I'm writing Spark code in the same flow, you know, I can push that into the Spark environment. If I have in memory jobs that need to run on a Kubernetes cluster, data IQ will take care of that as well. And you can define exactly how those pipelines, which are orchestrating all of these external compute environments look using those automation scenarios, which as I've kind of demonstrated is all, you know, kind of customizable and made flexible via all of the different custom coding options available. Okay, so the recap. There are a million and a million and one different places to write custom code in data IQ. And I'm not going to go through this entire list, but I'm going to harken back to my initial opening of this talk, which is I love data science platforms because it can automate so much of the process for me. But data IQ, the platform has made sure that if you are a programmer who's used to building everything yourself, you're still going to have that flexibility, regardless of whether it's in feature engineering, data exploration, building applications, or defining custom pipelines. So the last thing I want to point out is resources because much like the general world, the population of coders in data IQ is smaller than the population of people who just use the point and click features. And so sometimes it can be a bit hard to find exactly how to learn about all of these things. So the first place that I would tell you to go if you want to learn about how to use all these different coding features in data IQ is to go to our free academy. You can sign up for this academy, even if you don't have a data IQ license. And you can use a free trial if you want to use this academy as well. And it includes a whole bunch of courses on a whole bunch of the different topics that I just went through. So that's the first place I would go to kind of get the lay of the land about how to use the data IQ APIs, what your options are, building web applications and plugins. And then the other piece is we've recently launched a developer guide. So if you go to developer.data IQ.com, this is a documentation page that is specifically geared toward developers, and it contains tons of getting started and tutorials that can answer a lot of the questions that I hear on my calls every day. Like how do I take advantage of pre-trained models in my data IQ flow? So if you're one of these people asking me that question, visit this website first, and then you can ask me your question. Okay, so with that, I will thank you for your time, and I will also say if you have more questions, I will be here in the demo booth, so feel free to ask me anything you want. Thank you.