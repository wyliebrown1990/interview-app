 Hello and welcome to the observability trends webinar series. I'm Grant Swanson, your host for today's session. We will be talking about observability best practices for multi-cloud environments. Our agenda begins with a brief presentation, setting the stage for those managing work modes across multiple clouds. We'll then dive into a live product demo of our observability cloud. Lastly, we'll open the floor for questions and answers. We recently launched our 14-day free trial offer for observability cloud. If you decide to sign up, our data engineering team will provide access and will be available if you need assistance with data ingestion or any specific use cases you're looking to solve for. Now I'd like to introduce our guest speaker, Max Kydin, a distinguished solution engineer. Take it away, Max. Before we dive into the demo, let me cover some of the observations, best practices, and lessons learned that ad-observe will learn from ourselves as well as our customers. Let's talk a little bit about the challenge first. The problem is that cloud platform, especially multi-cloud environments, they unlocked many unique technical business opportunities, but they also created many new challenges, especially from the observability perspective. And the reason for it is because these environments are based on ephemeral services, microservices, event-driven architectures, highly distributed computing, and all of these led to the explosion of observability to limitry. Sometimes reaching hundreds of terabytes per day in log metrics and trace data. And that really creates a lot of noise and difficulty in issue prioritization. Cloud providers have their own built-in observability and monitoring tools. But there's a couple of limitations in these tools. One is they're very focused on the specific cloud environment that they belong to. So for instance, Azure Monitor, well, is specifically for Azure. Google Stackdriver is specifically for GCP. So typically you end up leaning on third-party vendors, conventional monitoring platforms, and not very well-sutined for the modern cloud-native world, especially multi-cloud type of setups. One of the leading problems with them is they become prohibitively expensive in scale, because they're not based on an architecture that can handle massive volumes of telemetry data economically. And so what you end up doing as a result, you end up going with aggressive sampling, low data retention, juggling multiple storage tiers, all so that you can control the cost. While that may help you in terms of the control in cost, it all erodes the value of the telemetry that you collect. Additionally, because you store in data in disparate platforms, you may end up in a situation where you are unable to do correlations between this data, with this makes troubleshooting extremely difficult. So at Observe, we learned these hard lessons and we built the platform that is designed to address specifically multi-cloud challenge. So first and foremost, we believe that what you need is unified observability at scale. In the modern architecture, you need to collect more data than you think you need. And the reason for it is because you want to have the possibility of dealing with future unknowns. And if you have sacrificed a lot of your data, if you decided not to collect it, you get hit by an issue, you may just simply not have the telemetry. And at that point, you're kind of stuck because you cannot go retroactively and retrieve it. So our belief is that you should collect everything and store it in a single data store, and the data need to be perpetually caught. You cannot go and juggle tiers and then deal with data rehydration in the middle of an incident. Data needs to be always available to you for the full duration of the retention period. And the retention period needs to be long. Two weeks are not going to cut it these days. You cannot do seasonal analysis, you can undo trend in analysis. Typically, you need to store data for more than a year in order to be able to make most use out of it. And once you collect that data, you need to be able to do something with it. And here we believe that schema on ingest, or as they call it schema on right, is actually the wrong solution. You need schema on demand or schema on read, which essentially gives you the ability to ingest any type of data. It doesn't really matter if it was structured unstructured, semi-structured, could be log, metric, trace, or any other type of data really, and just store it in the data lake for future use. But then let's say you often use a specific type of data, a specific set of metrics as a specific set of logs, you can retrieve it from the data lake and accelerate it in real time. Also, while you're at it, make sure to bring not just observability data, but what we call contextual data. So for instance, if you're using GitHub, then bringing GitHub Action Runs, bringing commits interior observability platform, they will help you to do meaningful data correlations and do root cause analysis, which brings us to the next topic, which is the platform the observability platform needs to be able to link and correlate data both temporally as well as based on the relationships between the data. And all of this needs to happen automatically in real time on the fly. So when you get hit by an incident, you can troubleshoot in context, and you can drive an understanding of impact and probable root cause analysis a lot faster. Now finally, if you're thinking of choosing the solution in this space, you need to lean on the vendors that provide out of the box content that can get you started with the common cloud environments out there. The reason why having an out of the box content is just as important as the flexibility of the platform is because you want to get started fast. Let's say you just migrated a portion of the application from a monolith sitting in a data center into a cloud environment, you want to be able to get going and monitoring it day one. And then finally, you need to get from all of this data, from all of this content, you need to get actionable insights and alerts just so that you don't get overwhelmed. You want to be able to leverage those relationships, the data correlations, to have actionable alerts and the dashboard that provide you that context and aid in triage and prioritization. So with that being said, let's dive into observe and see how we deliver these capabilities within our platform. Under the hood, observe is a data cloud. It can entice any type of data whether from your cloud environment such as AWS, GCP or Azure, as well as any on-prem data from physical data centers that you might have. Upon ingest, the data gets parsed in real time using flexible schema on demand to create a so-called data sets. Each data set represents a particular type of data, particular type of event. So for instance, cloud which locks is an example of a data set. GCP cloud function metrics is an example of another data set open to let me trace is yet another data set as you may have already gleaned from this picture. Data sets are not isolated from each other. Observe in real time creates correlational links between them that aid troubleshooting in context. Let me show you what I mean by that. Let's say you got an alert for a particular AWS account. Now this alert might be related to an EKS cluster that is running within the account. Observe knows that EKS clusters are actually under the hood Kubernetes clusters. And it knows that Kubernetes clusters run a variety of different things. Most important of them is being pods. Pods in term have containers and those containers in term are running microservices that are instrumented with open telemetry. And this microservices are receiving regular updates when engineers are making changes to the application code and committed it to GitHub. So notice as I was traversing the graph, observe cap track of my actions by creating this breadcrumb trail on the bottom and look at this functionality graph link which is essentially the ability to create a series of correlational links through the graph. This is something that allows you to ask interesting questions of the system. So for instance, you can say we have an alert for this EKS cluster. Show me all of the services that might be affected by an issue that we're investigating here. And by the way, pull in all of the GitHub commits that may have went in and can help us understand the root cause better. Just before we dive into a more in depth investigation, let's see what kind of content observe has out of the box that it can help us with our troubleshooting efforts. Observe has this concept of data apps or applications. And what these essentially are, there are bundles of content that is targeting a specific technology stack. So for instance, I have an AWS app, GitHub integration, GCP app, Kubernetes, primatious open telemetry and so on and so forth. And each one of these apps comes with four things. Data set and data sets can be logs, metrics, or traces or really any other arbitrary type of data that you might be sending in through the system for that particular application. Dashboards, monitors and correlations. All of these come bundled with the application. So once you install the application, you just have these things out of the box. Now a lot of folks will start their troubleshooting naturally from dashboards. So let's say I want to understand the health of my AWS environment. I could just go ahead into a high level overview dashboard for AWS. And I have at my fingertips information about my AWS environment. Same goes for GCP. If I want to understand the GCP, I can go again, I had look at the high level overview dashboard for GCP and see what I have running up here. And in any, if any of these things are under pressure, if there's any sort of issues going on with this particular environment, there's also a lot of specialized dashboards that will give me an understanding of specifics of each individual service. So if I want to, for instance, look at GKE, I have a dashboard for that as well. Now, if a high level dashboard, which is really at just an aggregation of different metrics, is not enough and you want to dig deeper. Let's say you want to pick a particular metric for a particular service where you can jump into metrics explorer. A metrics explorer conveniently exposes all of the metrics that are coming into this environment. And by all, we don't just necessarily mean that metrics that are collected by the clouds themselves, such as Azure Monitor or Google Stackdriver, but also pretty much any other metric you may be sending into the system. So let's say you have open to elementary metrics coming in here, they'll show up in the metrics explorer as well. Now, we could go ahead and look, let's say, for instance, for GCP metrics in here. And let's say I want to understand the volume of a traffic that has been processed by my cloud functions, the volume of data. Well, it's very easy for me to go start customizing these different charts and just kind of filter in on a specific maybe lambda function and just get a really solid understanding of what's going on under the hood. So all of these functionality is available here. This is very powerful and can help you get further along in terms of the in depth understanding. Now, once I get these charts in a way in which I want, I can immediately spin up monitors from them. I can add them to dashboard and do a lot of other creative things with them that can help drive my troubleshooting process. We also have traces in here. So this is coming from the open to limited instrumentation. And this can be extremely powerful if you're trying to understand the behavior of microservice that spans multiple cloud environments. So I mentioned earlier, the cloud native tool and for instrumentation is usually focused on just one individual cloud. So AWS is going to do a tremendous job with X-ray for just AWS environment. But if you want to bring in your monolith applications from the data centers or you want to your microservices, let's say, span multiple clouds, it's going to become very difficult. But observe doesn't come with that limitation. You can very easily have a trace that spans multiple clouds. And finally, we have monitors. Observe comes with a number of monitors out of the box packaged with the applications. Now, as you may have noticed here, these come as templates, which essentially means that they're in a disabled state until you enable them. You can also customize them to your specific requirements. But this cover a lot of things out of the box. So you don't need to guess on things that you need to start monitoring once you onboard a particular cloud environment. And this is not just observability and monitoring. Some of these things might be related to security, such as S3 bucket permissions change. So there's really a really good coverage of the things that you might want to look out for across all of your different cloud environments. In the next section, we're going to go a little bit beyond of the out of the box content that is provided by Observe. We're going to create our own custom dashboard for a microservice that spans multiple cloud environments. And we're going to try to understand if there are issues in particular cloud environments that may be causing this microservice to malfunction. Let's see how we can explore a behavioral microservice that spans multiple cloud environments. Now, for our troubleshooting, we're going to jump into application logs. And we're going to look for a particular microservice. Let's say we have some people complaining that maybe there's something wrong with our shopping cart microservice. So I'm going to go ahead and select this microservice and filter on it and just explore the logs a little bit. This particular type of record at item A sync looks very interesting. And so I want to go ahead and filter on it. And I could say log approximately equals add item. Let's go ahead and filter on that. There's not much I can do with this type of log yet because this information in this log is kind of locked behind the structure of this log, which is in this case is key value pairs. So what if I wanted to extract this using schema on demand into individual columns? In Observe, we have built-in parsers. So for instance, I could use a built-in key value log pair parser or I could use only GPT to generate the type of logic that is needed in order to extract the data from the log. And I'm going to go ahead and extract this data using the logic that has been suggested to me by Observe. And so now I have my individual columns, which include product IDs and quantities. And now we can go ahead and start visualizing this data. Observe is going to suggest the default visualization, but this is not exactly what I want. So I'm going to go ahead and massage it a little bit. I'm going to say let's go ahead and look at the quantities of different products. And instead of counting the values, we want to sum the values as we want to group them by product ID, which is an information that we have just extracted. This probably will look better as a stacked area. And I personally like to have my ledges of the right. Now what can I do with this visualization? I want to create a little dashboard. At this point, this chart represents everything for all cloud environments. But what if I wanted to understand the performance of different shopping cart microservices that are spread across multiple clouds? How could I do that? What's very easy to do? I can just duplicate this chart. Let's go ahead and expand it a little bit. And I can edit it and say, well, instead of grouping by product IDs, I don't go ahead and group it by the cloud provider. And so now I could see that my my business is actually hosted on three different clouds on GCP, AWS, and I'm also using an IS provider popular in Europe called Hertzner. So I have a separate environment running in there. And I can see that majority of our traffic is actually served from Europe. Probably we have a lot of shoppers in there. I see some that are served from AWS and GCP that are located in North America. Now what if I didn't want to understand? Well, which specific EKS, GKE, or Kubernetes clusters, are servicing this microservice? Well, I can break down by that too. I can go ahead and now add another grouping by cluster. And so now I see that this is actually serviced by four separate clusters. One of them is located in Amazon, two in GCP. One is located in Hertzner. So this is already very useful information. If I do have some sort of issue, I can very easily pinpoint a particular cloud environment in here. In the interest of time, I'm going to scroll a little bit forward and show you a more complete version of this dashboard that where I leveraged matrix explorer and traces explorer to bring in some certain metrics from the .NET runtime. This particular service is running on .NET. So in this case, we're looking at garbage collection give utilization as well as we're bringing open telemetry tracing data to understand the lead-unces between different functions, between different operations within this microservice. And so this just showcases the power of the underlying data lake platform that allows you to seamlessly bring different types of data into a single dashboard across different cloud environments. In this final part of the demo, I'll show you how to leverage observe to troubleshoot an error that is generated by a service or an application or a piece of infrastructure with the goal to dig deeper into the root cause analysis as well as trying to understand which part of the cloud is actually creating this problem. I'm looking at a notification for a high service error rate and observe and I can tell right away this is related to the ad service. So I'm going to go ahead and click on this notification to see a little bit more detail of what's happening in an observe. I can see the time frame on when the notification is triggered. I can also see any prior instances of this particular alert been raised for the service. I'm going to go ahead and drill into this service to understand what's going on here in more detail. As I explore red matrix within the dashboard for this particular microservice, which is the ad service, I can see that the latency, the rate of execution looks pretty good, but my error rate is spiking quite severely and at some point it reaches 50% of all of the requests. I can also tell which specific function call is causing an issue, which specific route. I can understand the upstream and the downstream impact as well as understand the slowest operations that have taken place in here. If I'm an SRE, the part of the screen that I'm probably most interested in is this one here on the bottom that shows me all of the error spans and traces. So my immediate reaction might be, just go ahead and click on one of these traces. I can see that the culprit is here, which is this get ads microservice, and I can get additional contextual information by looking at the specific error messages. So here I can see that we have some sort of GRPC error message that says resource exhausted, but unfortunately there isn't a lot more of additional information our error messages now. At this point, I might decide to escalate this to an engineer, and it's very easy to do it with observer. I can just simply share a link with them that encapsulate all of the filters that I have applied in the current time of my investigation. So when the engineer receives this link, maybe via Slack or Teams call or whatever, they are taken into the exact same place I was looking at when I was doing my investigation. So they don't have to start from scratch. They can pick up where I left off. An engineer might decide to do a little bit more of an ad hoc exploration of the spans. And the way to do it is by spinning up what we call a worksheet and worksheet is essentially kind of like a running the runbook that encapsulate all of the changes that you're making in observe as you're investigating an issue. So one thing that they might decide to do is say, well, for this particular microservice, is this localized to a particular cloud environment? Is this maybe localized to a cluster or something like that? And yeah, here we can see that all of our problems seem to originate in our IAS provider and localize into their EU location one. Immediately, they can say, why don't we open up the view into the virtual infrastructure that is running this microservice and see if there's any problems in there? And what observe will do is again, it will use the power of correlations, it will use graphene in order to take us from spans and traces into the underlying infrastructure in here. We can see that this particular piece that has been running fine for an hour. There are no issues necessarily that are happening here. My CPU and memory utilization metrics are looking pretty healthy, not seeing really anything that would indicate a hardware or a virtual infrastructure failure in here. So at this point, I might want to pull in additional logs. And it's very easy to do. I can go ahead and say, for this set of containers that we're looking at in here, just go ahead and put additional logs. And what observe will do is again, it will use the power of graphene to create a correlation on the flight for the specific logs that we're interested in. I'm going to go ahead and filter on failures and look a little bit deeper into the logs to see if I can glean any additional information in addition to what we had in traces. And unfortunately, I'm kind of a little bit out of luck here. I could still see this resource exhausted, but the description and the causes now. So unfortunately, we're not further along than we were before. Now, this is where bringing in contextual information within an observe is so important from any cloud environment. And so what I could do here is I could say, for my service, let's see if we made any changes to the code that is run on the service. Have there been any GitHub commit? And indeed, we can see that around the time of the incident, somebody made a commit that set at service resource alignment. So that could explain potentially why we're running into the resource issues. And I could just go ahead and jump right into GitHub and say that, yes, indeed, Max Skybin committed a commit a commit about an hour ago. And this is something that is potentially causing our issue now. So now I see the culprit. And I can go back to Max and asking for to have a look and maybe roll back that particular commit. Now, notice that as we were doing our investigation, we essentially build this worksheet, this runbook that we can now save. And this is something that's going to be available not just for us, but for other people. And other people can modify, for instance, the time frame that where they want to troubleshoot an incident, or they can select a different microservice where they can make modifications to other filter and then selections that we've done. And so they can make this workbook their own and apply to their own use case. And this way, we can disseminate the knowledge that experts have about our infrastructure and applications to other folks who maybe knew to our platform. So at this point, we're going to wrap up the section and open up for Q&A. Thank you, Max, for the demo and presentation. We have a number of questions coming in that I'm monitoring at this point. The first one I see is, can I create my own custom correlations? Yes, certainly. While we do provide a vast array of correlations out of the box with our data apps, you're also at liberty to create your own on demand based on the data that you're sending into the system. Excellent. Let me take a look and see the next question. You mentioned cost savings compared to other platforms. Can you elaborate where they come from? Yeah, certainly. And that's a big topic, obviously, in and of itself. But generally speaking, the cost savings that you get in observe are a function of our architecture. And one of the critical pillars of that, if you will, is a separation between storage and compute. So the way we take the data is that we compress it on the fly and what you end up paying is the compressed storage of that data. Now, the data is still perpetually hot. It's not like we're putting it in a cold storage. So we're stored compressed hot data that you can retrieve at any point and work with. Now, when you're retrieving the query in this data, the portions of that data are become in accelerated. And so you consume compute as your accident already, a fraction of accelerated data that is stored in the data lake. Over time, that ends up being significant cost savings comparing to a lot of other platforms on the market. Got it. Thank you for that. We have, give me just one second. I'm looking the next one. Do you support Azure? I didn't see it in the demo. Yeah, yeah. So I briefly mentioned Azure as we're going through the demo. And yes, we do support it. I'll post in the chat a link to our documentation to our Azure data app. Just simply, I didn't make it into this particular demo environment. But yes, we definitely support Azure as a cloud platform. Excellent. So the next question is interesting. What are the challenges associated with data governance and compliance across multiple cloud providers? Yeah. So I tried to answer some of these questions in the in the general chat as they were being answered. But obviously, these are really, really big and media topics. Right. And remember, we think about these topics from observability perspective, right? So we're not necessarily a data governance type of platform. We're thinking about them again from more of monitoring and observability perspective. But we see this manifest itself in a couple of different areas. One is it's very common to have a conversation about data residency data governance. And so, observe as presence in multiple Geos, we have customers, for instance, in Europe or in Asia, in order to be able to satisfy a specific governance posture within those specific geographies. Often times where the question also comes up is I have all of these multiple cloud environments. And I have some data in Azure, some data in GCP. All of this is subject to maybe PII, PHI, some of the other data governance restrictions that are specific to the domain that you're operating in. Now, I've sent that data into observability provider. How do I ensure that my role-based access control is applicable in this new environment? And so we provide a very robust data, sorry, Rbeck framework that allows you to leverage our data sets under the hood to do per column locking of information. So, for instance, let's say within your data, you have a field that represents some sort of PII data, maybe an email address or a password. This is something that we can upfuscage using schema on demand. And let's say only give full read access to security team for that particular type of data. Everybody else, like an SRE or somebody else, might only see the off-fuscated data within Observe. So you can have very, very granular controls. If anything else fails, like let's say you accidentally leaked that data that you were not supposed to to us, we can just go and wipe it out from the data warehouse and provide you a report that that action has been taken place to satisfy your compliance requirements. Excellent. So I think that answers all the questions. I appreciate that, Max. Thank you so much. I want to let everybody in the audience know that our next event will be on November 16th at 10am Pacific time, where we will discuss real-time incident detection and resolution and complex environments. We will also send a link via email with this recording and we will invite everyone who registered for this event to the upcoming webinar. This concludes our session for today. Thank you for joining us and have a very day. Thanks, Max. Thanks, Grant. Thanks, everybody for joining. Bye.