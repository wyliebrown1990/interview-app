 Hey, everyone. How are you doing? My name is Amanda Melberg. I'm a senior partner sales engineer at DataIQ. And today I'm going to talk to you about how we can operationalize large language models. And you might be feeling something like the way that I was feeling when they told me I had to talk about operationalizing large language models in 20 minutes or less. Whoa, right? This is a bit of an overwhelming problem, a little bit of the wild, wild west. We know we have to do it. We know it's important, but we don't really know where we need to start. But as Albert Einstein once said, the way to solve a complex problem is to break it down into simpler parts. And what if I told you these simple parts were actually available in DataIQ today? They might not just be in one complete piece. And so today, what I want to do is discuss how we operationalize a large language model, moving from POC to production. And today I'm going to discuss a use case of what I had to do to move this model from POC to production, discussing common challenges that I faced when talking to key project stakeholders. And we'll walk through an illustrative use case today where we developed an internal chat bot for our go-to-market teams to answer questions about what is DataIQ for our customers. This is an illustrative use case. We don't have this in production yet, but I thought it would be, we could relate to this to go through the session. And I'll walk through this final project in detail. I'll talk about how we integrated components to monitor this project from a cost and performance perspective, to do some content moderation, to also add a human in the loop interaction. But before we get to this final project, I want to go back to where we started, as I was developing this initial state POC. So by show of hands, who has either played around with some generative AI LLM applications or potentially developed something lightweight? You can put your hand up if you have. Awesome. So maybe I could guess that your stack looked a little something like this. Usually for a POC or an MVP, we start by connecting to some sort of large language model available through an API. Maybe something like OpenAI, something from the cloud providers, and this is a great place to start. You develop an initial lightweight pipeline, something that may include prompt engineering, retrieval augmented generation, and then you get a generated output, which you surfaced through some sort of data product, chatbot, web application, a model as an API, etc. And this works. This is how I started too. It's a great way to get a working MVP out there for every sea level executive who wants to see generative AI in their enterprise. So I got my POC working, and I go over to my project stakeholders. And they're like, OK, this is cool. But if we're going to deploy this into production, we have to make sure we have governance and safety requirements. So my VP of customer ops says, you know, we need to have a human and review element, even if it's just for a statistical sample of some of these output and generated results. Because our customers expect that accurate and correct answers that our CSM's provide them. And if we're going to use an LLM, we want to make sure that this output is validated. Our head of risk wants to ensure that we balance the speed of innovation with caution. And she, along with this human and the loop element, wants to make sure we have an automated way to do some things like detect PII contact in that input query and remove those, you know, from the LLM. These are customer questions. They're going to have things like maybe your email in it. And we don't want to send that information to the LLM. We also want to make sure for the outbound generated content, we can add an automation layer, something like toxicity detection, right, to ensure that we're not sending any harmful content to our customers. They wouldn't like us too much about that. And finally, director IOT, as all IT directors are, they are, you know, nervous about the cost, the cost of these API calls, the cost of potential new infrastructure, whether that be a larger CPU or a larger GPU. So they just want a way to monitor this cost in like a dashboard, see what's happening on both the project level and the instance level. But also have some sense of automation to send a message to our go-to-market teams if we're, for example, exceeding our daily API threshold limit. We want to make sure we're actually having ROI using this generative AI and it's not being just, you know, a large API bill at the end of the year. And so I take this information and I go back to my LLM pipeline and I realize while this pipeline was functional, it wasn't going to be something that we could deploy at scale. And so I think, okay, what are some things we can do in this pipeline to achieve those requirements of our stakeholders? Well, first, if I deploy an LLM tomorrow and that stays in production for a year, I can put money on the fact that there will be a new LLM that does come out, a GPT-5 of sorts in that year, right? I don't want a hard code, GPT-3.5 turbo in my code. I want to have this agility to switch between different LLMs. What's working today might not work tomorrow in six months, et cetera. And so I want to make sure as new technologies emerge, my flow in data IQU has this agility. Two, as I mentioned, I want to evaluate that sensitive information and I want to reject it from my queries. How can I add some sort of processing layer in between my inference to ensure that I remove PII? Three, providing some sort of caching functionality. I'll go into this more a little bit later, but I've been a technical support resource here for two years and you all ask the same questions. So I don't want to keep on regenerating that LLM with the same information if I already have that answer that potentially has even been validated by someone from our team. Obviously I want to make sure that I'm checking and monitoring that context. And then finally, monitoring the entire LLM services, you know, over the course of this going into production. And as I think about how I need to work on this for this POC, right, this is a formula that I'm probably going to have to use for all of my generative AI applications, showing the importance of really decoupling that infrastructure layer from our services layer. The LLM mesh. This is really how we think we'll be able to deploy generative AI applications as scale. We have our LLM service layer, we have our applications, and then we have our mesh in between, right? And this mesh consists of a lot of those requirements that I heard from my project stakeholders. Doing things like routing, so being able to send requests to different LLMs is important. Doing cost reporting, PI, I-I-Detection, audit trail, et cetera. This LLM mesh is really what's going to help us scale not just one, not two, but N number of these generative AI applications into production. So this was the idea, started from the POC, learned from my stakeholders, understood the requirements that I need to move this into production. So how does that look like in DataIQ? So this is the overall DataIQ project flow that I'll be walking through today. It consists of integrated elements of the LLM mesh architecture. Today I'll just be walking through quick videos of each section, but if you want to talk after, I'll be at the demo booth so we can deep dive into this project, it's a real project, real working project. So we'll start by showing the POC element that I explained. I use built-in text analysis recipes to create or add external business knowledge to our LLM using a retrieval augmented generation approach. This approach will be discussed in the next session. We're talking about the operations around this. Then I leverage Prom Studios to track different experiments and compare across different LLMs. That iteration of the prompt is really where we can see a lot of performance gains. So I want to test check, make sure it's cool. Now I think back to my VP of customer ops, she wants me to add this labeling human in the loop idea. So I use a labeling recipe available in DataIQ today to annotate the generative output. I look at things like, is this correct? Is this not correct? I can store that as a source of truth that I could use later for caching, for reinforcement, learning, et cetera. I can use a PII detection layer using a code recipe. The benefit of code recipes in DataIQ is we can leverage the latest and greatest that's out there in the open source community. And in this LLM ops monitoring space, there's a lot of new things that are coming out each day. In my example, I'm just using any our natural entity recognition, but for the content moderation output, we use another LLM toxic bur to moderate the output. On this generated output, I get a score across different categories, and I'm able to add metrics and checks to alert if something fails, et cetera. I'll explain how we can use things that are sent to the event server, again, something that's available in DataIQ today, as a caching layer to re-avoid generating the same output over and over again. And then finally, I'll talk a little bit about compute resource usage. You might be familiar, you might be using it in DataIQ today to monitor cost and raise alerts via metrics, checks, and scenarios in DataIQ to help with this performance and cost monitoring. So I'll go through each of these components, step-by-step, and hopefully convince you that we have the tools that we need in DataIQ today to operationalize an LLM. So let's start with this concept of LLM manage connections. So just like you have the ability to connect to things like snowflake, Databricks, Cloud Storage, et cetera, we want to make that same accessibility available to you at the LLM level. Also make the ability to manage who has access to that LLM, et cetera. Almost like an API gateway, of course. So let's take a look at what this looks like in DataIQ. So I have my flow, and I want to develop this retrieval augmented generation approach. I use an extraction recipe to turn this from unstructured data to unstructured data and store this text in a vector store. Now I have a multitude of text analysis recipes that I can apply to any generative AI problem, and each of these look the same. I have the option or the opportunity to select which LLM that I want to use. Future-proofing the solution to making sure what's cool today is still cool tomorrow in six months and a year, et cetera. Now from a prompting perspective, I can edit this in prompt studios if I want, but really what I want to show here is the ability to switch providers between LLMs. Now let's talk about content moderation on both the input layer, the output layer, and that human and the loop review as I was talking about earlier. We know that there's going to be some operational legal and regulatory compliance standards that we need to follow. So let's plan now, right? We see things like GDPR, et cetera. We can expect these to be applied to LLMs. Well, what does this look like? In DataIQ, we can add something like a PII detection layer. This allows us to add a code recipe to do things like extract, NER, and PII detected layers. Where this is flagged, person, phone number, email, et cetera, we can simply filter that out of our DataIQ data set to only input questions where we have the information. From the output layer, we can add toxic bird, which allows us to get a score across different categories. These categories we can use metrics to find the maximum and ensure that thresholds are passed before sending this to our automated output. From the human and the loop perspective, we can use the labeling recipe to provide annotation on our generative output. We can highlight things that are correct, incorrect, and pass that on to a second level reviewer to validate that this is right or wrong. All of this information is being stored as a DataIQ data set, so you can use this information either for a caching layer or as your now source of truth data that's been validated for these questions and these answers for future learnings. So let's talk a little bit now about caching and cost control. So as I mentioned before, what if we can add a caching layer to not regenerate or recall our LLM if we don't have to? This could make sense if we're talking about an internal chatbot about what is DataIQ. The same repetitive questions of something like can DataIQ be deployed on cloud might be answered a few amount of times, right? So what we can do is we can create a query log, either using something like that labeling recipe like I showed or using information from our event server and essentially do a similarity caching mechanism over these new input questions plus some business logic to say if this question is similar to questions that I've asked in the past and the feedback is positive, let's resurface that answer that can potentially help in cost control and make sure that your latency is really low because you're essentially not even a query now all that. So again, what does that look like in DataIQ? Well, if I have this caching or these audit logs in a managed folder, I can simply parse them. I can get information like the previous question from the past, the previous answer from the past, as well as that feedback if it was positive or negative of the past. Now I can add something like a Python recipe to do that semantic search and I can see in my output where the source of this information came from. So did it come from the cache or did it come from the LLM? And on a large scale, this can allow for a lot of cost savings. We don't need to regenerate information if we don't have to. So a lot of power right now in this concept of caching and cost control. And finally, one of the most difficult topics that we can talk about is how do you monitor the performance of a large language model? And I'll use the example of why this is hard in this explanation of the fact that LLMs are non-deterministic. So what this means, if I ask an LLM the same question, what is DataIQ? It might answer it in three different ways. And that doesn't mean those three different ways are wrong. If I asked you and you to summarize a paper, you'd all write it a little bit differently as well, right? This is just a human element, right? The content and the idea might be accurate, but the language might not be exactly the same. And so traditional machine learning models are often deterministic. That's why we can use metrics like precision, recall, F1 scores, et cetera, to evaluate them. But what do you do when you have non-deterministic outcomes? Well, we like to propose this approach that you can shift to a more qualitative approach. Learning and evaluation matrix of sorts, and across different things like latency, cost, coherence, and bias. Because what's important to one persona might not be important to the other, right? Director of IT might want to prioritize latency and cost, where your customer opts person and your IT risk might want to look at things like coherence and bias. And so really what we want to surface is the ability to look at LLMs across all four dimensions and prioritize what performance looks like to you. And we can do this using those compute resource usage logs in DataIQ that's already being sent to the event server. What we did is now with the new LLM manage connections, we have this new concept of LLM context types. This means we're tracking different things about the activities, like the estimated cost, which user is calling the LLM and what LLM types are used. You can add metrics and checks to look at cost performance, make sure they don't exceed a threshold, or more broadly monitor LLMs through dashboards and charts. You can do this at the project level or the instance level, seen across your entire instance, what the cost savings, cost by project, etc. is happening on your LLMs and also look at usage to kind of understand and take action. If we want to send alerts, we can do so via scenario, running metrics and checks if daily thresholds are reached, and sending for example a Slack message to our team to say your API call is reached, no more APIs for you today. This helps give confidence to your IT team that, again, that investment that you're making in generative AI, you actually do see ROI. And it's not being offset by all this API and computation cost. So hopefully, over the last 18 minutes, I've convinced you that what you have in Data IQ can help you operationalize LLMs today. We have concepts like metrics and checks, compute resource usage, or the event server, code recipes that allow you extend capabilities to the new stuff that's out there in the open source community. So you can also use web apps as well as scenarios to help you make sure that you're deploying these generative AI applications in a safe, secure, and governed way. So you're armed on your journey, you're ready to go, and take over this overwhelming problem. Thank you.