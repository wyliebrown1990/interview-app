 The DataIQ is the leading platform for everyday AI, systemizing the use of data for exceptional business results. In today's video, we will take a tour of DataIQ's end-to-end capabilities by exploring a real-life use case around environmental impact. Let's take a look at how a DataScience team with different skills can work together to turn data into consumable insights. We'll begin our story on the project homepage as we explore our use case, determining what are the best indicators for predicting a decrease in carbon dioxide emissions for our country. We can see the team members contributing to this project, and how they're using some of DataIQ's project management features to track their goals, as well as their progress. Our entire project structure and data pipeline are visually represented on the flow, where we can see data sets represented by blue squares, various visual and code-based transformations, which we call recipes as yellow and orange circles, and machine learning elements in green. Multiple users with diverse backgrounds can collaborate on different elements of the flow together. Data engineers or architects establish the connections to data sources and compute infrastructure. Analysts or citizen data scientists can clean and prepare data and build insights and reports. And data scientists will perform advanced feature engineering and modeling tasks. Finally, ML engineers and IT operators handle deployment and ongoing operations, all in a single centralized space. Let's take a closer look. The first step is accessing data. DataIQ is an infrastructure agnostic platform that simplifies connecting to your existing data storage systems, without needing to move the data or write code, including a variety of SQL databases, Cloud Object Storage, Hadoop, and so on. For this project, we built our data pipeline in Snowflake, which we will utilize for both storage and computation. Using one input data set, we can explore the data in a tabular spreadsheet-like view, regardless of its original source system and format. This view automatically detects each column's data type, gives us insights about missing and invalid values, and provides summary statistics in the built-in analyze function. To get started with some data preparation, we can choose from the various visual recipes available. We can move data, join multiple sources together, aggregate values, reshape the data structure and more. The most common data transformations can be found in the prepare recipe. Here, we can find nearly a hundred different processors to quickly accomplish the most popular data manipulation tasks. We can see transformations that have already been applied, dealing with missing values and column changes. We can quickly add more steps to create new features for our future model with custom formulas like this. At various points during the exploration and preparation phase, the analysts use the built-in charting capabilities to visualize and profile the data in different ways. DataIQ's interactive statistics also provided automated suggestions that helped rapidly identify highly correlated features and discover relationships between combinations of variables, in order to inform which features to later include or exclude in our model. Combining data sets is one of the most common data preparation tasks. A visual join recipe enriched our table with geospatial and country-level information. A couple notable things while we're here. GeiQU allows us to run these computations in database for maximum performance and efficiency, a SQL engine in this case, but we could use Spark or Hive in other situations. And by clicking here, we can see the now Spark SQL query being generated for us behind the scenes. A coder could easily choose to convert this visual recipe to a Spark SQL code recipe and modify this script from this point. But without having to construct this massive query from scratch, a huge time saver. Along these lines, if we zoom out to the overall flow, we can see other flow zones where data scientists perform similar data wrangling on other input data sets, but they chose to do these steps using Python and SQL recipes instead. Coders can develop scripts using this native code editor or the built-in Jupyter notebooks. Or even their preferred IDE, such as VS coder or Studio. Programmatically communicating with the project using the data IQ APIs and SDK instead. Code snippets and shared libraries make it easy for teams of coders to collaborate and reuse functions and modules, whether developed specifically for this project or imported from Git. Now that we've finished our data preparation, we can move on to model development. We build and compare models in the lab, which is a sandbox environment with a guided visual interface for many types of machine learning, clustering, time series forecasting, computer vision tasks and more. Let's take a look at how we created our classification model, which predicts either an increase or decrease in CO2 emissions for a country. Data IQs AutoML made initial baseline suggestions for many design settings, which can of course be modified and overridden by data scientists in various iterations. If not chosen by the user, Data IQ detected the type of task based on the target variable, and set defaults for sampling and the outcome metric to optimize for. We turned on diagnostics to alert us about any issues such as overfitting or data leakage, and we were also able to manually input model assertions, a sanity checks. These are informed statements we believe to be true and should be reflected in the model's predictions. And if that isn't the case, we're notified. You can think of this like a canary and a coal mine, the first sign that the model may not be performing as it should. AutoML also suggests appropriate feature selection and handling strategies based on the variables type, with plenty of statistical context to help data scientists decide how to approach rescaling and missing values. Data IQ comes out of the box with a variety of best-in-class ML libraries and algorithms, and you can also add custom Python models of your own. Modellers can easily assign the runtime environment for training and take advantage of elastic infrastructure. For example, Data IQ can distribute the hyperparameter search across four Kubernetes clusters, with automatic resource spin-up and shutdown on completion. Let's explore the results. Each of these sessions represents a model tournament. We can see all of our experiment results in a single view, or click into any model to dive deeper. For each run, Data IQ automatically generates a variety of interactive, performance, and interpretation reports to provide a better understanding of a model. We can determine which features, where most influential towards our prediction, in this example, renewable energy consumption impacted a country's emissions outcome the most. We can inspect the model's partial dependence on each variable, and check for bias, with sub-population analysis, to ensure the model performs equally well for subgroups in our data. And we can investigate explanations for individual or country-level predictions and extreme probabilities. Finally, in the interactive What If Analysis tool, we can not only try various combinations of input values to review the change in predicted outcome, but also systematically discover what specific changes in our features would lead to the outcome we want, allowing us to make more prescriptive, data-driven recommendations back to business stakeholders. Using these insights, we chose our best model and deployed it back to the flow, to be scored against our test dataset. Data IQ can operationalize projects using both API services and automated batch scoring, and provides many ways to deliver results to data consumers, including business user-friendly data IQ applications, web apps, and dashboards. To share insights with others in the organization, we can export outputs to external BI tools, like Power BI or Tableau, or easily create interactive dashboards right within Data IQ, which contain datasets, charts, KPIs, model reports, and many other elements. Teams can build and host easy-to-use custom web applications, developed with or without code, that empower end users across the organization with self-service analytics. For real-time applications, models and other objects, such as Python functions or SQL queries, can be exposed as a REST API endpoint in just a few clicks. Or, to productionize your pipeline with a batch scoring approach, we could use scenarios, Data IQ's internal scheduler. IQO's contain a custom series of steps to refresh your pipeline or update a model. From checking the validity of new data entering the system, to retraining the model with a custom compute allocation, generating documentation, and potentially replacing the version in production, if it outperforms the current version, and it passes all of our quality requirements. Whether we deploy our model in real-time or in batch, we can use Data IQ's Deployer to manage project versions, dependencies, and infrastructure across various environments, such as Dev, Test, or Production. Note that although we're showing the visual interface for these operations tools, all of the API services and automation scenarios can also be created and managed programmatically using the Data IQ API. To be sure our model remains healthy and continues performing well in production, we must monitor and maintain it. Each time a machine learning model is run against the new batch of data, Data IQ automatically evaluates and stores the results. This gives us a visual gauge of a model's performance over time to detect any degradation since the original training. Automatic drift analysis helps us diagnose whether changes are due to evolving input data, a changing target, or a poorly fitting model so we can take action. Contrary to the model comparison feature, data scientists and ML operators can compare disparate models and versions side by side, or perform champion challenger analysis to identify the best model to deploy. Finally, it's worth noting that for enterprise-level AI governance and oversight, Data IQ provides central registries for both models and project versions, along with standardized process workflows, mechanisms for approvals and sign-off policies, and a way for teams to systematically compare project value and risks across the entire AI portfolio. We've reached the end of this quick tour. As you've seen in this video, Data IQ makes AI accessible to everyone, from creators to consumers. To learn more, get in touch with us on our website, or try out Data IQ for yourself with a free two-week hosted trial. We look forward to hearing from you, and thanks for watching.