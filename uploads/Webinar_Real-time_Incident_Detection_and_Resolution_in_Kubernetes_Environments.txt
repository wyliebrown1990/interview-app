 You just want to show your life and no one else can stop you You want your life, it's your life You enjoy your life, so it's gonna stay the way It's your life, we'll try to enjoy our lives And you ain't different baby You want your life You can change it As long as you can stand there It's the same girl When no one else can hurt you It's your chance now Before you find it still, let's see The heart we dumped is stopped, just can stop you Between them, like a rash, just can stop your breath I can't have a track, only to see you The heart we dumped is stopped, just can stop you Between them, like a rash, just can stop you They're gonna have a track, only to see you Welcome to the Observability Trends webinar series I'm Grant Swanson, your host And today we will explore the topic of real-time incident detection and resolution In Kubernetes environments We'll kick off with a presentation that will challenge your perspective Followed by a live product demo of our observability cloud After the product demo, we will jump into questions and answers Feel free to type your questions into the questions window And anytime during the webinar We welcome everybody on the session today to visit our website And sign up for a free trial Upon signing up, our data engineering team will grant access And remain available for any assistance needed Whether it be related to data ingestion or addressing specific use cases you aim to solve Now I'd like to introduce our guest speaker, principal solutions engineer, Brian Fisher Welcome Brian Thanks Grant Before we jump into a demonstration of troubleshooting in a Kubernetes environment, Let's quickly review the challenges of observability in these environments I think that most would agree that the move from physical servers to virtual machines to containerization Has both changed the paradigm of computing and exacerbated the observability problem The hyper scale, ephemeral nature of containers And just the challenges of getting toometry from containerized environments like Kubernetes is well enough And that's before you add in additional complexities of cloud-based environments And the new serverless computing paradigms that are provided there Sure, it was difficult before having metrics logs and traces and separate tools But that becomes even more difficult as you now add the hyper scale and ephemeral nature of Kubernetes environments Which you really need is a unified approach to observability Which leads up to what a complete observability solution should look like in Kubernetes environments First and foremost, an ideal observability solution will provide all of your observability data in one place in one data store At full fidelity to address well those feature unknowns Sampling and aggregation just causes problematic issues as you're trying to take a look at data and identify unknown issues that are coming up It would also do this with long retention rights so you can leverage the data for things like seasonal analysis, baseline, and trend analysis Having data for two or three weeks is really not enough to accomplish these types of tasks either Second, an observability solution should provide flexible, extensible ingest and schema on demand Now why? Well it's because this provides you an ability to enhance your insights on the data even after it's ingested With the hyper scale of containerized environments and while the variety of development teams were usually involved in developing microservices architectures It could be difficult to enforce perfect standards for logging or telemetry You often need to address these inconsistencies even after ingest so that you don't lose the visibility It's also important to bring in contextual data which can provide key information about your containerized environments such as events from your CICD pipeline, Jenkins builds or GitHub information or information from your service desk This helps provide key context when doing correlation and analysis Correlation and automatic linking of your data is next With containerization especially in an environment like Kubernetes where things can be very ephemeral It's critical to be able to correlate observability data both temporally so you can see what's happening at specific times to specific containers But also relationally so that you understand what containers are running for each pod on which nodes All this needs to also happen automatically in real time on the fly so you can troubleshoot an incident quickly and in context This will allow you to resolve an incident much faster Lastly, as you're choosing solution in this space, it's important to look at solutions that provide out of the box opinionated content That can provide insights into your Kubernetes infrastructure The reason is that really this type of out of box content is as important as the flexibility of the platform And that's because you want to accelerate your visibility into the Kubernetes infrastructure so you can get the value from the solution, frankly, day one You also want a platform that provides actionable insights and alerts that aid in your root cause analysis The last thing you want is your observability platform to overwhelm you with information It should provide you with key insights about your Kubernetes environments and then allow you to leverage this discovered relationships that are there to aid you in the root cause analysis and really prioritization of the issues Well, with all those concepts reviewed, let's dive into observe and look at how this can be accomplished in a live environment So as we get into the demo portion of the session today, I wanted to take a minute to talk about some of the key concepts around the observer platform And really to discuss how they address some of the requirements that I talked about on the earlier slide, which is presented Let me switch over to our data lake you very quickly before we talk about some of these concepts We believe that observability is a data problem. In fact, we've based our solution from the ground up on a data lake, in this case, snowflake And what this provides is really an ability to separate storage from compute. Of course, what does that mean to you is you're doing your observability on your Kubernetes clusters? Well, what it means is that you can send us all of your observability data, your metrics events, your logs, your traces, and store all that data at incredibly low cost Observe will ingest that data, compress that data, and store it into S3 buckets full visibility, full fidelity, and do so at up to 13 months for data retention or frankly more if you wish Of course, by separating storage and compute, it also allows us to optimize the compute that's required to store, analyze, and frankly present that data to you So that we can provide you with tremendous performance, even with petabytes of data coming in per day, and still keep that cost of analyzing and presenting that data to you low Now you might be wondering, well, how does observe capture data from your Kubernetes clusters? Well, observe is leveraging all open standards based collection techniques such as permethias, open telemetry, fluent bit, et cetera, et cetera, to capture your information from your Kubernetes environment. These are all easily deployed by home charts and deployed as Damon sets in your environment. Now once that data comes in, observe will go ahead and take the stream of data coming in and curate that into a variety of important data sets and more critically add correlations and relationships for you automatically This object view that we have here, in fact, when we focus on more of like a lineage view and then take a look at Kubernetes gives you an ability to kind of visualize this in an easy way. We can see as that data comes in, we're going to take that raw data and break it up into the different pod metrics, container logs, API updates, et cetera, et cetera, and then not just create these different data sets, but also create the different relationships. So for instance, let's take something like a container here is we focus on container, we can take a look at the variety of different relationships that container has with other parts of your Kubernetes environment. This is nothing that you need to be aware of. Observe will automatically take your speed stream of data in and then correlate it into the variety of different data sets and create these relationships. You can even follow these relationships and we can do so really quickly here if we wanted to go from container to pod and then pod to something like node, you can see that not only are we keeping track of the different relationships for each of the different components, but as we follow the different links in the environment, we're also following the bread crumbs as we go from container to pod pod to node, et cetera, et cetera, understanding those relationships exist. Now as we get deeper into the actual demo itself, you'll see how these things relate to doing troubleshooting in analysis and your environment. Now there's one set of concepts that did want to also address before we get into the actual troubleshooting and scenario within the environment. And that's really to talk about how observe presents data. We've got two different areas in terms of how we present data applications, which are those opinionated views on data that allow you to get insights automatically in an opinionated way and then our investigate views are explorers, which allow you to take a look at your metrics logs and traces and do so in an unapunited way. As you're exploring your telemetry information. Now, of course, we want to start off with the applications you so I'm going to switch over to applications here. And as you can see observe has a variety of different prebuilt applications that are within observe and of course a whole set of a catalog of additional applications that you can install based upon the environment that you have in this environment. We can see that we are running Kubernetes within AWS. We have a number of EC to virtual machines so we have host monitor during install as well. And we're also monitoring with open telemetry. Now as we take a look at something like Kubernetes, if we were a cyber reliability engineer, that might be a place that we live and we might want to take a look more at that content. Before we jump into the actual content itself, let's view the variety of content that's actually provided. So for Kubernetes, we provide a number of different prebuilt monitors and alerts to alert you on any of the different issues that are occurring within your environment. And also again, curate the data that coming in the from Kubernetes into different data sets and provide a variety of different prebuilt dashboards and opinionated views for you. If we go ahead to open the application, we could of course open the application directly. It will take us to that landing page for Kubernetes, which of course is the high level view of the clusters year environment. In this case, we just have one cluster in this environment. It's running a simple application we call our cars application that's providing effectively an e-commerce site for you to access and order cars from online in the environment. Let's take a step back here and take a look at this environment and imagine we're a site reliability engineer. As an SRE may be responsible for Kubernetes, we would want to get critical information about the Kubernetes infrastructure and also be made aware when issues are occurring. Of course, this landing page is providing just that. It's showing you the number of nodes and pods and deployments and namespaces in the environment, as well as the high level metric information, the CPU memory utilization, the number of throttle pods, unhealthy nodes, etc. And of course, it's going to identify when things are unhealthy as well, because this is a pin and opinionated view. It's telling us here that we have a number of unhealthy pods in the environment, and we might want to drill down and identify what's going on with the SRE and healthy pods. So, if we were to have a summary that's responsible for this infrastructure, that might be a key thing that we would do within our day to day, although I wouldn't necessarily expect them to be staring at this dashboard 24 by seven, but we can talk about alerts and events in a minute. So, here, let's say we were in this dashboard and we did want to drill down in these unhealthy pods, all of the views within observe are actionable. In fact, we can see a further listing of the unhealthy pods down below here. And if we wanted to drill in and start doing analysis on these unhealthy pods and frankly just these unhealthy pods in this environment, we can click up in the upper right here and say that we want to drill down into that pod data set to get more information. So, what we can do is we land within the dashboard itself. It's going to very similarly provide us the high level information, not about all pods, but of course, contextually, just the unhealthy pods that we were drilling down in and not. In fact, if we take a look here, we can see that right now those pods themselves are not actually running in this environment. And this, of course, is due to the temporal nature of observe in monitoring these pods, which in this environment could be really a failure on the environment. So, if we actually drag the time scrub over, we can see when specific pods are running. And see the change. So, it looks like in fact there was maybe a deployment that happened here earlier during the day, about two and a half hours ago. And then of course, if we go down a little bit further here, we can see key information about things like the memory, the CPU usage and the environment. And we can take a look here and see some somewhat problematic patterns in this data where we're seeing that typical saw to pattern. And we're also seeing over here a restart count continuing to take up in this environment. So, it looks like this pod after the deployment is starting to have an issue when we're starting to see restarts in the environment. In fact, if we wanted to visualize this from a temporal perspective, we could take a look and say instead of taking a look at which ones are active, we could take a look at restarts count. And again, grab that time scrubber and see when the specific pods were running. And of course, as we drag along, we can see the pod going rather and rather as the restart count is taking up here higher and higher. Of course, we've identified that there are some issues in the environment, we see that restarts are happening, but these metrics are great from a high level perspective to understand that events are occurring or specific metrics like resources might be impacting that. One thing we know, however, is that this isn't even approaching the amount of memory or CPU that's allocated to this system. So we not know it's not necessarily a quote unquote resource issue that's causing these restarts to happen. So we might want to do is drill down a little bit further into the associated telemetry data about this environment. By clicking over to logs and events, we can take a look at any of the additional telemetry information that's related to this environment. And of course, observe because of these graph links relationships are going to take and bring in any of the associated data sets, the application logs, the container logs, the web logs, pod updates, events that happen in environment. So there are frankly notifications that observe itself has sent out. They're all here at your fingertips to take a look at that information. And in fact, if we take a look at those notifications, we can see us in fact, we do see errors in alerts that were sent out by observe maybe into your Slack channel about plots frequently restarting or errors in the container logs where this is something we might want to drill into. And of course, we're already notifying people proactively of this issue in event. Since we're already here, instead of drilling down in notifications, it's where we see the pods are frequently frequently restarting. We can drill directly into the container or application logs and start doing analysis on any of the logs that are in this environment. So for instance, if we wanted to take a look at the errors in the container logs. We can go ahead and filter and see any of the errors that are occurring in these container logs quickly and easily to see what's going on in this environment. What we do see here is that there are a number of different errors. It looks like we're seeing an error here about a web application that was stopped threads not being reviewed over time and potentially a probable memory leak. We know that memory isn't necessarily an issue. We also see some other errors and exceptions here from the application itself. In fact, we can see that these are all from the app server container that's running within the pod. And we can see some specific issues around span IDs and exception messages around the errors decoding car of the way. Now, of course, these are the container logs. We may have actually pulled the application logs from these. In fact, we have so if we switch over the application logs and filter for errors here. We'll very likely see those same errors and see them in a much more process context. So we want to see errors. So it doesn't contain errors. Sorry about that. So if we drill in here, we can see now, yes, these are the errors code and cars array. We're seeing the specific user session, the span and trace ID and in fact the actual stack trace. Now, as a site reliability engineer, I may or may not be a developer and may know may or may not know what this specific exception is happy is and how it relates to the specific service. But if I needed to involve others, I could take this data. Maybe I want to go ahead and copy this link to this, including all the data, set that up into my Slack and then invite additional developers to kind of to really and on the issue. Now, a developer themselves could even take this context automatically open up this copy link. We'll take a take you directly in context to that specific view that I the SRE was viewing so they can do deeper analysis on the specific error. Now, of course, we have a potential would cause, but as a developer, we want to go further again, we might want to use something like a graph link and drill into the actual trace or the spans of those transactions. And we have a deeper information about the specific error, where it's happening in context of what transactions happening. And we of course can drill into that data within this specific transaction and get deeper information. Now, of course, I'm going to stick with being in SRE and one of the things that we talked about earlier is, you know, we might want to include others in the analysis of this. And of course, it'd be very helpful to understand who, what developers are either working on this code or frankly, since we saw that the deployment, trying to understand what builds happened and what builds were rolled out in this environment and by who who made the commit of the code. Because let's face it, about 50% of the time it's been proven and also reviewed this again and again that changes are often the root cause of issues in the environment. Now, of course, within observe, graph link is incredibly powerful, so we could actually ask this question. If we wanted to, we could go up to graph link here and say, hey, I wanted to see all the builds in this environment. That happened in the last four hours and see during that roll out what were the builds and who was involved in that build process, who were the developers that made commits or changes. Now, you might want to stop me and say Brian, wait a second, graph link in the linking is wonderful, but how did you get from problematic pod to the builds that were happening in that. Well, as I mentioned earlier, we're monitoring a variety of different to our training this environment. And if we take a look at the graph link here, we can actually follow the breadcrumbs of how we got from pod all the way to builds. We see that pods are related to containers, containers have images images have a Jenkins image Jenkins mapping, which is basically how it's being deployed as that image in the environment. And of course, Jenkins does the bills, so we have the build information. Now, this has done obviously through an integration to Jenkins to bring this information in, but this is all correlated automatically for you. So when you want to ask the question, instead of going to Jenkins to find out that information, you can go right with an observed use graph link and go right to the specific builds in this environment. And here we see the builds. In fact, we can see that this is a small change to caching code. Now, we might want to take it a step further and understand, well, caching that certainly could relate to an array within this this environment. We might want to take graph link one step further and say, hey, let's see who made the commit to this so I can include them in the analysis of this specific recalls. So now not only do we have the recalls, but maybe we can bring in the specific resource that made this change and understand, is it impacting in this this definitively the recalls, is this the small change of caching code. Well, here is the specific commit. We can see Tom bachelor was the author who actually made that commit. In fact, if we wanted to, we want to get really interesting, we can even follow this and take a look at the code changes within the GitHub environment itself to take a look at the code. Now, I want to take a minute and switch over to some of those explorer views so you can get an understanding of how you would work with your telemetry data and let's call it a more unimpeded way, as well as address some of those key criteria that I talked about earlier around the flexibility of ingest and the ability to work with schema on demand. So we kind of take a look here, I'm switching over to the container logs within our log explorer view. We've got a number of different container logs and of course we have the ability to filter down in and in fact, let's take a look at how we would process these container logs to create, let's say an app server log out of the container logs that are coming in. So we have the log data coming in here, we have span and trace data, we've got a whole host of other things that are coming in from the actual container logs. We might want to actually extract some of this data to leverage sort of schema on demand to take some of the information, important information we might want to add here. So let's go ahead and let's grab and extract the span, the trace ID. Let's extract the method. The level. And maybe the session ID. And then maybe the message as well, those are some some key fields that we might want to pull out and extract from the container logs as we're creating sort of like our subset of that in the application logs. Now you can see all that data is pulled out on demand. And then we can go even further however, let's say for instance we have session ID, but we want to pull the session identifier over from the actual session ID table. Well, we could go deep into this into towards creating a new data set by just opening this up quickly in a worksheet. From our explorer view and in this worksheet, taking that ID that we're looking at here in session ID, saying hey, we want to go ahead and pull in data and link this from another data set. In this case, the user session data set coming over from the web server. We want to link based on the session ID because we have a session ID and both hit apply and now those two data sets are linked. And that graph link relationship, but we've actually pulled over the session identifier, the user ID from that user session table quickly easily. It's here if we wanted to pull other information from that table that session ID table, we could go ahead and bring in additional data, let's say for instance, someone at a related fields. And for username not only do I want the username, but I also want to know the country they're coming from. That easy. Now I'll add into this new sort of data set that we're creating here. We have the actual user. Country as well. So we now quickly have that information go even further into creating these links. In fact, we can even use compound keys like let's say we want trace ID and span ID and we want to go ahead and relate that to the span data set. Trace IDs trace ID span ID span ID hit apply and we now have that complex join to that span data set. So it has to have that trace and span ID and now we have a direct linkage to the open telemetry stand data. And that's how quick and easy it is to create your own custom correlations here within observe using schema on demand. Now, of course, the log explorer and the other explorers are just good for creating and working with schema on demand. Also, of course, very actionable for you to be working with your data. So for instance, we have something we're filtered down to ask for your data. We might want to do the same thing and take a look at errors in the environment. We hit run. Now we're taking a look at just the error messages or the error logs messages that are coming in from this environment. Now, we might want to go ahead and take a look at from a different perspective, like maybe visualize this data, being able to see that data over time. In fact, let's take a look not just the last 15 minutes. Let's take a look at the last 24 hours of data. So taking a look at the errors that are happening over a time. In fact, we might want to even go further by saying, hey, we wanted to have this broken down by container. In fact, so we don't just want to take a look at the overall error trends here. We want to actually have it broken down by container. I'll hit run now and now we'll have it broken down by container as well. Great visualization we might want as we're doing or creating a dashboard. And of course, once we have that visualization that we were looking for here, let's say this break break down by container on all of your error messages. We can just quickly add this right to a dashboard. So this would be a great start to a custom dashboard and custom dashboarding is very easy within observe. Now, maybe we don't want a custom dashboard. Maybe we want to actually use this data instead of a monitor for creating again those contextual events and alerts being able to drill down in. Creating a monitor is equally easy. You can just say I want to create a monitor. And now we have that great trend data that preview of that data and start creating a monitor. Let's say, for instance, during the last five minutes, if we're seeing more than, I don't know, say 30 errors during that specific time frame. I want to understand and understand across the last 24 hours whenever that happens. Identify that and show me when that alert would be fired. In fact, we can see that visually represented above. We see the actual visualization. We see when the errors and events happen. And of course, down below, we can see the specific events when they'd occurred when across threshold. This is a little noisy, but maybe we're okay with that. And of course, we would set up a new action like sending out an email or a Slack or sending an event, a major duty when these things occur. And then we're often running, saving off this alert for again, getting that contextual and set for information when again errors or events are going over as well. We'll stop right now because that was quite a bit. We've gone through the full demo. We want to see if we have any questions. Sure. There we go. Thank you, Brian, for the demo and presentation. I'll take a look at some of the questions coming in. The first one I see here is do you support other cloud versions of Kubernetes like Azure Kubernetes service? Yes, we absolutely do support really any version of Kubernetes that you have from the different cloud providers to on premises to even if you have something like a, you know, a commercialized version of Kubernetes. Excellent. Can we just one moment? The next question I see is how are you pulling in information from Jenkins and GitHub? So we have again, as I stated earlier, those applications, they have a set of integrations or pollers for something like a Jenkins and GitHub to actually pull for those pieces of information to build information, the commit information, etc. So you're bringing that important contextual data from your CICD pipeline. So it's a pretty easy and simple setup. And then you can bring in that data and have it directly correlated into the rest of your telemetry. Awesome. Give me just one moment. This question is saying we subscribe to configuration as code. How do you support that? And can we use terraform to deploy your agents? Yeah, and so observe also as a big believer in configuration as code. So I mentioned earlier in terms of deploying any of the agents in your environment or any of the collector in collection in your environment, you could use something like a home chart because Kubernetes. That's a main way for deploying within the environment. But we also provide terraform providers for deploying the same agents that's right into the environment. And again, we do subscribe to configuration as codes. If you want to store any of this configuration in like let's say your GitHub repository and then utilize something like terraform for deployment, that's something it's 100% supported. Excellent. If anybody in the audience has further questions, please type them into the question window. I'll give it just a few seconds here Brian, as I think we've got the three top questions answered. Looks like that's it for the questions. So our next webinar will be enhanced security through observability with the observed basic threat Intel app. This webinar will occur on December 7th at 10 a.m. Pacific time. This concludes our webinar and I'd like to thank everybody for joining us today. Thank you, Brian. Thank you.