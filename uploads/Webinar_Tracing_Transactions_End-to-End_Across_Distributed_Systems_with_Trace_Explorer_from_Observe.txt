 Hello and welcome to the Observability Trends webinar series. I'm Grant Swanson your host and today we will be discussing our latest product launch Trace Explorer. We'll kick off with a short presentation, followed by a live product demo. Feel free to type any questions into the questions window at any time during the webinar. All questions will be answered via email immediately after the session concludes. Everyone who registered for the webinar will receive a link to the recording and a link to the upcoming workshop. Now I'd like to introduce our guest speaker, director of product management, Rakesh Gupta. Welcome Rakesh. Thank you Grant. Hey everyone, I'm Rakesh. I'm the product manager for distributed tracing here at Observe. And I'll be talking today about what distributed tracing is and what it's used for, how observed handles distributed tracing data and how you can get started with tracing in the Observe platform today. So a little bit about me. I'm based in the San Francisco Bay area. I have a little more than a decade of product management experience. Most of that is in observability. Most recently prior to Observe I was at LightStep for almost three years, both free and post acquisition. LightStep is a distributed tracing product and for those who are not familiar, it was founded by the same founders as open telemetry. Prior to LightStep I was at AppLanamics for about four years. Again joined free acquisition and stayed on after the acquisition. I initially was working on the dynamic language agents. So that was known in Python but also expanded to Ruby to go as well. I did that for a couple of years and then moved on to the APM platform at AppLanamics. So extending the data model so it would work with more distributed systems and more modern environments and architectures. So overall I've had the chance to work really closely with dozens of customers in a variety of architectures, a variety of industries as well and to really help them realize the value of tracing data. But also when not to use it, when is it not a good idea to solve a problem with tracing? And also just to help them solve business problems with it. So let's kick things off with an overview of what distributed tracing is and who uses it. So I've seen a variety of personas that make use of tracing data but there is definitely one that stands out and that's developers. So the typical scenario is customers are saying the experience is slow or they had an error but the user requests coming in are flowing through dozens or hundreds of microservices running on all sorts of architectures. If you're trying to debug a bad request, the questions you're answering are where is the issue? Where did this fail? Was it upstream of me? Was it downstream of me? Is it my own code? I've worked with dozens of customers where these are the needs that their developers have. And I've also heard from customers that adding tracing data or answering these questions with tracing data can cut down the meantime to resolution from hours or days to even as low as 15 minutes just because it brings together so much contacts in a single place. And you can use tracing data to model business services and business objects which other folks in the org outside of developers may find useful. But that's for another webinar. So I say a tune for that. So if you're new to tracing and especially for coming from a logs background and you are super familiar with traces, no worries, this is going to give you a baseline of knowledge. So a trace is a representation of a single end-to-end user request flowing through a distributed system. The trace is made up of a bunch of spans, each of which has the trace ID. And each span represents some bit of work done by some part of some microservice. And it has a bunch of metadata saying what work it did, how long it took, what resources were involved in the doing of that work. And you can kind of think of a span as a log with a trace ID on it as well as some context about who is pyramid. And so that's really what the trace instrumentation is doing that makes it distinct from pure logging is it's propagating that trace ID across the whole request lifecycle, creating spans and adding attributes and that trace ID to them to make them useful for debug them. And again, this is the very basics of distributed tracing. There's much more to unpack, but this should be enough to follow along for the rest of the presentation and also just to get started with using distributed tracing at observe. So let's talk about in a little bit more detail how are people actually using tracing data. So let's start on the left with number one, looking at an individual trace. So this particular request went bad. I want to inspect it in great detail to find out exactly what the call structure was, where the time was spent, what services were involved, what infrastructure those services are running on. I need to get to a narrative that tells me exactly what happened with this request. And trace data is a fantastic place to start for that. With the trace data, you can say, if flow through these services, it hit this endpoint and then this service and then for example, it made a thousand database calls or it made one database call that was very slow. Or it called an external API and that was slow like your payment processor or it hit an inefficient code path or we ran two things in sequence that could have been run in parallel. Or all of these things happened in a single request and it was just death by a thousand cuts. And that's great information because now I can go and debug this request and I can make it faster the next time somebody calls it. So now we go to the right for number two, which is basically searching across all the requests coming into the system or just to my service that I own as a developer and refining that search to figure out what groups of requests are slower or more error prone than others. Maybe requests with the latest version are slower than requests with earlier versions or requests that ran on a particular Kubernetes pod all errored out but requests that ran on all the other Kubernetes pods are fine. And really what you're trying to do is form and test hypotheses about which of these populations of requests has the issue so you can narrow down to just a small set of traces and look at those and really start to understand more deeply what's going on. And there are other needs that that users have when it comes to distributed tracing but these are definitely needs that ranked at the top and these are really key uses of distributed tracing. So now with that sort of baseline knowledge, let's talk about observe and observe distributed tracing solution. So how do we handle tracing data? So the first thing to note is that we fully support open telemetry and we have an API endpoint that you can send traces to that's fully compatible with open telemetry tracing data. So take anything that corresponds to the open telemetry spec for tracing and send it to that endpoint and we'll be able to accept that data. We also provide a number of tools and scripts to really just help instrument your apps and get that data into observed. And so some of these things are a guided setup flow in the product. So if you use our free trial, it's one of the very first things that you're going to see as soon as the free trial loads is a couple of different flows for basically just getting data in will help you, you know, as soon as you start. We also offer a Helm chart. So if you're in a Kubernetes environment, you can use that. We also have scripts and examples in GitHub that basically show how to use the hotel auto instrumentation, which exists for a number of really popular languages to get your app instrumented automatically with tracing without having to make any code changes and get that data into observed really easily as well. And so I also wanted to show some of the ways that we are making use of open telemetry data. You can see some of the tracing features that our product has and which hotel data is used to power that feature. In most cases, the auto instrumentation libraries for hotel are going to automatically collect this data. So you don't have to worry about collecting it yourself. But if you are using custom instrumentation, you can use this as a guide to plan your instrumentation in order to make the most use out of our product features to solve your use cases. You can also find this table in our docs and I'll just take some examples. So and we'll go through this live in the product as well. But if you are doing a search and you want to compare versions or you want it to filter just a particular environment or you want to see which populations of spans or traces are erroring, you can do that using the filters that we have built into our trace search feature and the actual data that you're filtering on basically comes from these open telemetry attributes. So now let's pivot over into the product and actually see some of these tracing workflows in action. So here we've instrumented the open telemetry demo application, which is an e-commerce service and it's sending its tracing data to observe into this tenant here. So we've got the trace explorer open. You just navigate to it via the left nav. And when you search for traces and observe, we return a list of traces that match your query as well as some summary stats about the traces that we return. So for example, how many spans we return over the query window that count of errors in the query window, which in this case is 60 minutes, the distribution of latency across all the requests in the window. And then we also have how the latency has changed over time. So you can use these graphs to basically hone in on the traces of interest so you can start your debugging. For example, if I go to this duration chart and I group it by version, what I'm going to see is that requests with version 1.6.0 are consistently exhibiting more latency than previous versions. And I might want to filter my query to just those requests so I can dive deeper into what's going on. So there's two ways that I can do that. I can either use the filter bar up here and you just go ahead and type in service version people to 1.6.0 and a lot of correct for me. That's one way to do it. Another way to do it is I can use these facet filters over here on the left and we can go down and find service version 1.6.0 and we'll go ahead and check that and then it'll automatically rerun my query to now just look at requests with version 1.6.0. So remember we saw higher latency so I might want to filter my list down here to just the slow requests and so I can do that by going back to the distribution chart and just scrubbing the latency distribution here, the higher end of the latency distribution. So now my list of results down here is going to contain just the higher latency traces. So let's now click on one to examine it in greater detail. So earlier today I found a good example of one to really take a look at here and now we're looking at a flame chart for this trace. And what a flame chart is, it's a visualization that shows a number of things. It shows all the operations, so all the spans that executed as part of this trace over time, as well as the parent child relationships from top to bottom of those spans. And there's a lot of information about this request that we can glean from this flame chart. So for example, what are all the services that were called in this request? We can open up the legend over here and basically see that that's what these colors correspond to. So we can just look at this flame chart and instantly see that it started in the front end. It went to the checkout service, checkout service made a call to shipping service later on, the checkout service made a call to the email service and so on. We can also see the full call chain and the sequence of events and any parallelism that has occurred just by looking at how these operations are laid out here in the flame chart. If there were any error spans, those would show up as bright red. We don't have any errors in this particular trace. Now let's go ahead and dive into one of these spans in particular and see what information we have about each one. So I clicked into this place order span and now we go down to this list of attributes. We can see all of the attributes on this span. So for example, the Kubernetes infrastructure cluster namespace and pod and also node. We see that here as well and deployment and so we can tell that this span executed in a Kubernetes environment, we can see that it was a GRPC call. We can see that it ran on Alpine Linux. We have the Linux distribution here. We can see that it ran on GOG that this is a GO microservice that this operation is executing on. So there's a bunch of information that we can glean from looking at these attributes. We also have span events and span events are kind of like logs that are emitted by open telemetry tracing instrumentation that are associated with spans as well. And in this spans events, we have a couple of custom events which give us more information about this particular request. So we have a tracking ID and we have a transaction ID. And this is the type of information that we can use to make other correlations with business context and other parts of observe. And if this was an error span, we would also have a span event here that basically contains the statutes for that exception. So you would just click on here and be able to view the full statutes. So let's go back to the attributes. And let's say I have a hypothesis that I've looked at a couple traces now and I think, well, this pod name keeps showing up. And I want to basically test to see, is it actually spans with this pod name? Is this pod name actually the cause of any particular slowness in my application? So the way that I can test this hypothesis is I can select it here and then I can click show similar filtered spans. And so what this does is it will open a new trace explorer search and it will just search for spans that executed on that particular pod. And now I can sort that population by latency. I can look at traces that just contains spans that contain that ran on that particular pod and I can quickly test that hypothesis. A second thing that we can do to continue our investigation is we can jump over to logs that are associated with the trace with this button up here, view logs related to the trace. And so what that's going to do is it's going to open up a log explorer search for basically any logs that have that trace ID in them. And we can continue to see in our log data, maybe these logs were not even emitted by open telemetry. If there are any other signals, maybe it's application logs, maybe it's infrastructure logs or anything that might be associated with this request, we can continue our search from there. So at this point, I have a pretty good picture of what happened in this particular request. I can tell the story of what happened. I can refine my search through the show similar filter spans. I can jump off to logs related to this trace to continue my investigation or I might even have enough information at this point to now go and debug this issue or go and optimize this request. And that wraps up our introduction to trace. Excellent. Thank you, Rekesh. I wanted to highlight a few points about trace explorer. Customers of observed get industry leading retention durations where 100% of traces are stored for 13 months by default. There are no limits on trace duration. And you can quickly visualize all spans for a trace. As Rekesh mentioned, if you go to our website, we have a fully featured free trial offer that includes our long explorer, metric explorer, trace explorer, dashboards and alerts. We also have a pricing calculator available on our website. Thank you for joining us today as this concludes our session.