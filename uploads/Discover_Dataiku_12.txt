 Hi, everybody. This is Dan Darnell, and I'm Christina Schau. And we're from the product team at DataIQ. Today, we're going to give you a quick tour of our latest release, DataIQ 12. But before we go and look at what's new, let's take a quick look at our last release, DataIQ 11. Now, in this release, we had some really great features, some of which really get extended and enhanced in 12. So it's worth taking a look. For example, engaging with your expert technical community. You're going to see some of that in the new release. Things like code studios, which allowed us to integrate better with the IDE's that your technical community are. And they'll flow integration with continued to work on that. So you could use these open source frameworks and leverage open source frameworks within your business. A new feature store, a managed labeling systems. All of these are great features that really help you build your AI initiatives. In that next category, we're helping you empower your skilled workforce. So these are the people out in the business who you want to take advantage of advanced analytics and AI technology. And there you see great features in your visual time series. So they don't need to be a super coder in order to do a time series analysis. Things like outcome optimization. Again, lots of great features there. And if you didn't see this release, I encourage you to go take a look at some of the videos that we have and the content around the release. And last but not least, in this release, we help expand confidence and trust with executives through things like the flow document generator. So that's taking a snapshot of the entire project at a given point in time to help build trust in that project. And you're going to see trust as a key theme that we're talking about in version 12. So before we get into the new features, some context, right. So we're all hearing and reading about the latest innovation in AI in particular, large language models or LLM's like chat GPT. At the same time, we're thinking about this great promise of these new models and how can we use them. The news is full of cautionary tales about people who accidentally disclosed confidential information or used code written by these tools and didn't have great results. So naturally everyone from executives to analysts is wondering if they should take advantage of these technologies or wait. And this challenge though is not new. I mean, as we think about being in businesses that utilize technology. You know, this is something we always have to face. So I'll admit, this is a unique moment though LLMs are, I believe, a transformative technology. You know, thinking about how you're going to use them is important, but adopting new technology to gain advantage or remain competitive is something that we have to do as part of our business. And there will always be risks that we have to manage. So the question we should be asking ourselves is how can we use the latest technology in a way where we still have confidence in our people, our processes and outcomes. Now, large language models are an excellent example where an AI platform like data I could makes all the difference in how you respond. When you have a really I platform like data I could that you're building on top of you're not starting from scratch or flying blind. With a platform you naturally build on top of what others have already done. And if you're using data I could today, you're already doing this. You're taking advantage of the latest and greatest open source models and technologies cloud AI services and other innovations. Now in an AI platform, you spend time on the big stuff, not on the little things. So you have time to think about how you're going to use LLMs safely and new technology safely instead of writing blue code or how am I going to spin up an environment. And with an AI platform, you take advantage of new innovation because you're operating in kind of a safe environment, right, with people and processes that you trust and that you know already works because you've done it before. And ultimately, you're more comfortable doing things where you perceive this risk. You're more comfortable building things. You could not have done on your own as an individual or a company. So let's take a look at the kind of at the next release data I could 12. Now data I could use an AI platform that allows our customers to take advantage of those latest innovations and do it with confidence. And our latest release really exemplifies this with new features that help increase transparency into AI projects so that everyone can understand what's going on and build trust and AI outputs as they prepare to use them in the business. Now, it may not be as sexy as LLMs, but a huge part of what we need to be successful with AI is reusable components and repeatable processes. So we provide standardized components to ensure your teams use best practices and proven and approved modules to build projects. And last but not least, we're helping you centralize operations so you can consistently deploy, manage and govern projects and deliver more projects and more success from your AI initiatives. So enough from me. Let's take a look at some of these fantastic new features with Christina. Thanks, Dan, for that introduction. For the remainder of this session, what we're going to do is give you a quick tour of some of the great new features we are introducing with data I could 12. And they are kind of bucketed in those three groups that Dan just mentioned. So we're going to walk through them one at a time. We'll show you some demos. And before I dig into the features you see on this slide, I do want to mention that there's a lot more to the release than just the nine that we've highlighted. We've made systematic improvements to every one of our key capabilities. So you can see here we won't do demos, but lots of improvements to data operations, workflow management, as you saw on 11, we wanted to make things easier to move from request to administrators back to the business and so forth. We've introduced profit support for that visual time series forecasting that Dan mentioned. We have released a new plugin for GPT for so lots of stuff happening here. And what we're going to do is start with that first bucket of what are some of the new features to increase transparency and explainability for data scientists for business users for people who are domain experts who need to make any decision support. So I'm going to cover auto feature generation first, which is really a way to simplify the feature engineering process that usually is quite manual and takes a long time as a data scientist, Wrangles data encode to create the aggregations and transformations needed to create features for your machine learning model. Let's talk about this one first by showing you what it looks like in the product. Imagine you have three source tables and you know, classically in data, who you can choose your own adventure. Do I want to do some preparation to transformation? Then I want to do a join, then I want to pivot it, group it, win do it. So let's generate features recipe. We can do a lot of these things in one step in a more systematic way that other people can understand, even people who aren't coders. So here what I've done is I've created relationships between my three source data sets. You see in this case, we're trying to predict flight delays. So I have root details, many to one and a join key. I can add additional history to beef up my model. And here it automatically suggests all the candidate columns that I can pull into my auto feature generation from those enrichment data sets. I can turn the monitor off. And I can also choose what type of transformations I want to make to each of these features based on the type. And you can see here I can turn these off. And dynamically we see the number of transformations going down the number of new features changing in this dynamic counter. As always, because this runs on SQL databases, you can look at the code being generated for you behind the scenes without having to write this from scratch. So really great here time saver. If we look at the output data set. We can see we started with four columns in the original data set. Now we have 39. And it's very easy to profile and investigate all these new columns. We've even included automatic. And we've also included the descriptions, which help people understand the lineage of how that feature was generated. So you can see immediately how this would be useful for well trying to find more signal in the noise for their machine learning models. So that's auto feature generation. Give that a try in your projects. Next is universal feature importance. And this happens a bit later in the you know model and AI lifecycle. So what are the most important features which influence the predictions. But the problem is if you have a tree based model, well, there's one method for doing feature importance with that. If you have a non tree based model, or you've built a model outside of data, who in ML flow. It's difficult to have an apples to apples comparison across those to really understand which models the best. So in the universal feature importance, we've used a sharply implementation so that we can get you that universal view. So let's take a look at a model here. In this case, the use cases were trying to predict the resale price of a used car. So we've built the regression model. And over here in feature importance. Well, we've always had this genie feature importance for a tree based model because we're in a random forest. And what we've added is this sharply based feature importance with three three visualizations that will work on all different types of models. Built in data, who are not tree based or not. So you can see here absolute feature importance. And in this case, the type of transmission, the fuel type, these are important. So let's go further. Let's look at the directionality of their influence. We can see here it's a categorical feature for transmission. And I can see that automatic leads me to a higher resale than manual. Same thing for fuel type diesel commands more price resale than gas. But for a continuous variable, we can also make really interesting insights. For example, we can see seven seats. As I get higher in the seat number, why resale value goes up. And the opposite is true for the number of kilometers driven, which makes intuitive sense. Right. The more your car has been driven, the less you can charge for it on the back end. So to get even more depth, we go into feature dependence and we look at it at the value level. So let's look at those two that we just talked about. About 50,000 kilometers is the inflection point at which you start to see a negative influence on the price you can resell your car for. And how many seats can I have? Well, five, Spana neutral doesn't get you that much. Seven starts to get you into the positive range. So you can make very insightful conclusions. This really improves trust with understanding the model and trusting intuitively that it does what we expect. Finally, uplift modeling is the third one we have in this bucket. This is a causal ml approach causal predictions, which allows you to quantify cause and effect relationships. So if I do X, what is the quantifiable impact on the outcome? Why that I care about and we can do these measured experience experiments within data, whose visual UI. So if we look at an uplift modeling flow, let's say we're trying to decide, you know, we have limited budget this year. We're trying to reduce churn in our subscription based business. I have a promotion for a discount, but I only want to send it to the, I don't know, 20% of people for whom is going to make the most difference in whether or not they renew. I don't have the money to send it to everyone and it's counterproductive to do that. So when I create a causal prediction in our auto ml template, it looks the same we have design. We set up our experiment, the outcome I care about is the renewal, the treatment that I can toggle on and off is my discount that I would offer them. And so I set up my experiment, saying, you know, this is the controls the control class. I turn on some propensity modeling to make sure that in my experiment, my treatment is properly randomized so that I have a good experiment, a methodologically sound experiment and it's going to help me find out if I've set it up poorly. And I choose the type of algorithm that I want to use for my causal prediction. So this looks pretty familiar from our traditional ml, but we have new types of meta learners specific to causal ml. Well, we go look at the results. You can see here the classical view. And we can start to understand was our sample properly randomized. What does my treatment effect? What is the conditional average treatment effect? And start to understand, you know, like maybe my cut off is I only want to send it to people above 0.01. Those are the people for whom they have the largest impact if I send that out. So we have lots of ways for you to investigate these outputs and when we go back to a scored output, you can see here we can even get you to the next step, which is, okay, recommend that 20% for whom it's going to make the most difference. And that's what we've done here. We have a split as a business user. I can know I have this flow set up. It's going to tell me the 20% of people for whom I should release in that treatment to you. Now you can use this for marketing, but you could also see how this is useful for fundraising for political campaigns for clinical research and healthcare. Many more applications. Let's move on to the next set of features which really help people standardize components and processes and move faster by not wasting time recreating things that already exist or looking for answers. And so today we're going to talk about the help center, which is a trail guide to data, who it's a map that's on your shoulder all the time and it helps you understand where to go for help, where to go for inspiration, who to contact if you get stuck. So let's take a look at the help center. On every page, we have this question mark. It's always been there. You might not have ever clicked on it. But what it has is a whole menu of contextual recommendations based on what you're looking at at the moment for what might be helpful for you that moment. So you can see we have dynamic recommendations. I can search for things like this window recipe and it will pull up all the assets we have in all of the familiar places you go for information, the reference docs, the knowledge base, our blogs, our community forums and so forth. You also can get help. And one thing I really want to point out here because this is new, you probably probably haven't seen it yet is a developer guide. So if you're a coder and you want to know something about how the API works, you want sample code, it's all here. So this is a great place for those technical users to learn how to use data, who programmatically. If you still need to raise a support ticket, you've got a shortcut to it here. And then we have a lot of educational. Here's where you can access customer stories use cases. You can learn more about how data I could work. You can watch, you can look at sample projects. And you can browse some of our prebuilt industry and data, who solutions, which are starter templates for you to accelerate your work on key use cases. So those are really great to look at to get started to bootstrap your your solution. And of course, you can contribute to data, I go by giving us feedback and all those types of things. So this is really useful. It does change contextually. So you can see how if I go to a prepare recipe, these changed. If I go to something programmatic, the recommendations change. So definitely give this a try for upskilling. Data catalog is another place to save time by making it easier to reuse data sets that have been vetted that are curated that have maybe been owned by a data steward so that you can trust them. And you don't have to waste time recreating it from scratch. So in the applications menu, you always used to have this search CSS items. It was called the catalog. You still have that still has every artifact that's in your data, who instance. But additionally, you have a dedicated place to discover data. And data collections are lists that you can set up for your team. They're completely customizable grouping data sets by some kind of theme. So for example, you have finance data sets and you can go and you can browse these look at the metadata, look at the schema. And you can also look at the data steward that's been assigned. This is the data engineer or the data scientist who maybe built this table is responsible for maintaining it. This is your subject matter expert on the table. If it's been shared for you to use, just go ahead and use it into your project, select your target destination project. So it's that easy to shop for data. And in the data catalog, you also have your easy access to any of the connections that your company has connected externally, your snowflake, your cloud storage, you know, your local SQL databases, those can all be surgical here as well. So this is the great place to start when you're just starting a project and you want to shop for good data. I mentioned data, whose solutions a moment ago, really want to highlight these again because these are coming online all the time, not just with major releases, but we have introduced a lot of new ones in the last six months that you may not have noticed kind of hitting the shelves. So things like process mining, financial forecasting, credit scoring, product recommendations. Some of these are quite industry focused, summer transversal and horizontal. We definitely recommend going to our gallery, going to our website and browsing these to see if you can go from zero to 60 in the matter of hours instead of months. Okay, let's go back. Let's go to the final bucket, which is new features for centralized operations. I'm going to start with model overrides because this is a very exciting feature that gives you a lot of control when it comes to enforcing conditions in deployment. So let's say the way it works. Let's go back to my project here. Let's say I have a model where I know that for certain cases, I want the outcome that the business specifies to occur regardless of what the model predicts. Now I can input that subject matter knowledge into my model and at score time at inference time, make sure that we're kind of safely predicting within the boundaries of what we what we want. So the way it works is at design time, I'm going to specify the conditions and the outcome that I think is permissible. In this case with the car example again, if the car is broken, I want to cap the resell value at $10,000. But I don't want it to be zero either because I can always resell a car for parts. Now if it's functional, my range is much higher. And so I want my model to respect these boundaries. Now when I go to the results, you can see here in addition, in addition to diagnostics, we also have an overrides flag to let you know overrides were applied to this model. I can analyze what proportion of my test data hit the override rules that I set up. So in this case, we can tell that a lot more cars were functional than broken just based on the branches in the Sankey diagram. And I also tell of the ones that were broken, the vast majority, the prediction was actually in the range we wanted anyway, but there were, you know, there were how many year, three rows that had to be overridden to match the requirements that I said between 200 and 10,000. Other way to view it is looking at, you know, which ones meet the criteria without intervention versus need a specific override to be applied. So we need to be able to see what we mean in practice. For a model with overrides, once you score it, you have some fields which help you understand what's going on behind the scenes. So we see these rules that have been matched in this case, a functional car, we said it couldn't go above $40,000. The model suggested was $48,000, but we had capped it if you recall. So we did apply that override because the prediction changed is true. And if you look at the final prediction, it's actually at that max range, $40,000. If we look at a case where the car was broken, remember we had another rule for that. Well, the predicted price, resale price was actually $13,000, but we said, remember, it's capped at $10,000. And that's what you do see, in fact, is the prediction that's going to be delivered by an API, for example, if we're using this in an application. So this is kind of a practical understanding of what's happening behind the scenes for overrides. Couple of other things on operations. We've added new drift metrics. We've added more visibility into monitoring for endpoints that you've set up. So you can immediately see kind of where your project as a data science is being deployed and look at the health of it here. And we've also added in the deployer kind of easier ways to lift and shift projects from design to automation by doing things like propagating security permissions from individual accounts to system accounts. Automatically setting up an ML ops feedback loop in your flow in automation. Those types of steps are really going to be appreciated by operators to make sure they're doing the same things every time when it comes to setting up monitoring for ML ops. And then finally new governance views. We always try to improve our governance module each release such an important part of AI governance. And giving teams the visibility into how to prioritize their projects. So you'll recall we added the bundle registry last release. What we added this time are two new views on your govern projects. The first is the can ban view. Which gives you a really nice high level overview as an executive of where all your govern projects are in terms of their development lifecycle. And we've also been able to monitor and explore and not that many have made it to delivery. This gives me a good angle on what's happening over time. And we've also improved this value risk matrix. It used to be you only had value and risk. Now we can actually choose the X and Y axis. So I might want to look at things by feasibility. And I think that's the first thing that I want to talk about is the cost that by use case technical dimensions. You have a lot of flexibility to compare your projects in a standardized way against each other to prioritize resources. Okay, with that, that's kind of a high level tour of the major features. I'll kick it back over to Dan. Anything you want to say about what you've seen so far Dan. I mean, I as I look at this as a business person, you know, some of the things that stand out to me, I think the uplift modeling, you know, so useful for things like marketing. For and further areas where you need to figure out what treatment works. And I think like of people trying to figure out how can I continue to work in this environment with my current marketing budget. Well, something like that's going to help you figure out which who you should send an ad to that where it will actually matter in there and in helping them make a decision, right. So features like that today, I think are really relevant. And of course across all these, I think you can see, you know, how working within a strong platform like data, who really does help you have confidence in what you're doing. And you have all these kind of guardrails and all this visual experience. And you have people in the loop. That's the other thing that we can't forget about working in data IQ. Like a person is looking at all these things and it is involved. And while you are using automation here and there, you, you have, you know, smart people taking advantage of this technology. And kind of step by step going through a process. And especially with with our governance module, you know, on top of that, really helping to make sure that, you know, where a project is and that it's being reviewed approved and even signed off before it goes to production. So this release just has a ton of great features that I'm really excited about. And I think it should give people a lot of confidence. You know, as they're thinking about rolling out new AI projects. And as I said, if you're thinking about doing something like trying out LLMs, if you're on data IQ, you should have a lot of confidence that you can do that, you know, in a truss trusted and safe way. Great. So with that, this will conclude this overview of data IQ 12. But if you're eager to learn more and we hope that you are, please do watch the videos we have on the website. We'll have lots of articles published and a crash course that you can take to actually get your hands on these features yourself. And we do hope you'll tell us how they go in the data IQ community or you could hear your feedback. So with that, I think we can close. Thank you. Thanks everyone. Take care.