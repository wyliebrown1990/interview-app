 All right. Hello, everyone. Welcome to the Generative AI Group Camp session number two. Use cases and design patterns for Generative AI in the enterprise. We're super excited that you've all been chosen to join us today for discussion with SOPE DNA, Data I Cruz General Manager of Business Solutions, and Jonathan Crowther, and how to predict a analytics advisor. Just want to let you all know that you can move questions and the questions to a mission portal on the right side of your screen, and SOPE and Jonathan will be doing Q&A in the last 15 minutes of our session today. All right. I'm going to go ahead and pass the mic on the SOPE and Jonathan. Maybe you guys can kick us off by telling us a little bit more about ourselves. Yeah, thanks for the joy. And I think I'm going to be doing the introduction. Jonathan, it's fun for you. Actually, can I leave you the ground to start by introducing yourself? Absolutely, Sophie. And thanks for having me. My name is Jonathan Craven on the head of predictive analytics within the GPD organization within Pfizer. And my role is really about how can we start to look at different ways that we can accelerate our clinical trial delivery using data, and usually we say the more of Tuesdays, the data, the better, and then also build some solutions that will enable us to create a better end to end infrastructure for using data analytics and in the future machine learning AI, particularly with this topic, Generative AI. And so to quickly introduce myself, I'm Sophie. I lead a team within data IQ called business solutions where we look specifically at a number given industries on and looking at how our technology can further accelerate the usage of data and analytics and AI at delivering tangible business outcomes. And as a connection to that, so as part of my team, I have a mix of business professionals and data scientists. I think Jonathan is the help on your side. It's more than the data scientists and all of the side, right? Correct. Yeah, that's exactly it. So what has been happening over the past few weeks, months, is that ever since OpenAI has really been accessible. We have seen a sizable amount of questions arising from our existing customers from prospects pretty much everywhere around that technology. And I would say the most interesting part of it beyond the technical side is how to make relevant usage of that technology. How is it changing approaches to data? How is it changing approaches to analytics? Is it something completely new? Is it something that needs to be combined with the rest? And so as Charlie was explaining last week, there was an exploration with Florian Duetto and the other panelists I'm looking more at a high level vision around what the technology is about. Next week we will be dive much more on the pure product side. But here today, what we will want to try to explore together with Jonathan is what does it mean in terms of accelerating these types of outcomes is changing the game. Opening the ground to a new use cases or not. I'm going to be sharing a few slides like to send the scene as a few thoughts and and then we will see what we can derive from this in terms of learnings. On our side, when we started thinking about generative AI, one of the elements that was standing out is that as usual when we think about a new technology, it raises so many potential opening capabilities. Is it something I should put in the hands of my lawyers? Is it something that can actually help me reduce the number of people answering IT support tickets? Is it something that can significantly change how I interact with my customers? And so you have this vast ocean of possibilities out there, which then usually trickles down to a second question, which is around how would it be possible to deliver these? And what quickly came out when we were doing some brainstorming on our side, which was fueled by our own forward processes, but also by conversations we were having with our prospects and customers, was that there were a few different patterns that were standing out, and which seemed to be then driving potential for applications. As part of the first part of this webinar, we're going to be looking a little bit more about these and and Jonathan, what is really going to be very important for this is really getting your perspective around how you see these as being relevant, applicable, useful. As you will see on the screen, the first one that we identified and I think everyone identified it really quickly, which is around conversational self-sourced analytics. In a few words, what it means is that today, when we want to give access to a business professional to certain insights, you need to have these insights being packaged in a data set. So let's say, for instance, I want to give an overview around my targeted predictive maintenance calendar for all my machines, or I want to give access to my forecasted sales numbers for my different regions. And most of the times, I will then need to have it integrated in a specific information system for a vast number of professionals to consume it, or I will need to think about different dashboards that I want them to be able to consume. What we see today with generative AI is that actually you can break down this barrier by having much more of a conversational approach to digesting the information and asking questions for which answers and visualizations will be automatically generated. And here in that screen, you see something that we have done, which is on predictive maintenance. One of the beauties of that, like at least in terms of, you know, initial wow effect is that you can ask these questions in any given language, here it's in Japanese, and you actually do get the answers in that language. And Jennifer, did you get the conversation started? Is this something you have identified on your site and that you have tried using it Pfizer? This is something I'm really excited about, to be honest, and it comes from our experience a couple of years ago with, you know, COVID, the big C word we want to say too often. But in essence, one of the key things we had was we're very, very proficient at building dashboards. The problem we found was as we went to different leadership meetings or what we call light speed meetings to determine which direction do we go now, what's our next step so that we can keep on the the coding edge of the technology and of the, you know, tackling the pandemic. What we found was that the questions were being asked, we were always just that step behind in terms of what was needed in the dashboard. We did our best to predict what people were going to ask, but the reality was someone always came left to feel with a question that we weren't expecting. So this used to have to trickle down and then we'd say, yeah, we'll come back in two days and we'd scrramble to get the data, put it together, and then build a dashboard visualizations tested and send it out. What I found really valuable about this and we started to look at this prior to and Chachi beauty and all of the generative AI solutions. We were looking at how could we solve this graph technology and what this opens up for me is that when we're in a meeting now, if we can ask this question, we don't need to rely on six, seven different analysts, the leaders themselves can ask that question and we can have one person supporting it or the leaders themselves in a meeting outside of a meeting as there was a lot of things that we can do. And we're just asking, are there any other questions that we can ask about this? I think that's a tremendous value for us in the world that we are now trying to really accelerate clinical development moving left or right as we hit kind of important decision points within our day to day business. So being candid on the change management behind because I think we were as excited as you were on the potential when I was working with my team on this, I also found myself sometimes pretty much wondering what was the question I could actually ask to the machine because it was a bit being a bit more in front of a black and blank sheet of paper. So we started with an existing dashboard, you can plain because it doesn't answer your question, but here was a more like, hmm, what is the meaningful question I can ask it? Is this something that you have seen on your side? Yeah, so more and more, I'm really looking to work with the data scientists that we have in our team and they've been hammering it home to me like weekend weekend since all of this has come out about prompt engineering. And we actually tried this when we were building a kind of approach to use graph technology to ask these questions when we sent it out to the end users for testing, it was a colossal failure, which I'm happy about because it's important that we fail, but it failed because people are so used to using filters and the way we've created dashboards is this like filter step by step approach. So when we gave them a blank canvas and said ask what you want, they kind of had a panic attack and said, oh, I don't know what to do. So one of the key things for us that we've identified from this is that we need to be able to step back and really train the people who are going to use this. So I think the rollout immediately is not going to be pan organization, but it's going to be kind of stage by stage different groups and we'll prior to that rollout have kind of extensive training on this is the best way to ask a question. And interestingly this week, some very close colleagues of mine have been sending me random questions, so we have a very controlled rollout right now in Pfizer and they've been sending me random questions like ask it this, ask it that what's the answer quick, quick, and we're like, okay, calm down. What is it you're really trying to answer? So I think the key thing for us is going to be that training and you know, controlled rollout with people to make sure that we're getting the best use out of it. Also for for like an consistency of results as well, I worry that if we just leave it as an open canvas, we could have six different people coming back with six different responses. So we really want to make sure that we can be as consistent as possible. Yeah, and that was also what struck me on my side is what we're working on this. We were looking at topics like, you know, friends and financial forecasting. And so if you take a topic like, you still need to have people speaking exactly the same language based on the same type of underlying data analytics foundations, I think we'll go back to that of how I think this makes even more important, the resilience and the robustness of the underlying data. But I don't think it's going to kill the need of having keep your eyes that are crossly shared across an organization and some visualizations that are seen as being the the landmark that's a for this, but it also frees a lot to the capacity to answer and silly questions. Is it a bit of what you have in mind? Absolutely, I think it's going to be like a combined strategy where I mean, we're going to keep those dashboards. There's no way we're going to get rid of incredibly valuable for people to go in and have self serve, but I think sometimes we need people to think outside the box, particularly with like trying to come up with creative solutions. So we need to be able to give them a bit of freedom so that they're not tied in and also that we're not sending backwards and having to download excels and then manually do analysis as well. We want to get rid of that. We want to make sure that they have the analytics to their hands and that's really the goal. So it'll be a combined effort. The second pattern that we saw on our side when we were doing these experiments is and maybe it's the most obvious one, of course, is looking at how the most specifically LLMs is actually finally unlocking the door to all the information present in unstructured data. So all the, let's say the the area of documents intelligence and with a few different possibilities there, like the possibility that I'm putting there on the screen, which is like the analysis, for instance, here working in ESG and I need to be able to query inside documents with natural language, the type of information that I want to get from, or I'm an insurance company and I want to be able to explore a large amount of previously emitted contracts for the type of policies were involved. Which also creates some, let's say questions around how you actually make this happen considering the current limitations of LLMs. So it's more combining techniques. Similarly on within Pfizer, is this a space on which you see an opportunity and are you exploring that at the moment? Yeah, there's an awful lot of discussions about it. I think this is an interesting one because instead of people getting very excited saying, oh, we'll use it. We're actually seeing the opposite people say we need to be very cautious about this as you can imagine different industries of different levels of regulation, far more very, very highly regulated. So being able to, you know, just generate documents, I would love a situation we could just automate it end to end. But the reality is we need to make sure we have the right checks and balances in place. So like generation of these documents or materials from unstructured data while there is a very, you know, as an endless possibility of what we could do here and say. We also have to take into consideration the limitations that are there with LLMs as it is right now, if we take temperature into consideration. We can't let it be very creative about what it's saying because again, there's legal implications of that. There's compliance implications. So I think this is something we're really excited about. But at the same time, we're very cautious and we're not sure about this yet. We do want to do it. But I think we need to make sure we're very robust framework in place to make sure there's QC there and that, you know, it's really aligned with one our ethos and our patient centric kind of approaches. So it's probably less, how should I say, accelerated than maybe the analytics in your first design case, but it is something that I think over time we'll find more solutions will evolve that will support that with us. And I think it reminds me if you remember when it first came out, everyone was afraid about kids generating their homework. And what, it was about a week or so that we had someone was able to come in and say, oh, this essay was generated from from AI. So in reality, I think it will be a matter of time and probably somewhere we need to focus on would be like test driven development. How can we make sure we have a QC control in place and then generate the content so that we're kind of really making sure that it can be checked qualified and then work on the content generation. So that's probably the logical approach that we would take. But I don't know how you feel or other experiences you've seen from other industries. I think on other industries, it's reacting a little bit the same way and the natural power forward would be to think about how it can literally help a lot on the actually on the regulated side of the house, like on legal people and compliance people. One of the areas I've personally spent most time into was on the ESG side, so environmental social and governance topics. I think it's because I've been operating in that space since 2005 and since 2005, everyone has been thinking about the structured data that we get access to from providers is not accurate enough. If we need to find more information, there's so much more information that is available out there and it demands tapping into that unstructured data. And it's true. But there's few things that struck me when we did those experiments is I think that one a little bit similar to what we were saying before, I don't think it's going to be eliminating at all the forms I mentioned of things because you still do need the facts and you still need to have basic comparisons out of grades. The second is to your to your element about the temperature. I think there's a natural flat that there's a natural appeal using elements because you have the conversational approach, but there's a real question of how much you want them to generate an opinion. If you leave it on pure descriptive, it is an acceleration to using information that can be a mix of public and internal and I think I'm using internal information, it can be powerful accelerators. If you leave it to formulate an opinion, then it's then for as soon as this opinion is part of a critical business decision process. So in the example of seeking can be on an investment decision, for instance, or decision of giving a credit or loan to a given company. It triggers the point of how much can you trust that opinion, how much do you then challenge or not that opinion, how much do you reinforce it with other factors. So I think the potential definitely is there, how we are going to be, how organizations are going to be able to weave it in their day to day and how it's going to be delivering that value probably still needs to be tried. We have that issue for sure when we're designing a clinical trial, our first decision is which countries do we go to. So we have an awful lot of experience people in different countries in offices around the world. I've been more than welcome right now. And the reality is we go to them and say, what's the regulation in your country, what's this, what's that. And then at the end, we have somebody who aggregates all of that and then tries to make a decision. I think it will help to get and compile all of that unstructured, you know, kind of new on stage it together. So that's someone can look at it. But ultimately at the end, we need that one person who's just going to eyeball it and say, OK, I agree with this or I disagree. And that's we're still trying to figure out what that framework could look like. And that's just one small example. Yeah. And so it remains a topic that is more around augmenting individuals and replacing them because I can I think we can be honest among ourselves that it has been one of the fears ever since this technology has emerged out of the four pattern out of the four examples on giving on the screen. The one that we see with, for instance, the most tangible applicability and I know everyone has identified it is anything that would be linked more to chatbot type of approaches. Or when we put in there like the IT support ticket advisor, if you're a person working in IT support, you have a number of different tools that you need to provide answers to being able to much more quickly tap into all the documents provided by these software vendors in order to arrive with the right type of information. It makes full sense. Like this is just a powerful reduction of painful tasks and ultimately those types of technologies should be about that. So these are like the two first ones that we've seen. The third one, tightly connected and I think we already touched upon that is is is is not just about using information in in unstructured data. It's also about generating contents. One of the experiences that we did that I thought was actually quite appealing is and then you can see on the screen is it's tomorrow you're you so you have a predictive model that is forecasting your revenues and that's awesome. And it means that at the end of the day you're going to have charts. And these charts will tell you something around whether you're going to be hitting your numbers, you're going to be above your numbers, below your numbers. But in most cases actually you do want to have the voice over. So a financial analyst person will be spending quite an important amount of time dissecting through these different numbers and formulating an opinion. I think there's an opportunity to accelerate that and and also same thing, you know, to accelerate that in the number of different languages. We also see the potential of course on the marketing side. So I mean, I think you've touched upon it and I understand that on clinical trial selections that was typically a space on which you would be tightly monitoring how these technologies are used. Do you see this as being an opportunity maybe for other sides of the house within within Pfizer? Absolutely. I think it's it's applicable everywhere. Yes, there's regulation, but I think there's no reason why we can't deploy this to somebody to help them make their decision. And an example I think about is as we pick sites or clinical trial sites or hospitals, right, we send out feasibility surveys where we asked the hospital all these key questions about are you suitable? Are you, you know, is this a feasible study for you? And the nice bit is they send the surveys back now as you can imagine, we have one person who has to review that would maybe ten other people on the study team traditionally. So if we could actually use the LLM to generate this content and those comparisons given a set of parameters that we feel are like good approval parameters, then it could always give a recommendation to say pick this side over that side. And it really falls into something important if we're in Belgium as an example, we've to hospital to a very closely ranked in terms of performance or startup or whatever key KPI is that are important to us. It's that summary of data it can generate or analytics that would actually help us so or could help the analyst to say be more on bias because we also have people who work with specific sites and their personal relationships and hospitals that happens right so it's how do we really let the data or how do we use that data to help us kind of get over or maybe some of our biases or experiences to make sure we have the best site. And yeah, so that's for sure that's one part another part of the business where this can be really helpful as you say look at global clinical supply as an example a huge amount of data that we have there. This is a massive thing to forecast out where do we need to have drug you know do we need to scale up certain production lines those types of things and they're very very applicable and I think something that would be a massive benefit to the organization. And so the and so the last one well there's another one we will touch upon after that but the last main topic that we have identified is also the capacity to actually do either entity resolution or to structure outcomes from different types of text. As an example when you do when you have customer reviews coming in or when you have medical reports how do you make sure that you extract the right type of information from these in a faster manner with in the example we took here with with the right oversight with the right capacity to validate because I think again we're more looking at proving the impact of these technologies. If we go back to your example of clinical trials I can imagine that this is a space on which these types of approaches could actually be excessively important to maximizing the efficiency right. Yeah, we have the electronic health records are EHRs so when we are trying to like screen patients for studies will be looking for specific inclusion criteria you know what your weight is are you pregnant are you not pregnant and so each study has those specific criteria for inclusion exclusion. So being able to scan that and actually have a database of subjects which you can have in the UK or in the UK in the US of participants then essentially we can run a model like this and almost pre screen in a very very efficient manner so being able to look at where in the world where in a specific country we have a higher proportion of these patients is huge. The other part are things like medical research for instance so we might actually be looking for is this a valuable area for us to explore for further investment so this could be you know drugs that could be repurposed etc so we want to be able to scan through text and give some key parameters that we might want to say what is a good opportunity here like long covenants and example or anything like that you know so that is a massive area for us and it's completely applicable because we're spending a huge amount of time and we're going to be able to scan through text and give some key parameters that we might want to say what is a good opportunity here like long covenants and example or anything like that you know so that is a massive area for us and it's completely applicable because we're spending a huge amount of time and it's completely applicable because we're spending a huge amount of time and it's completely applicable because we're spending a huge amount of time and it's completely applicable to the number of times we're going to be able to scan through text and give some key parameters that we Needjam поп your data where you want to get feedback from soon so I guess we can now perform our tasks. I don't know if you can just come and do all the things actually but, and I don't know. into a certain ID has these different characteristics, but we're not there in terms of delivering the value of that state. We're just reinforcing our database, to some extent. And so the, at least on our side also, what students, when we're working around Gen. Vega, and when you, I think, when you pass the initial one week of super excitement on the conversational part, is that value will come by combining. Is it also something that you've seen on your side? Yes. So going back to the EHR, I don't, I hate GoVaco, it's the same thing, but the EHR is a nice example, because let's say we scan that. Well, what happens, COVID again, is a nice example when it's a completely new entity for us that we've never worked for before. We need to be able to classify that patient into a specific, what we call, Mesh Terms Medical Subject Hitting, or Ontology. So we have a whole tree of ontologies where we classify breast cancer, and then the subtypes of breast cancer, for instance. But if we start to pull in EHR, it's like, you can say, right, we can classify this patient, we know they have this. Then we say, okay, where do they fit within our tree? But then if you expand that out to, let's say, financial management, et cetera, within, within pharma, we wanna say, how much did we, how many trials did we have with this particular disease or indication, what commercial applicability does that have? All of this needs to be connected somewhere. We're really bad at that within pharma. Anywhere you read, you'll hear about silos or everything's broken. So being able to connect all of these different pieces together, and as you say, layering them in on top from very, very high upstream, right the way down to product marketing, essentially, we need to be able to connect and combine all these different technologies. Within my space, we're working a lot with a couple of key stakeholders who are looking at predicting, I make predictions at my part of the business, which then feeds a predictive model with their part of the business, et cetera, and et cetera, and this keeps going. Now, we always have this terrified moment of like, well, what happens if one of these predictions is wrong? What effect does that have further downstream? But the reality is that this is what, this type of technology is definitely gonna help us in that kind of goal, so that we can have everything connected all the way through, and then layer it in in each of the different groups and different units that ultimately from ideation through to commercialization, we actually have kind of a connected fabric of different technologies and data that we can use. So this will play, when I think about that fabric, this is really at the top of that level, because that's completely utilized across every step of us. So in my case, I don't see this as one tool, I see it as like an overarching tool for what we do, and I think we'll all be leveraging it and sharing it between the different groups for business units. Let me push it a little bit on that conversation, because I think the beauty of that technology is that we see a real opportunity of providing an easier access to data to a broader audience, which back to our initial topic on self-service analytics and a very large scale, or capacity to digest results from a predictive model to derive some insights, which is great. But then to your point, it also, it almost adds an overarching layer that hides the underlying modeling complexity. How do we keep trust and faith to the underlying parts? It's always a difficult question to answer. I think trust is one of our biggest issues. And it kind of goes in, you mentioned earlier at the beginning about governance. I think for us, the key thing is about being transparent and explainable in what we're doing. And for, you know, when we look at this, one of the key challenges that I have is when we present any of these like machine learning models or present any of the results from them, we get that black box, rather than the headlight thing, where did this go from? What does it mean? So we try our best to provide some explainability. I think what's really nice about the hype, and I call it hype because it is really with this technology, is people are starting to see that and are more aware. And I find it really interesting because there's people coming to me from the business that I would never have expected asking about, how can we use this? I want to be ahead of it. How does it work? And there are all these key questions. Some of them, I don't know the answer to. And you know, that's where I have a great team who are, we can loop in and we have a lot of brainstorming sessions. But the reality is like just trying to provide some sort of explainability like we think this happened. It also links to another point about rolling this out within our organization. So we don't have full access to this today. We're very sensitive about what we use and how we use it. And I think rightly so. You know, if I was a digital, I've really put on a, put the brakes on us at the moment. But which is a good thing. But I do see that we have to make sure we don't stifle innovation moving forward by just blocking it until we all feel comfortable and safe about it. So we still do need to take some risks. I think we're working with you guys in data. I could have been very helpful because we've been talking about how can we use this. We've put it into a bit of a safer space so that we can try it out. But the reality is if, Tansier question about trust. You know, what can we trust? We can't even trust data that's generated from a team, you know, two doors down from where I sit in the office. Right? And that's a big question. Like, oh, so I think it is a bigger issue. I think the best thing we can do and solving recently that we within our team and many others, we just opened the doors. We just said, right, this is what we do. This is how we do it. We don't hide anything. There's nothing to hide. And if somebody comes with a suggestion ahead to make it better, we go, okay, fair enough. If someone comes with a suggestion, we don't agree with. We're honest about that. So I think the transparency is the best solution to build that trust. And then for the end users, try and provide some form of explainability like where did this come from? Where did the sources come? So I think, you know, having the people who are going to use this or apply this in the early stages need to come with, these are the prompts that I run. Right? This is what we ask. This is what we expect and this is what we got. So that there's kind of a framework there to give us some explainability and trust to the end user. So, you know, in short, transparency and explainability is the solution for now. And it'll be interesting to see what happens when we roll it out within the business, how it changes, because it will change ultimately. Yeah, the impression that gave me when we were working on this is how it was going to be reinforcing the importance of having strong data foundations. And so for old, that's the, that's not the easiest, that's far from being the easiest part of the activity when you want to do analytics. If you don't have proper foundations, it's kind of building a sand castle. And I think with generative AI, it's just further going to be in flip, in flip-firing that move. So actually for all organizations that are grass basically ingrained in really building robust structures of data where they know reference-shaled data, where they aren't capable of identifying referent, what I would be calling, referentials-terrated analytics, like those that are quite foundational to the organization. I think it's just going to further reinforce this. It's probably going to be further amplifying the move also by freeing access to data, which is always an opportunity to identify kind of what's useful and not useful, because we can all have fancy ideas and we all can have fancy ways of looking at the world, but you need to have a little bit of consistency in terms of it. To your point, unexplainability. When I work with my teams on these types of topics, we always try to look at two sides of the house. We try to look at what is needed to build the trust with data professionals and what is needed to build trust and value for the actual business beneficiaries. With really, my takeaway from the past two years is that you can't have the right of the same topics. How a data scientist is going to be looking at the world is not the same way, because there are trained professionals who can understand specific data metrics. That after two years still don't always echo with how I see it. So to your point, I think, explainability, which means capacity to trace things back to how a certain given data was formulated, how an opinion was formulated. If we go back to what I was presenting before in the document intelligence side, what we have built in our side is not just giving the answer, but also giving a full view on the underlying pieces of documents that were used to formulate that answer. That, I think, is going to be critical, notably initially, to build the right trust, and then to keep as something accessible whenever there's going to be questions. Because if we want this to be as impactful as we want, in terms of changing how decisions are being made, that should be the point. Some point, like, accelerating process is about that. Yeah, seems to me as being something quite important there. The lineage scares me a lot, right? And reaching me and I'll put my hands up, I totally take blame for this. How many times have you created data sets that you just dump them into some general storage? But in that dataset, you have all of this very nuanced filtering and that the problem is, as we kind of get bigger in an organization like ours is so big. I've seen, like, Mikhail on my team here, like, there's been a couple of times we got tricked out, but we use this dataset and then we went back in and on, no, because we had no understanding of what really was happening in the background. The fear I have with this, and this is definitely so we need to be very careful about, is if we have a model that's maybe, you know, we use the baseline model and then have a kind of fine tuned with some of our data, I worry about what data we're actually feeding it. So I think from a governance perspective or lineage, we have to be a hundred percent certain of what we're fine tuning the model against, because I think some people are going into this and just give it everything, throw it in, let's see what happens. And that's a terrifying prospect of you, Faker. Let me take you to another, I don't know if it's terrifying, but actually I think it terrifies some people. The last question that we looked at, it's on our side of the team, we just applied it to one specific analytics technique, which was prompt to graph analytics. So graph analytics is something that we have invested in over the past two years, as I think, similarly to what you have seen on your side, I see this as a pretty fantastic opportunity to explore simply the links between data, but also beyond that, we know that it can play a very significant control on a number of business processes, like track repurposing, like understanding lineage between clinical outcomes and specific drugs, and on other activities that are like in financial services is largely used on anti-monument lunging, on fraud related topics. And what was really impressive there is that if you take a topic, building graphs, like properly building graphs, it's not easy, it has a specific terminology, speak about nodes and edges, and if you've never heard about nodes and edges, I don't think it means anything to anyone, which makes sense. And with LM, it's true that you can really significantly lower the barrier to creation of graphs. So we build that today on our side. I think generally speaking, the prompt to data sets, prompt to machine learning, prompt to development, can also be seen as a pretty scary trend for data professionals. Is it going to take my job tomorrow? Is this something that was raised within your team? How do you react to this? How do you see this? I think we take a very unique look at it. Like if we can sit back put our feet up and let the machine do everything, there's the perfect life, right? We can go to the Caribbean, relax and enjoy ourselves while the work is happening. It is a question that comes by, and I treated it facetiously, because I think we have that same question with the printing press, right? We have that same question all the way through history, all these different things are pop up. The reality is, I don't think it's a threat to the job. I think what it will do is it will free up space for people to be more strategic and think, because I think we spend a huge amount of time right now, not just in our business, but across business, of people doing things that are probably inefficient in their time. As I said earlier, some of the generation of even power points, things like that, so slow, waste a lot of time, people spend two hours trying to say, is this the right way to formulate a paragraph or bullet points for what I want to show? I think we can free up a huge amount of that time so people can be more strategic from just a day-to-day role. If I link it back thinking about your insight at hand, when you can just type a question and get analytics, does that threaten data scientists? I don't think so, because again, the same thing, if we look at prompt engineering and that sort of thing, it gets so sophisticated. This is only the first step into also using this technology. So God knows what's going to happen next and next and next after that. So I think if anything, it should be sparking a revolution of people, analysts moving to say, actually, I really want to move into that field. How can I get into that gendered array, I feel? What do I need to do? How can I use that to my advantage? The other thing I found, and this was just in the last week working with Nicholas in my group, was it actually made me as being a team lead. It's made my job easier because a lot of the times I, oh, it's probably made Nicholas's job easier than mine, because what's happened is I turned up and I give him one or two lines, and I have a very high level idea in my head. And I think he goes home cursing me trying to figure out, like, what does he actually want? But the reality is now what I've found, I've been able to use this to generate like POC code, because I can code a little bit, not as good as the guys on the team. But I can get a rough POC going. I can use Copilot to generate a bit of code and say, okay, this is kind of a rough idea of what I want to do. So when I come to the table with Nicholas, we have an awful lot more, and other people martain, and such in the team. I have an awful lot more insightful conversation where we get much more results much faster than we would have typically by me just generating some mad idea on a wifeboard. So I find that it actually makes us more productive and generate better ideas. So I don't think it's going to take our job, but actually make our job better and more enjoyable, I feel. No, which is a good takeaway. And I think also going back to the, let's say, more of the control and oversight side. If I get the take again, the example of the graph's topic, so it's a powerful acceleration to generating the first graph. So it creates the first parts of the nose and the edges and some initial filtering. But then there's still the, it almost refocuses back on the proper expertise, which is be around understanding this data and understanding what are the valuable outcomes and taking it further from that initial baseline. And I think to your point, it's normal to see this as potentially a threat initially. And it will change something. This is what change new technologies are bad. But I do agree with you on the potential that it brings to probably accelerating the impact. If we put it in the right framework. Before we wrap up and open the ground to questions, maybe I had one last question, because specifically as you are in a regulated business, how do you see the regulators reacting to this? So if tomorrow you were to submit to the FDA, applications for a new drug that will have been tested by real professionals, real humans as usual, but leveraging a bit more of these types of techniques or with reports that will have been generated with generative AI, do you see them as reacting properly? Are they some concerns being expressed on this side of the house? Yeah, it's funny, because I had a discussion with, and I would say discussion is probably more heated debate with our head of information management. And he, we were talking about the same thing, and we brought up a very good question in the conversation, which was, what would the FDA say? And fundamentally, some of the feedback I thought was interesting, because I thought the same thing, will they be clamping down saying you better not be doing it this way? But the reality is a lot of the submissions we make are highly regulated on their side. So it's actually our responsibility, because we will be generating that data and we'll have a signatory who will sign that off and submit it. So fundamentally, it's our responsibility. The FDA, while they will care, of course, ultimately are kind of like, well, if you follow our submission process and you fit our templates and what we regulate, it doesn't matter how it's generated, it's more our side that needs to be cautious about it, because obviously what we submit it, and we'll have a signatory. I think what will happen, and what I'm hoping will happen, is that submissions will change to be much more accelerated, hopefully. We might be able to do it in a much faster way. And I think what will have to happen is FDA will probably come up with or work with industry to find a kind of, there already are a lot of accelerated paths anyway, or pathways to drug approvals. But I think that might change even further, because they have to be very promising molecules, and we have to do a lot of extra work on it. But the reality is now, if we could do that for everything in molecule, they're going to have to scale up probably on their end. So that's probably where the question might come. But I think from a safety, regulatory, everything is still going to follow the typical standards as it is right now. So I don't think it really matters. It's more our responsibility as business owners to say, you know, we are confident and we sign it. So I don't see a massive change immediately, but I do think maybe in the future there will be a better way or a more efficient way to actually do these filings and submissions with the FDA. And that's probably where change will come. And it's interesting to get this perspective. And I think on top of that, we will also get the the perspective of different regional regulators like European Union, as you know, has the EUA act and has started shaping this a little bit further to accommodate with the generative AI stepping in. I know that they're present, cons, around regulations. I come coming from a regulatory industry, maybe so used to it, but I also see this as being a healthy way of having a common taxonomy to be able to look and to analyze and to think. And maybe sometimes to hate, but at least it gives this common starting point. On the FSI side, similarly, I would imagine, I think we're going to be seeing very different attitudes from different types of regulators. In Europe, probably, as you know, a bit more of a cautious side to things, but two points, it probably also will vastly defense how these technologies are being used. If we take FSI, for example, there are some suitability requirements as to what products you push to different types of customers, this will say. And so as long as it's blended in into these techniques, then it will work. If it comes to other critical regulated processes, like, again, anti-money-landering, probably it can serve us some acceleration into investigating and understanding root causes, for instance. So I think, for me, the main takeaway is that in any case, as usual with these new technologies, it's not just about looking at them on a standalone basis, but it's really thinking about how they embed, how they get combined, how you preserve understanding of outcomes, and how you arrive with these step-by-step approaches in order to deliver impacts. And come to think of it, actually, with the FDA side, as a good question you raised, in reality, they would probably benefit more from adopting the technology to scrutinize the farmer industry in a much more robust way. And I think they could actually benefit more from it than from us, if anything. We might look at it as they all know they're using it. So I see that as actually a massive benefit for them to be able to accelerate and find, like going back to some of your points about in the knowledge graph that you have here, the structured data, comparing data between different submission documents, there's a huge amount of value that they can derive from this. So if anything, I'll probably see it through the way around. I'm going to wrap things up because I want us to have the time to get to the Q&A, and I'm sure there are tons of questions. The take I went on to sign when we were thinking about this, it was, again, it was about both identifying these types of usage patterns, identifying how there was cross-sections between different types of industries, different types of business functions, how it could contribute to, let's say, generally speaking, these types of outcomes that companies are looking for, it can be on the wrist side, it can be on the revenue side, it can be on cost containment, it can be on innovation, it can be of sustainability, and that identifying these usage patterns, not restricting the technology to this, but still seeing this, I think it's a way of guiding the thinking forward in starting making that impact. And you know, you confirming your viewpoints around how you've seen the same, how you've looked at different, how you've used, also, similar types of approaches, and identifying how you could use this. At least for me, has been, you know, very enlightening, so Jonathan, thanks a lot for this. Is there anything you want to add on your side before we move to the Q&A part? No, I'm just eager to hear some questions and see what we can answer if we can. So moving to the Q&A, I will encourage all the audience, if not done ready to add any other questions that you would have. So let me go through the questions, and then we can see how we split them with ourselves. There was one directly for you, which was Jonathan, with advancements in AI and in in in quantum computing, used 14 mass customization of medical drugs. That's getting into something quite technical. I think we're already getting to this customization with targeted therapies, even before LLM's and quantum computing, we were seeing medical advancements and targeted therapies, and I came from a genomics background, so I've seen that already. And I think where we are looking at it is, instead of just saying rolling out rules, we're actually looking at it, how can we make our processes better? How can we use data more insightful that would enable us to to get to business decisions and business answers, or even predictions for modeling a lot faster. So quantum for sure, you can see an advantage in protein protein interactions or drug protein interactions, et cetera. And that's definitely something that we will benefit from in terms of high throughput screening to bring candidates to the business. But where we'll see real value in speeding things up is actually how we run that molecule through the business. So that's probably more advanced kind of usage of from our perspective anyway. So I think there's benefits on both sides. There are two questions on the Q&A side, which are about the setup that we put in place to address these technologies. And if I try to split them into these sides, there's probably one question around how much do you use open APIs, how much do you decide to use containerized APIs, how much do you decide to actually build your own internal L&M? So maybe that's one first thing, and I'd love to get your perspective on that. And then maybe the second part is, what is the current text type that you think you are going to be needing in order to leverage these technologies? Yeah. So the first answer your first question, I think just use an open API is not going to happen straight away. That's not going to happen with us, unfortunately. Well, fortunately I suppose. But I think the reality is, and it goes back to your point about combining, I think that having the base model and then having the fine tuning is going to very much be the way we do it, and it will all be in a containerized approach for us, because obviously very sensitive of some of the work. I don't see right now, I don't see the point in, maybe this is just me, I hate reinventing the wheel in terms of efficiency, so I don't see the point in, I was trying to, that's not our job, that's not what we're experts in. So why would we create an L&M? I think combining the best of both worlds will definitely save us. And so that was, hopefully answers the first question. The second one was about the, what was it the text document? Sorry. Yeah, there are some questions around, and please feel free to, of course, to share the level of information that you can, but the type of thinking around the text, like that you need underneath. Yeah. How are you going to be leveraging the cloud, or are you going to be leveraging your private cloud, or are you going to be leveraging on-prem? Yeah, so at the moment we're using private cloud, and as far as I'm aware. So again, as I said, we're just a slow rollout. Right now, like a lot of the stuff that we're doing, we're not doing real business questions out there yet, because we're trying to, you know, we're following all compliance, but we are using a lot of ideas for, just general ideas and generalizing it to something like, hey, how would you pick the best site, or you know, take this random, publicly available data set and give us insights. So we're kind of playing around with that. And when it does rollout properly to the organization, it'll be in a private cloud structure, and which we're working with. And again, how much, because it's so new, after we carefully about how much I can say, and not say it's unfortunate, but happy to come back when we know more, and it's rolled out more, I'll be able to give more details about it. I will add my two cents on that, and also address some of the questions we had on the chat on size of companies, or how can you get started? Because I think we have the benefits of having a very large organizations like yourself, but it's true that different size of organizations could take different types of benefits from those technologies. One of the questions was around everything that I showed was it built in DataIQ. Short answers, yes, everything was built in DataIQ. Does it mean that we have like specific homemade dataIQ LLM far from it? We're exactly on the same philosophy as the one that Jonathan has been presenting, which is that we're not here to reinvent the will. We integrate open source packages, where we call them APIs. This is what we do. We're here to accelerate the usage of these technologies. But to explain a little bit the philosophy behind is we want to provide within DataIQ the right capacity to control the usage of these technologies, but also to make that combination we talk about. If we go back to the first usage pattern, which is around self-service analytics, you can't do it if you don't have proper analytics, and you will never have the availability if it's not done properly. So that's what DataIQ would bring with just the LLM acting as kind of the cherry on the cake. Document intelligence, you can do it if you have digested your documents properly before if you know how to slow them down, and it kind of acts just as a query overarching mid-act mechanism on top of it. So that's two answer one. The second, when we were, we was one part of my team works on AI governance, which is why it's an important topic for me, and we spent quite a substantial amount of time thinking around depending upon the usage patterns, what are the implications from the governance and privacy side, which I think also guides how we can help organizations depending on their size and maturity, think about how to use these technologies. If we take the first topic, like the self-service analytics part, actually the volume of information you're sending to the public LLM is extremely limited. It's just about this schema, and so there's no massive amount of privacy in there. And so on other topics, if you are starting using LLMs to query private internal sensitive documents, as long as it's content-erized, honestly, nothing is really leaking out. So I think we have issued a document on our site called the rat framework, which is really here to guide people through that thinking, knowing also that at the end of the day, there are some questions around the sensitivity of information being sent out, but more importantly, there's what is the output and how sensitive can this out-bit be? In a bit of a selfish manner, if tomorrow I mess it out, in how I send out a person to do maintenance in a machine, I can lose some money, but I'm not harming anyone, hopefully, whereas if I am starting to be discriminating in my communications to someone, the harms are very different. So having all this in mind and how we analyze this, I think, is also very important, but I would say that for the smaller organizations, there are ways at finding safe, approachable use cases and ways to start understanding how these technologies can help you on your own business outcomes path. Is this very Jonathan in your view? Like, if I say it in this way? Yeah, I would agree with that, yeah, absolutely. Yeah, no, definitely, I wouldn't have too much to add. There were some, sorry about that, I'm looking about the other way. Yeah, there was one question, I was just having a quick look as well. There was two really nice questions, one about the future areas that we're looking to prove business value, I think we're building an awful lot of end to end solutions. So one of the key places that we'd be looking to show value is writing of protocols, right? If you think about that, we have scientists and doctors and clinicians going in and writing what we think a good experiment would be, but that has a dramatic impact when it gets rolled out to the clinic, right? So in terms of is this even a logical one? So if you have a physician who's in Pfizer for 20 years and hasn't seen a patient in 20 years, I'm not saying this happens, but they, you know, and then they write something, they might be removed from the nuance. So actually, this will really help because we can use this to scan across complexity between our protocol and the other protocols that are within the indication to give us a nice benchmark to say, oh, you might be a bit more complex, you might be less complex. So that's one place where I think we'd have dramatic value. And the other question is about synthetic data. So this is something we are looking at as well from a perspective of generating what we think enrollment could look like. So we're generating like simulations, just complete simulations, random simulations, but we use some bounds of reality and say, okay, this is a reality that we think is the, this is a realistic world. And then we use LLM, we were trying it last week, use LLM just to generate fake data and see what it looks like. And what impact that has if we look at it and compare it to our projections. So that is one way that we can use it for anonymizing data just to answer one of the questions. So we are, I think we are thinking about that in a very robust way as well. So they're two that I just thought quite applicable to what we're doing. Yeah. And once again, I want to insist also on that, like I'm the synthetic data, so capacity to generate the data, capacity to, there's also the really use starting just from understanding data schemas and how LLM can help you leverage outcomes of the data back to that conversational self-service analytics. It's one way of doing it, which is, which doesn't require to have your entire data fundamentals before asking yourself some, some questions. They were, if you, there were some reactions on how we tackle the regulatory side of how we were seeing DXB being combined with AI, with AI regulation, with how, again, the regulators were going to be embracing this. Is going to be a bit of a layering topic, right? We don't know, like it's still too early to rule that through and how it's going to work. And as I said, the discussion we had this week with some of our leaders were really getting into that, like, how are we going to do this? So I think we're still in like a blueprint phase right now to understand, okay, where is this going to go? At the moment, we're working on triaging our use cases to say, okay, these are the use cases and then doing impact assessments to understand, okay, where does it touch in the business? So it's probably a bit too early to re-know that yet, but I'm sure it's going to evolve very, very quickly. Before the end of the year, we'll probably have a robust, okay, this is how it's going to work. And I think to this point, the, and it's usually how it's done in any case, whenever there's, there's changes. It's, it should be an inclusive game at the same time between different players working together from similar industries, their regulators, arriving as think tanks. For these new technologies, you make a question about cracking the impact together and also understanding kind of the rights and wrongs, having these viewpoints. Coming from the FSI space, if I think the, the moments where it becomes hard is when we have the regulator on the one hand side forming their opinion without actually running these businesses themselves and the business is kind of running their opinions on their own. So the more it can be an inclusive game of people working together on making these, these tests and looking at these outcomes, because again, Jonathan, to your points, they can actually be a lot of benefits for them also. I think that's, that will be, yeah, that will be beneficiary for, for everyone. Absolutely. Giant or you control, we probably have time for one last question. I'm scanning through. So, yeah. And by the way, for everyone who's on the call and we appreciate we won't nestly have time to answer all the different questions and comments. So much, much of that. So maybe the last question to conclude, there was one around how do you intend to prove business value? And maybe to twist it around, do you think it's going to be a different thought process than for any other AI topic? Not really. I think it depends on the use case, right? In our case, I think the first protocol is going to be the low hanging fruit. So within our business processes, where can we implement it? That we can actually see kind of fast results in terms of the impact it has. Because you have to remember, sometimes some of our studies might run for 12 months, maybe longer. So if we make predictive decisions now of other sites performance, I have to wait three to four months before the site even opens. And then I might have to wait another 12, 18 months depending for them to recruit. And then I'll know what the difference is in those results. And that's only one study. So then our end numbers are very low. So what we're going to try and do, I guess, is look for opportunities where we'll have fast turnover in terms of, oh, can we apply it here and see if we can get decent end numbers and then look at the impact that it has driven across the business? I think there's a lot of cases like that. But it also is a tradeoff between impact effort for us too. Because if it's a huge amount of effort and the impact isn't massive, then what's the point? So I think it's still, as I said, this kind of working through the use cases that are there in the business night right now, working with Pfizer Digital, is going to decide where we see the biggest impact. So as I said, there's a slow rollout. And we have a number of different use cases that I'm very excited about. The problem is it's kind of like which one will get chosen that we can really invest the time into on top of our day job. So it's going to depend. I think something small and quick that we can get feedback on fairly fast will be our primary use case. And on our side, a little bit the same. We will be dividing a bit more in the product and probably also on the raft AI side in other webinars from a team standpoint, what we're heavily investing on is all these usage patterns we talked about. We're actually working on industrializing them where they make sense. We're also working kindly with our customers to understand, you know, based on all these ideas that we initially had, which I think all make sense to just be clear, clear, which are the ones that are triggering the most immediate value. We'll be, you know, I just want to thank everyone for their contribution and for their contribution in the chat else, who of course would have thanked Jonathan for this wonderful conversation we just had. And I think Joy, under your control, this is just the beginning of a series, right? We have one plan next week, but there are all the other ones we're going to be planning forward as we are cracking this new spatial together. Yes. Thank you, Sophie. Thank you, Jonathan. That was a wonderful conversation. I want to let everyone know that there will be an on-demand version of their recording available soon. And please join us on the 20th for the third of the series. All right. Thank you, everyone. Thanks, Mary. Bye.