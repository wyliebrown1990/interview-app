 Welcome to Observe Enablement. In today's video, we'll be going over Data Modeling. To go over Data Modeling within Observe, we're going to go over what Data Modeling is an Observe, why we should perform Data Modeling and when to do so most appropriately. Then we'll move into how Data Modeling is actually achieved, which is through the use of worksheets within Observe. And then lastly, we'll go over some considerations and tying everything together when it comes to publishing data sets, which are ultimately the result of any Data Modeling we do in Observe. So what is Data Modeling? Data Modeling is simply a set of filters and or transformations applied to the data, resulting in a new curated view of that data. And this is simply taking blobs of text or JSON or CSV data, however it's being sent into Observe and then transforming it into a table of relevant columns. Next, I want to take a moment to relate Observe to the rest of the industry and other observability concepts shared amongst other organizations. And that is introducing the idea of ETL versus ELT. This acronym represents extract, transform and load, which is the process of extracting data from source systems. This could be our logs or metrics. And then we have a decision as to whether we should transform it on the source system itself, which usually requires complex source specific configuration, or we could load it to an observability platform and then send the raw data and leave any and all transformations to be performed within the platform itself. As you might guess, Observe very much embraces the ELT mindset we prefer to always model and observe. We think this is best in terms of managing source systems and also being able to adapt to the unknown unknowns. And finally, to conclude what is data modeling in Observe, we want to relate a data model or the output of any modeling that we do to a data set. A data set is a construct within Observe that represents the filters and transformations applied to a particular stage of data. And that captures not only the data transformations, but then provides a way of interacting and viewing that data. So a data set is a key term here and something we'll revisit throughout this enablement video. So before we get more into how we actually model data or when to do so, we'll take a moment to appreciate why we're modeling data in the first place. We're sending raw data to an observability platform. Is there a reason why I have to do anything? Well, the main reason we model data in the first place is for usability. To actually make our data useful, we typically need to curate those logs or events or metrics into data sets, making it easier to investigate that data, visualize it, and also monitor and alert on that data. Secondarily, some queries require parsing the data to some extent so to be able to perform the query intended. And this is in the case of filtering on nested values within some larger piece of data. And so the reason we want to model data is ultimately to make that data really usable. However, when it comes to using this data and modeling it, there is a best way of doing so. And so another reason why we model data the way we do and observe is for the sake of efficiency. Define data models, or in other words, data sets represent pre-process data which can be done in batch. By taking the incoming data and performing pre-defined data transformations that we set up in advance, we can allow our optimizer to best process that data. So that it is both most efficient in terms of query performance, but also most efficient in terms of usage cost and the underlying computation required to perform such a query. And so why do we model data? Ultimately, we want to make data usable and so we need to model it from its raw form into something more comprehensible. But then furthermore, in terms of actually modeling that data, we're going to do so with pre-defined models so to make it most efficient. All right, so you might be convinced on why we need to model data. When do we model data? Or in other words, when should we create a data set? Because at this point, I'm going to start having to use my words more carefully because modeling data can mean very different things in observe. This could be visualizing data, monitoring data, or just modeling it out for future investigations. And in this case scenario, we're mainly speaking about modeling data for the sake of investigations. And really, this is just modeling data to produce a data set. So when looking at when to model data or when to create a data set more specifically, yes, we would want to create a data set if any of the following conditions apply. The first condition is if this type of query will be run repeatedly, maybe it's a specific set of logs that need to be extracted into a proper form. If this is something the remaining team would also like to view regularly, then we definitely want to publish this as a data set. More than just looking at logs and event data is this a specific type of view, maybe a visualization that will be needed more than once by yourself for the remaining team. This could be time series data to look over the number of logs, errors or the latency within your application. If this is something that's needed regularly, yes, we would want to model this and publish it as a data set. And lastly, in the case that we need to monitor or alert on a specific set of data like app errors, then we would want to model this into its own data set to make that monitor most efficient. But there is cases where we would not want to model our data and publish it into a data set. Maybe we just want to model data and save it as a worksheet. And so worth noting here, we're still modeling data at the end of the day, but the difference is coming in between whether we publish a data set or just save it as a worksheet. And so we'll look at what this means when we go into observe and actually see how we create a data set, what it means to use a worksheet and how these two work together. But for now, it's worth appreciating that a data set is a published long standing data model, whereas a worksheet is just a snapshot. And so we would only want to work sheet if it's only going to be needed a few times or just once as part of an investigation. So now let's look at actually how to model data and I've already started mentioning worksheets without properly introducing them. And those are ultimately the vehicle for how we model data and observe how we create data sets and more. And so let's look at what a worksheet is because that's what it means to create and model data. Worksheets are a data editor or a scratch pad, a tool within observe that allows you to model data or transform it in any which way to the fullest extent that observe can provide this way of modeling data applies not only to data sets, but also dashboards and monitors. We use worksheets to model data and publish them into data sets, but we often then also want to model data so that we can visualize that data in a dashboard. And finally, we also model data in case scenarios where we need something specific for the sake of monitoring for errors or certain messages. And so worksheets will be a familiar UI tool that you'll see in various places within observe. A worksheet is comprised of three components. There's a lot going on in the interface which we'll see in a moment, so I want to take a second to simplify it and to just these three components. The first is data stages. We'll see in a worksheet how there is a stage which actually contains the event data or logs that we're trying to model. Below that we'll see the opal console and here we're introducing maybe for the first time, opal, the observed processing and analysis language. Very similar to promql or other similar query languages out there. Opal is an abstraction over sequel which makes it tremendously easier to model and query time dependent data. And so the opal console will be our editor for writing and editing opal that actually transforms and filters that data. And then finally we'll see there's a right hand menu where we can apply further configuration to the data we're modeling or a filter menu to help quickly filter on columns. So let's jump into observe and take a look at how to actually put all this together. So now that we are within the observed platform, we're going to look at one of the most common use cases for modeling data. Most common because it has to do with Kubernetes, but also it's one of the most simple ways of modeling data too. Because we're able to take advantage of some of this pre-modeled data that comes from the Kubernetes app. You should have installed the Kubernetes app as one of your observed applications and ingested Kubernetes data using the observed agent. And then that would lead you to having the container logs data set here available in your data sets tab. So I can click on this and then I'll be brought to the log explorer where I see the container logs and all of its data. It might also be possible that you're looking at the data set page for the container logs. And in either case, those are going to be our starting points for modeling data. By identifying the source data that we want to work with, we'll go to either the log explorer or that corresponding data set page so that we can then create a worksheet. So a worksheet is always created from the source data set that you're hoping to build upon and you'll look for it in the top right corner. On a data set page, you might just see a button called create worksheet. Here in the log explorer, you'll find this underneath the actions menu where you can click on actions and then open in a worksheet. This will bring you into a new tab which presents a worksheet. And like we mentioned earlier, a worksheet can have a lot going on at first glance, but ultimately there's only three major components here. The first one is our data stage, which by default is named stage one, but we could change this into whatever we'd like. I can resize this stage and I can see the actual data here as defined by the query window. Next is the Opal console, which you may find hidden down below, which you can click to open up or maybe it's already open by default. But down here below your data stage, you'll see the Opal console and like we mentioned earlier, this is where we'll be writing Opal, the query language for helping model and filter our data. And so I can apply filters or I can make new columns will come back to talk more about Opal in just a moment, but for now we can actually hide the Opal console because we won't need any raw Opal to perform the data modeling that we need to do. And today is exercise. But lastly, before we get into the data modeling itself, we'll look at the third component of worksheets, which is this right hand menu, which will have various configuration options or options to filter based on how you're interacting with the worksheet. And so let's now get into modeling our data and I'll jump into it by making use of this filter ability here. I'll go over and I'm looking at these container logs, I can see various metadata and I want to use the container column to filter down to my app logs. So just by clicking on this checkbox here, I can instantly do that and now I'm only looking at my app logs, which appear to have a JSON format. And so one way we can interact with the data here is by double clicking into any of these stages, so to bring up the inspector, the inspector is merely a better way of viewing the individual cells of data and you'll see it pop up right alongside the Opal console. And so this can be resized to better view the data and I can actually interact with this data right here in the inspector. If I wanted to filter to a specific value like only debug logs, I could do so or I could exclude debug logs or I could just extract this field into its own separate column. And then I can see all of the different options and filter even more using this new column now available for the level. And that's how easy it is to extract data from one column into a new column. And so now I probably want to extract more of this data given there's so many fields and I can do so more easily instead of one by one by instead clicking on the header drop down. And then I'm going to look to extract well, I would like to extract from JSON, but I see here that it only says extract from string. So I might click on this to see what I can do and I could use a custom regular expression or I could leverage common patterns to be able to parse out specific data from an arbitrary string. However, this doesn't really help me in terms of extracting fields from a blob of JSON. And so what's wrong here and this is very common so it's worth pointing out here, you'll need to convert this column to a proper data type. And in this case, JSON. But it is also worth mentioning that oftentimes when you can't work with columns that look like numeric values, but don't support numeric operations, you may need to convert them to an integer or a float. But in this case, I'll convert my string blob to a JSON block. And then I can click again on this drop down menu and see that the extract from option has changed to JSON instead of string. So I'll extract from JSON and this gives me way better options for extracting multiple fields. And so I can extract all of the different fields that I'd like. I already got the level and so maybe that's all I need for right now. I'll click apply. And now I can see I have this data available within their own separate columns. And one of the main reasons for doing this is one is it's much more comprehensible, but also I can click on these columns and quickly filter and see the number of different values within this set, which again is specific to this query window. So now what I've done is I've been able to pull out the data I want. I don't really need this log column anymore. And so maybe I want to hide it. Now I'll point out here the table controls button, which is often overlooked, but very powerful in helping model and view data exactly as you'd like. You can rearrange columns or hide as many as you'd like so to clean up this view of data that you had. But now I want to take a second to put this full circle before we publish this because we have a curated view now, which is extracted the data for some from some underlying Jason. And so really this concludes modeling our data. We've been able to filter to a specific set of app logs and then extract fields from a blob of Jason. Ultimately, we could jump right into publishing this as a new data set. But before we do so, I want to take a second to appreciate the Opal console once more. Although we can do most of our modeling within the UI, and this is often our first step in getting things started because it's easiest. However, even with basic modeling, it's best to take a peek at the Opal console just to double check that the automatically generated Opal is appropriate. And so what we can see here is two main Opal verbs being used. And you can really do a lot with just these two verbs filter and make call, which represents make call. However, it's abbreviated. And these two verbs are very simple, but yet very powerful. And we can see here retracing our steps from what we did earlier to model this data. I filtered to a specific container. And then I extracted the level column on its own when I was using the inspector. And so the way it did that is it actually parsed the log column in Jason in line so to extract the level column. However, I then wanted to extract all of the other fields. And so I parsed the log column again. And then I was able to extract more easily the remaining columns. So if I wanted to optimize my Opal here, I could avoid having to parse the log column as Jason twice. So what I'm going to do is I'm going to take the creation of this level column and just move it into the make column statement here below. You can see here that we can create multiple columns as a comma delimited list. I can delete this statement here. And so now I've cleaned up my Opal and optimized it. But yet it produces the same exact result as before. What I'll need to do though to verify is run. Note that if you save a worksheet or a data set without running the Opal console, those recent changes will not have been applied. And so always make sure you run any Opal changes you've made. Now I mentioned that filter and make call are the two most common Opal verbs that you can use to do basic modeling. There is one last one I want to introduce before we leave Opal. And again, these three verbs alone could be used to perform a ton of different modeling that produces a lot of value. And that last Opal verb is pit call. Pit call is simply a way of listing out only the columns that I want to keep within this data set. So any columns that are not listed will be dropped and excluded from this view. This also determines the order of columns and how they're presented in my data set. And so pit call is often used as a way of removing unnecessary columns and placing other columns in the proper order. So I can just start listing out my columns with a comma delimited list. I do need to make sure though that I include a column that represents my time stamp. All of this event data within observe is very much dependent on time. And so whenever we use pit call, you can choose to pick as many columns as you like or exclude as many columns as you like so long as one of the columns includes a time stamp. So we got our time stamp. We'll go through and start adding the remaining columns. Here I always like to put certain columns first to make it easier to digest and filter. So we have our level, our method, maybe our trace ID and span ID. Notice we can take advantage of auto complete here. And this includes all of the columns that I extracted from the JSON blog. However, there was some other columns here, including Kubernetes metadata that was already pulled out before I even started modeling. And we definitely want to carry these columns with us. And so to do that, we'll need to include these individual columns in the pit call statement as well. And we can start doing so if I type container. I can see container name. Maybe I need the pod name cluster namespace. And you may have noticed that some of these columns don't exactly match up with the column headers that you see here. And this has to do with the fact that these are links. And so it's worth discussing some of these nuances with links so that when you're modeling data, you understand what's going on. So links are a whole separate topic within observe something that will have its own dedicated enablement content for so to properly set up links and to use them best. However, for right now, we just need to understand how links show themselves within data sets. So here we have Kubernetes metadata attached to each log message, which tells me which container and pod that this log message came from. And in doing so, I can link together this log message with other pod data relevant to this specific pod. And so from the API updates, which is a completely different set of data within observe coming from Kubernetes. But this API update will give me information about the pod in terms of its status, its restart count, its underlying node, etc, etc. And so by linking together container logs with pods, we can then see both sides of the data within just a few clicks. And so this applies to containers. I'll see the columns here render after a short bit of loading. Also with the cluster, the namespace or the node. And I actually need to click on individual cells to see the specific data for that resource. And so that's why linked columns exist is so to be able to jump between different sources of data quickly. And so there's a lot more to links, but the reason they're introduced here is so to appreciate how they're used in terms of data modeling. So here we can see a column that represents the pod, but of course this is only the pod label, which is the pod name, but in this case, it's not actually the pod column because this is representing a link. There's an underlying column here that I can reveal by clicking on this button, which is the pod name. And so in this case scenario, I can see the pod name is the same as the pod label. And so these columns match up, but if I look over to the cluster column, I can unhide its key columns and scroll over and see the cluster UID. So here for a cluster resource, I can see the cluster name as its label, which makes it a lot easier to filter and view in the UI. But underneath the hood, the log message and the data here in this data set really only contains the cluster UID. The cluster UID is really what I'm going to want to use when picking it out using pick call. And so with this, I can run and by selecting all the underlying key columns, the links will carry through automatically. And so now to conclude with my data modeling exercise, I want to save and publish this into something that I can use later. We've talked a lot about worksheets, what they are and how to use them. We've been using them thus far to create a new view of our data. And the way we save worksheets is by simply titling it up here and then using this button to save this worksheet. And now this exact snapshot will be saved for me to come back to it later. If I go into the worksheets tab and then look under my worksheets, I would see my new worksheet here, which brings me back to exactly what I saw just a moment ago. However, if I want to actually publish this into more than just a worksheet and into a longstanding data set, I can click on my stage and go over to the published new data set button here. I'll click on this and see that there's already a data set called app logs and so I'll need my name to be unique. And I can also prefix any data set name with a package name separated by a forward slash and then I can name this data set and group it together with other relevant data sets. And so if I publish this, I'll see that I'm brought back into a worksheet, but instead of my input being container logs, my input is now the app logs. And so going back to the data sets I can search for my package. And now I can see a new grouping for my data sets here called my package. And if I click into my newly created data set called app logs, I can see exactly what I modeled out here now more comprehensible and more usable in terms of filtering. And this concludes a brief exercise into modeling data with observe. To wrap up this short little demo, we'll go over what we just did at a very high level. What we did is we started from a data set, our source in this might have happened through the log explorer as well. And then we clicked into a worksheet so that we could really start modeling this data. And then ultimately we can publish this data into its own data set and or save it as a worksheet. But that's really it when it comes to modeling data and observe and using just the tools shown in this introduction, you can achieve a lot of value by modeling out large sets of logs into app or team specific subsets. And really all you need to use is filters and basic extractions. However, we'll conclude today's enablement video by taking a brief look at some additional topics, which you can look into to become a more advanced data modeler. And so we'll take a peek at five different topics related to advanced data modeling, each of which will have their own dedicated enablement video, which you can use to get up to speed. And so just to introduce these topics so that you're aware and can know where to look next if you want to get more involved. The first one is a multi stage lineage, or I could say multi stage data modeling. What we just did in our exercise is we looked at a worksheet that contained only a single stage of data from which we applied all of our filters and transformations and then use only that single result as our published data set. However, in this little example shown here, you can imagine we might take raw data, which starts with a single stage, but then is split out into two additional stages. One is filtered to app one, and that stage contains JSON data, and then we have a separate stage filtered to app two, which contains plain text key valued pairs of data. And what we want to do is extract those two forms of data and normalize it or map it into a common format. And so what we would do is start from one stage, split into two stages, and then recombine back into a single stage using a union. And that would produce our general data set, which contains all of the different logs within one format. And so that's what we mean by multi stage lineage. There's definitely demos and exercises we could go through to help better appreciate how this works and observe. The next one is advanced transformations. Within observe, we took a small peak at what we can do using Opal, the processing language that we use to model data. There's many things we can do within the UI to extract data or make additional transformations. However, there will be certain things only possible through the Opal console and through writing Opal code. And this is a time to introduce the Opal documentation. Again, there will definitely be dedicated enablement content just for how working with Opal. But for now, you can take a look at the observed public docs, specifically the Opal ones, which you can find here on the left menu and get a high level overview of the language itself. But then also most importantly worth pointing out is the list of Opal verbs, which shows all of the different ways you can start a new Opal statement. So without getting into the actual details of how to use Opal, you can see though there's really only 30 to 50 Opal verbs. It isn't the most extensive language like you would expect with learning Python or GoLing. But in addition to Opal verbs, we also have Opal functions and functions operate very similar to other programming paradigms in which you can do simple operations to help manipulate the data or calculate different things. And so that concludes what we mean in terms of advanced transformations, just taking some of the stuff we did earlier and going a bit further. Next we have aggregations and windows, which are actually another form of advanced transformations. But being so comprehensive, aggregations and windows deserves its own separate topic. And of course, its own separate dedicated enablement video, which you can find linked to this. Aggregations though represent looking at multiple events or multiple logs or metrics over a certain time window and then determining how we should summarize aggregate or calculate something based on those many events. Next up we have joins and linking. We also saw a little introduction today into how linking data works and observe and we'll see how we can use Opal to actually define those links and very related to linking disparate data sets together. We can join disparate data sets together so to pull in that relevant information like the cluster name, if we need to actually use it in different data sets. And then last but not least, we have sub queries available and observe. I mentioned earlier that Opal is an abstraction on top of some complicated SQL. And so you can imagine that given how common sub queries are within SQL, we support a syntax that allows sub queries within observe using Opal. However, to conclude, I don't want to end on all of the different advanced topics. Those are available and can help you realize really interesting insights from your data. However, the exercise we went through together in which we only use the UI to make simple filters and extractions really represents 80% of the data modeling done by our customers at observe. By taking large sets of logs or metrics and deducing them into simpler, more comprehensible sets in which the underlying data is surfaced to the top. That is all that's needed to really bring value to your observability platform. So thank you for joining us today on introducing data modeling. We hope to see you again to learn more.