 Sometimes, it's necessary to override a model's raw prediction to ensure the final outcome complies with certain business rules or conditions. This can especially be true for high-risk models, driving critical applications or decisions, or in highly regulated industries. Model overrides in DataIQ enable you to add a human layer of control over model predictions. In the design tab of the Visual ML interface, let's visit the override tab. Here, we can add one or more rules that will be checked at inference time and applied if the override conditions are met. For example, in this regression model, where we are predicting the resale value of a used car, we may want to enforce that if the car is in broken condition, the predicted value should not be more than $10,000. But it also shouldn't be less than $200 because we could still resale some parts of the vehicle. And if the car is still functional, the predicted price value should range between $640,000. Once the first override rule matching a record will be applied and subsequent rules won't be evaluated, it's important to define overrides in the right priority if they can overlap. After training, head over as usual to the results tab to evaluate the model's behavior. Remember, overrides are applied once the model is fully trained. Notice how in addition to the standard diagnostics, each trained model also provides a visual cue that some overrides were applied. On the override metrics tab, we can easily assess model performance with and without overrides. And in this Sankey diagram, we can examine which rules were matched most frequently in our data. In this case, there are many more cars listed as functional than broken. Here, we can also visualize what proportion of predictions matched an override rule and met the criteria without intervention versus triggering an override. This table view provides another way to analyze results, where we can review the criteria for each rule alongside the proportions of matching rows and overrides in our test set. If you wish to evaluate a scenario under different override conditions, simply click here to duplicate the trained model into a new session, where you can easily change the override rules and analyze the resulting impact on outcomes without losing these results. When we score new data through a model with overrides, data IQ returns both the raw and final predictions, and if applicable, which specific override was applied. In this example, we can see that most of these first few rows matched the rule we set up where the car condition was functional. However, since the raw predicted price was between $640,000, most of these were left as is and weren't overridden. However, if we filter to rows where prediction changed is true, we see 52 instances where the raw prediction was outside the acceptable range and so the price was overridden. For example, this functional car's price was brought down from nearly 48,000 to 40,000, which is the top of the acceptable range for that rule. And for this second car which matched the broken rule, the final price was overridden to $10,000 to respect the boundary set in the override rule. Try model overrides in your own projects to ensure safe predictions for known cases, comply with regulations, and increase trust in your models. Thanks for watching.