 Hello and welcome. My name is Kelsey and I work on our sales engineering team here at Observe. And today I'm going to walk through a demo of the observability cloud. Data is ingested into Observe using open source collectors and does not require certain schemas or structures, reducing operational overhead. Here you can see high level stats of the data you have stored. There's about 70 or so terabytes in data in total, which includes various log data, permethiast metrics, hotel traces, and even contextual data like GitHub and Jenkins events. Of the data that is an Observe, about 30% is accelerated. Unlike other tools, you can access all your raw data at any time. However, acceleration is the process of curating data into datasets to facilitate rapid optimized queries. Let's take a look at Observe's data set graph, where you can see all the datasets built on top of the data light and the connections between them. At a high level, you can start to see the different out of the box apps we have deployed. For example, Kubernetes and AWS. If you want an observable system that enables you to reduce troubleshooting time and increase efficiency, then you need to be able to go from point A to point B without having to know the route. IE, you don't want your teams to have to have tribal knowledge to do investigations. Similar to a GPS, this is what Observe does. If I click on pod, you can see all the related datasets and all related traces that went through a set of pods. Or if I want to understand the Jenkins builds related to the pods that are restarted. No matter what questions you're trying to ask, graph link or these relationships between these out of the box datasets enables you to start anywhere and navigate to various layers of your text stack quickly and seamlessly without needing tribal knowledge. Let's show this in practice. Let's use the Kubernetes out of the box app as an example. Opening the app will drop us into the out of the box Kubernetes dashboard. This is an overall view of the health of your cluster. You are able to see high-level stats like cluster size and some other key metrics. If I scroll down here, I can see that there's some unhealthy pods. Unlike a lot of other dashboards and other tools, Observes dashboards are actionable. Meaning you can use graph link to investigate related datasets. Let's drill down into the related pods. Now you can see all the metrics for those unhealthy pods and the outlier metrics like high CPU and memory become more apparent. Over on the top and the activity tab, by default, you can see notifications which are out of the box alerts that have fired to specific teams via different channels like Slack or PageRDuty. But let's take this farther. Since metrics only tell us part of the story, often you want to take a look at other data like related logs. You can quickly pull up other related data, not just what is provided on the dashboard. Let's look at container logs. Notice that we didn't have to filter to the same time frame or the pods we were looking at at the dashboard view. It automatically keeps that context as you look at related data. Furthermore, I could seamlessly navigate to all the traces that went through these pods. By using graph link on this dashboard, I see I have traces sorted by response time. And there are some slow ones in here. So let's pick one and take a look. On the waterfall, I can see we started this trace in Engine X, then called down to microservice container in Kubernetes, which made some elastic cache and SQL calls, which then invoked a Lambda. Oh, we can see the problem here. That Lambda has made lots and lots of SQL calls. Pretty quickly, we have come to a conclusion that the car rating Lambda, that is the problem. We have some sort of SQL call loop. But what is that SQL call? Let's open one of these up. Here we have the span attributes, and I can see specific sequels that are being called. Further down this page, I get the tabular view of all the spans, and then below that, all the logs. We have a across this environment for this specific trace. Thank you.