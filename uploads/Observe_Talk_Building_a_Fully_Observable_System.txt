 Hello and welcome to this presentation on building a fully observable system. I'm Ross and I had a product here at Observe. In this presentation, we're going to first talk about how to form an observability strategy. What are the things you have to think about on day zero? We're going to dive briefly into Observe so that we can establish some context as we then move on to talk about how we use observability internally for troubleshooting and monitoring as well as for her whole slew of other use cases. Before I dive in, I'd like to briefly introduce myself. I am Ross Lazerwitz and I had a product here at Observe. I've been here for about two and a half years. Before this, I was working at a company called Spock. I've been working on very similar problems and I've sent most of my career really trying to make sense of machine data, whether it's been with I've been a security analyst and a security operations center, sis admin, wearing a pager, a pen tester working on front ends, and I just really like this problem. I think it's incredibly satisfying to take a morass of machine data that has very little to no structure and huge amounts of volume and make sense of it. So part one on forming an observability strategy. A lot of time ago in 2017, Observe was founded and in the early days of the company, we whiteboarded something very similar to this. This is an architecture diagram that we're going to revisit in a bit later. And the key things to take away here are we have a bunch of microservices that run on top of Kubernetes. We have a go back end. We talk to snowflake. We store data in Amazon S3 and Aurora. There's a Kafka queue for durable storage. So this is the kind of our forward looking architecture that we were kind of speccing out to build. And our big question was, how are we going to observe itself? What kinds of decisions do we need to make very early on to do this? And our kind of day zero strategy was really thinking about the data because it all starts with the data. So how do we admit log data? Do we structure it? Is it not structured? Is it coming out of standard out? Do we have agents? For metrics, what kind of metrics format are we going to use? Where are we going to store it? Where are we going to query it? But we found ourselves falling back to reliable and well understood sources. So we ended up going with Prometheus because of its very tight support integration in communities. So we ended up going with Prometheus because of the fact that we were going to use it as a tool to create a new tool. And we started with the same tool. We started with the same tool. We found ourselves falling back to reliable and well understood sources. So we ended up going with Prometheus because of its very tight support integration in community with Kubernetes. And as we moved on to logging, similar exercise, where a go shop, so we started with G-log. And what we found was that the ghost standard logging libraries weren't great. And we wanted to do structured logging. So we rolled our own very lightweight structured logging library that utilizes key value pairs. But there's tons of other great alternatives to go and do that. On the tracing side, in 2017, this was a relatively new thing. And there was no standardized library for doing this. There was Yeager and Zipkin and a bunch of very disconnected things. So we went ahead and we rolled our own library. Today, we recommend using open telemetry and we've begun work to transition over to that. And we recommend it only for tracing. We've evaluated for metrics and logs and it still seems that that isn't quite mature as the other. Best agreed solutions that we recommend. And I think the key takeaway from this kind of laser exercise looking back was that we became less opinionated over time. So early on, we were like super opinionated on what tools we would use. And this changed. For example, we wanted to do structured logging everywhere. But we found that that wasn't really tractable. So we used structured logging in some places with key value pairs. Some others, Jason gets logged randomly and sometimes there's just no structured logging at all. And that's kind of okay. And as you adopt more technologies, you're ability to control your tooling really drops. So we thought we could get away with just doing Prometheus. Well, guess what? We have Amazon's. Now we have CloudWatch metrics and we have logs there as well. And we have audit data and we had, we spun up build infrastructure that we had some stuff as well we had to bring in. So, you know, this opinion really changed. And as our platform, like your own technology that we were using, our observability platform became more capable. The gravity of decision of what we'd use in a collection site, we're not as far reaching. And that's because our tool allows us to ingest all sorts of data, whether it be metric data, log data, trace data, and really stitch it together in a single place and make sense of it. So if you want to put structured logs in somewhere and you want to bring in CloudWatch and you want to do all the correlation, we can do that. So we don't really care that much anymore. So that's kind of how we formed our really early strategy. And I want to take a brief moment to explain what observance, so you have the context for understanding the kinds of use cases we have for our own observability. It all starts with data. So observe will take data from pretty much any data source. Could be Kubernetes, could be Amazon, could be a server, could be a phone, a browser. We don't really care as long as the thing you're sending to us has a timestamp. When possible, we try to utilize open source and commodity data collectors, you know, things like fluent bit, fluent de telegraph from ethyus, stats, the BTCO system. When we get into the cloud, like, you know, for Amazon, we've created a cloud formation template that installs a lamb that you can use, open source as well. We can pull things really any way that you want to be able to either send us data or have data or have us pull data, we'll go ahead and do that. And we'll take all of that data and we'll store it in observe is what we call observations, right. So like I said, it's not like we're separating out our logs to some sort of logging index and metric somewhere else. It's all being stored literally in the same place. So once we have all that data, it's, you know, centralized that searchable in one place. But when you try that troubleshoot, it's kind of like staring into the matrix, right. Historically, you know, people would go and they'd log in on one of these systems and they would go look at all these log lines, then be zillions of them, and maybe they can do filtering. But it's really difficult to troubleshoot. So what observed us is it takes all of his data and it shapes it in the things you can ask questions about. We call these data sets. So, you know, depending on your use case, like Amazon, these could be things like EC2 and Lambda, even your users navigating around in your console and cloud trail and Kubernetes. Those are pods and containers. They could be users or support tickets, really anything. And the goal is to give you as much as we possibly can out of the box for well understood technologies like infrastructure. And when we don't have some sort of out of the box data set, we provide a bunch of tooling to easily build these things so that you can capture what's custom and unique about your application. And once we have these data sets, we don't stop at just structuring them into things you can ask questions about. We actually link them together using something that we call graph link. So you can imagine we have this huge interconnected graph of all of our data that hopefully represents your environment. And we can do interesting things like, you know, if you have a bunch of incidents in Patreon and you want to navigate to the builds that were behind the image of the container and the pod that failed, observe will figure out how to go across this graph and really link everything together. Under the hood, it's just fancy joins using primary key relationships. And you know, one, now that we have that state and those relationships, we can actually replay what happened over time. So because we make it very cheap to store large amounts of data for pretty much as long as you want for the price of S3, you can go and wind back the clock and look at changes a day ago, a week ago, a month ago, a year ago, and really understand what's going on. And with this model that we have with your environment, we were able to generate user interfaces that are familiar to different personas. So SREs, Mike, Mike, want their classic dashboard that drills down into something lower level. They hand something off to engineers to get more of a worksheet experience where they can go and really navigate all of the raw data. They can reshape it on the fly because we have very flexible schemas and look at things at the lowest and highest levels. And then for the customer support and success team, we're able to give a really like customer user-centric view into what's going on and really have this conversation in the tool, not over a bunch of random Slack messages. So that's observed. Let's talk about how we use observability for troubleshooting and monitoring. I'm going to return to that architecture diagram we had earlier. So once again, this is what our architecture looks like for our product. And there's really two main paths. There's the read path and the right path. The right path is in bold here. So data will come in from our open source agents from cloud formation through some whatever endpoint we have. It will hit an Amazon network load balancer, then forward on to a separate ingress controller and Kubernetes. It then goes into a Kafka instance. And we have that so that we have durable storage so that we can reliably effectively shovel data in the snowflake, which is our, you know, where we have our long term kind of data storage and querying. There's also a read path, which is the one underneath that. And there's two ways to enter that. You can have a user and a browser hit it. We have a UI that's built in TypeScript and react. Another entry point is also an alert action that might fire off to a third party system. And there's a separate ingress path for the read path that goes through. The UI will generate a bunch of queries in our language that we call opal. It's our observed query language. And that will then get compiled into SQL and then run and query inside of snowflake. So at a high level, that's our architecture. You know, the big important pieces here are Amazon, Kubernetes, snowflake, Kafka, as well as our UI. And we also have tracing instrumented throughout this whole process as I described earlier on in those days, zero decisions. But the question looking at this is, well, who watches all of this? Like who watches the watcher? Well, observed us. So we have something called O2. This is observed on observe. And it's a completely distinct deployment of observe. It has a separate AWS account, snowflake account, Kubernetes cluster. It needs to have its own distinct failure domain. So that if something happens in Prague, we don't also bring down the environment we need to troubleshoot Prague. And our stock agents for Kubernetes and Amazon will feed data in the O2. We also have the deployments for these things, not happening simultaneously, but one fall in the others that we don't have the same change, hit these environments at the same time. And it's not just the proud environment that we watch. There's also a staging environment, as well as an engine environment, which is our bleeding edge acid test. Every single commit that happens gets deployed to edge as soon as it's ready so that we can go and test our changes. We also have a bunch of built-in infrastructure. We utilize Jenkins for our CI CD and Garrett for source code control management. And that's being monitored with fluently running on a Linux server, where we scrape any metric and log that we can from those services. This all goes to the O2, which interestingly enough also eats its own tail. So you can actually troubleshoot O2 inside of O2. I want to talk about the scale of the system briefly. So in terms of data, we're roughly doing 4.2 terabytes of data a day for this environment. And this is quite a bit, but it's definitely another largest observed customer, but it's definitely in the upper kind of quartile for that. We have 51 users of the system on a monthly basis. This includes our entire engineering team, our SRE team. We also have users and sales and data engineering that can come in to understand what's going on inside of their customer environments that they're servicing. Even our CEO will come in and check this. So we have a lot of people congregating in here to do their job. I spoke about data sets earlier. We have 671. You can think of each data set as representing a very discrete use case and observe. So we've continued to build a lot of these. And you know, different teams definitely have different kinds of views on top of that data. There's a lot of monitoring going on as well. We have 68 monitors. I'm going to show you some examples of some of those that are able to use this data. If you can observe your system and it's entire you can also monitor your system because all you're doing is just having a question pre-computed on data. You can get that for free. We have a lot of worksheets. So worksheets are effectively like interactive spreadsheets or notebooks for investigations. So we're accumulating a lot of institutional knowledge that would otherwise just be a bunch of browsers have for someone. Whenever you have an investigation or an incident or some sudden performance review, we save it as a worksheet and we can go back. And because we're keeping all this data, you know, historically, we're able to go back and kind of use these things as a system of record. In terms of data sources, we have a few generic data sources. We have, you know, Kubernetes. We're getting container logs, ingresses. We're getting the entire state of the Kubernetes cluster. We're getting, you know, metrics from the Prometheus and about right endpoint being funneled into the system. In Amazon, we have cloud watch logs, cloud watch metrics. We get cloud trail data. We have VPC flow data as well as all of the state changes for everything in our entire environment, whether it's RDS or Lambda or Nest3 bucket, we can represent it. And we also have some observed, some observed specific sources as well. So we have a very lightweight JavaScript library in our UI that will send every exception and every UI kind of click event that goes in. We have a bunch of open tracing spans from all of our backend services, which we use to actually build and construct more context, which we'll show you. We're big users of Snowflake. And because that's our query layer, we need to know what's going on in there. So Snowflake has a schema called account usage. And we're actually using the Snowflake data sharing feature to get all of the query information and warehouse information in the O2 so that we can correlate it with everything else. We have very observed specific application objects. So, you know, in the UI, you might interact with a monitor or you might be a user, you might create a data set. We're able to take all of that data from a relational database, bring it into the O2 so that we have context. So if I'm troubleshooting and I have an identifier that represents an object in the system, I can pull in all of the information about that configuration object. And lastly, you know, as I said earlier, we pull in all of our CISD information. So we have all of our Jenkins build events. We have all of our source code management events coming from Garrett. So we can see every commit that's going into the system where it's being built and the code that eventually goes on to run in production. So now that I have all of this data, we can do lots of stuff. I mean, the most basic stuff you can do is basic infrastructure, you know, troubleshooting and monitoring. So in the left here, you're looking at a dashboard. We've automatically generated that showing me the health of all of the containers running on a subset of pods. And I can navigate over and go look at logs and, you know, this is pretty basic stuff once you have all the data. It really comes out in the wash. We can do this for pretty much any service that we're using all over Amazon, as well as the stuff that we've built. And this is kind of what you'd expect to get in a more like monitoring tool. And that's just coming for free with all of this data. It's really cool. We have a big use case around monitoring our UI. So we have a very small JavaScript library that we've written that will send events directly to our collection endpoint. It spits out some JSON blobs. And every single thing that you do in the UI, whether you click on something or we change the page, or you get an error, it's being collected and sent to us in no two. And our big use case for us is really error monitoring. So when we have a fatal exception in the front end, we generate what's called a white screen of death. And these used to happen really frequently so much show that one of our engineers went and created a monitor that will fork off any of these exceptions. Into a Slack channel and we would then go and triage it. And what's really interesting is, you know, since creating this alert, we haven't had one of these exceptions in months. We used to get them all the time. So it's really important to ensure the engineers can get feedback directly on the code that they're running in production so they can know what to fix and make it better. As far as open tracing goes, I mentioned earlier that we have a custom open tracing, you know, back in that we've had to build out, but we have pretty good coverage pretty much all of our microservices are instrumented and we're able to use that data not just to look at like the performance of a span or a trace and know what's slow, but actually build additional context using that data. So I spoke about kind of the barebone stuff that you get for free that you can do with observability, but I think what's really amazing is what is with all this data, the other things you can do once you can really make sense of it. So we have a lot of tracing derived data sets, meaning if we think about trace data historically, everything kind of looks the same right there's a span that had some time frame that belongs to a trace. But that's not very like generic or specific to an application right it's a big abstraction. We're able to take all of that trace data and turn them in a data set. So our graph QL requests, which is a subset of our spans, we can rip out more information and make those more interesting right we can pull out the end point we can pull in the objects that are modifying all of our queries that are running in snowflake we can pull in the snowflake data. So we've used the trace data to derive additional data. We also have lots of PM use cases. So I'm able to go into the product and using our trace data and the event data coming out of the front end. I can understand things like how many users we have on a monthly daily or weekly basis. I can look at how much time people are spending in the product. And I can look at things like feature uptake and really you know drill in. For example, a few weeks ago, I wanted to know whether or not people were creating monitors against Kubernetes specific data. So I was able to go and observe. Find all the monitor objects that are being dumped out from the system and then look up whether or not those monitors touched Kubernetes specific data sets. And then from that I could get adoption numbers. So it's really, really useful for the product team as well. Stoflake is a really important tool for us. And we're able to get all of the query history warehouse information and storage use imported directly into O2 using the snowflake data sharing feature. So what you're looking at here are all the tables that we've pulled in from the snowflake and just exposed new observe, which is one of the things that we can do because we're running directly on top of snowflake. And this lets us do really interesting things like we can go look here at the performance of all the queries that are being run over this time frame and I can see the execution time those queries versus the compilation time. And what our engineers are able to do are drill in this data and ask more questions. So if I notice that hey, you know, some of those queries are spending way too much time compiling what are they. So I can drill down directly into the individual statements that were generated and look at the sequel itself. And how to figure out what's going on and look at the kind of warehouse that that sequel was run on in the snowflake world. Observe has application specific objects. So, you know, every app has a bunch of configuration stuff that resides in a database somewhere in observe. We dump those things out on a periodic basis. So all of the things in the UI that you see, you know, data sets, users, monitors, you know, worksheets, your single sign on. And this is represented inside of the product, which allows us to at any point in time join that data in to more typical machine data. So if I identify her for an observed user, I can pull in what that user's name is and what account they belong to and what they're doing in the system and really hop across all those different data sets. And also use it to debug data sets. So a lot of times someone might call us and say, hey, you know, something happened on my data set, we can go and look at all the changes that the user made throughout time, try to kind of narrow down the root cause. What exactly happened with any of those objects. And you know, we use this to do a few things like we use them to add additional context or troubleshooting, you know, if there's an error, what were the steps that we're leading up to error? What happened in the system? So some should we reach out to someone should our data engineering team look at the user that hit that thing and go find them and let them know they had a problem and go and help them fix it proactively. We also have lots of routine support scenarios, right? Someone deleted the data set, well, we can go into to and grab their configuration and restore it from that. Another one that happens quite frequently is people asking us if we're receiving their events, right? Is there issue on the collection side or is it actually being dropped at our, you know, at our endpoints at our ingress side so we can go and for any customer, you know, pull up their access logs or ingress logs and demonstrate that, hey, you know, we are getting events or no, we're hitting a bunch of, you know, 400 or 500 errors and then drilling the what's inside those requests that's really causing it. So I'm going to tie this together with a really fun scenario. So at some point, it customaryed out to us and said they got an error from one of their data sets and the error wasn't particularly useful, but the TLDR is their data set was not usable. So we were able to go and start with that data set because it's an object in the system and we asked observe, hey, can you figure out how to get from the user's data set that was defined down to the actual SQL queries that are being run in snowflake. And you can kind of see on the right hand side that observe figured out how to portal across all of those different things. So it said, hey, you know what, a data set belongs to a task run and the platform, which goes on in the scheduler, which is a lot of things that are going to be done. So it says, you know, run on a warehouse and then eventually in snowflake and the user troubleshooting doesn't really need to understand what's really going on in this flow, just that we can get there by using all the links along the way and we can flip this around too. So another scenario might be noticing at the lowest levels that a bunch of queries are having issues, we can actually then ask observe, hey, can you go from these queries and bubble back up and show us the data sets that customers are having issues so we can go and help them. And that's that bottoms up flow. It's the reverse of the portal. And CI CD. We're ingesting all of our build information so we can go under, we can understand when builds are really slow. We can trace the builds back to specific get commits as well as who reviewed and accepted those and look at all their comments, all in a single place. And this is really great when you do things like this where hey, we want to we run these a bunch of benchmark reports we can see over time whether or not any of our performance is regressing across the platform. And you know the use cases don't just stop there. We have lots of other use cases. For example, whenever we're filing bugs, we usually attach a worksheet that captures the information that the sign is going to need the troubleshoot. We also have web hooks that automatically open up geartickets for back and errors automatically. When we're doing post mortems, you'll frequently see links to worksheets or screenshots from the tool itself that let users really tell a story as to what happened in a timeline type of view. We always share these things in Slack, you know, when we're in some sort of, you know, word room or don't insert response. We'll go and we'll share links back and forth to add to each other's worksheets to really build up this investigation. A favorite use case of mine is actually for our SOC2 audit. So we became SOC2 type to certified last year. And one of the requirements was being able to audit all of the admin operations that were done in the system. So because we're early locking this information, we very quickly built a data set that allows us to go and review all of the changes to the system. And because we are getting all of the cloud trail information as well, we can do the exact same thing for every configuration change to everything in Amazon. So if something changed on a server, someone detached a volume, we can actually go back in time and see the specific user and whether they're logging into that point in time, who made that change. I hope that you've enjoyed this presentation. If you're having issues with reliability, the time it takes you to investigate issues and the cost of your tooling, observe maybe able to help. You feel free to check out our website at www.observeink.com and if you feel so inclined, requesting a personalized demo session. Thank you.