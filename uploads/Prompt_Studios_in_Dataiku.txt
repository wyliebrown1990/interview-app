 For any generative AI test, engineering a high quality prompt is the key to success. In DataIQ, we use prompt studios to design, test, and operationalize the optimal AI prompts to achieve our business goals. Let's say we're a financial analyst wanting to use a large language model to automate the process of detecting topics of interest and discovering new themes from thousands of news articles each day. We'll first create a new prompt studio for our use case. Although we can craft one-off prompts for ad hoc queries, we'll choose to build a new template since we want this AI prompt to be reusable as part of a production data pipeline. We also could start from an existing template, develop for a similar use case and adapt it. For our new prompt, let's start in manual mode to quickly test and iterate on our query before applying it to datasets in our project. We'll select the model we want to use. In this case, we'll use GPT 3.5 Turbo, but we also could use any other services our company has approved, including private models. In the prompt studio interface, we see sections where we can explain our tasks and plain English, add examples of inputs and the expected outcomes, and, since we're in manual mode, an area where we can provide a few test cases to see how the prompt performs. Let's try this prompt first. We'll ask the model to determine whether each topic from the following list is covered in the financial news articles provided, and then we provide the list of topics we're interested in. To help the model understand what will be provided as inputs to our prompt, we'll add some brief descriptions. Finally, let's manually add one or two test cases. We just need the headline and the text preview for a couple of different articles. After we click Run, we can see results in just a few seconds. Two things we can notice right away. First, the format of the output as pros isn't ideal. Because if we want to use those results downstream in a data pipeline, we'll want a more predictable structured format. Second, we can see that the large language model took some liberties with our task. For example, some of the topics it returned, such as vaccine, manufacturing, risk on mood and personnel changes, were not in the list of topics of interest we provided. Again, this random factor makes it more difficult to systematically use the outputs. Let's modify our prompt, and this time add the instruction to format the response as a JSON object with a defined structure. Since it is potentially useful to learn what other topics the model detects in these articles beyond those we specified, we'll also include an instruction for how to list additional topics the model discovers. Another thing that could help the model better understand what's expected of it is examples. Let's add in some sample headlines and text previews along with the JSON objects we would expect as output. Notice the other key at the bottom of the sample JSON object, which is the location we designated for additional discovered topics. We'll rerun and evaluate the new results. They look much better, structured in a format where we can easily extract the known topic flags and isolate the new ones for further analysis. As a final precaution, let's explicitly enforce that the output from the large language model conforms to our expected output format. Prompt studios in DataIQ provide many prompt validation and evaluation options, such as those you see here. Now that we're satisfied with our prompt, let's switch from manual mode to using a data set in our flow as a batch of inputs. We'll assign the column names that correspond to our article's headlines and text previews and click run. Now we can see the results for a sample of the data set. We can see that all of the articles in the sample passed the validation rule and that indeed the outputs are in JSON format. Prompt Studio also provides an estimated cost to run this prompt against 1000 records, so teams can assess the financial impact of embedding generative AI into data pipelines and projects during the design phase, or compare the cost of running the same prompt against different models. Since we're happy with the results when tested against real data, the final step is to deploy this prompt in our flow by clicking save as recipe. Our AI enrichment step is now saved as a new recipe in our project flow, making it not only efficient to reuse, but also visually apparent to everyone that generative AI has been applied. From here, it's easy for others on the team to inspect and validate both the final prompt logic and the resulting outputs. With Prompt Studios and DataIQU, you have the ultimate tool at your fingertips to engineer impeccable AI prompts infused with business context to maximize the value you can obtain from generative AI models. Happy Prompting!