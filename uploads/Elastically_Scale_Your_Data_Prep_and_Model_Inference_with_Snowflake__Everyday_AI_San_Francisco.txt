 So we have a brief agenda. So snowflake, I presume a lot of people would be knowing what snowflake is, but we're going to have a couple of slides on the snowflake. Snowflake works very closely with data IQ in terms of data science workloads. We're going to briefly talk about the data IQ partnership and the current capabilities, and then we're going to jump straight away onto the demo. So I just put one slide for snowflake, like, to people to understand what snowflake is and what it is. Snowflake is a data cloud. We bring best of both the worlds. On the left to right, when you see all these, we are capable of having all these data sources onto the snowflake platform are elastic and compute power running across all the cloud provider. We are, we can actually run all these jobs. All these workloads on top of it when you see in terms of collaboration, data engineering, cybersecurity, and data science in ML. But we're going to talk about the data science in ML today. These all these applications and all these workloads are sitting on top of the snowflake engine. So imagine a wall where you don't have to tune in any hardware. You don't have to tune in any software. Snowflake does that for you. We work across the cross cloud providers like AWS, GCPs, and Azure. So bring us the network and then we can be able to do that. So let's briefly talk about data science. What Snowflake does in data sciences and journal. I was a data scientist myself. Imagine a wall where I'm having multiple different silos of data sets. CSV12, CSV3. I think a lot of people can relate to that. So snowflake changes and helps you to scale that. You don't have to maintain multiple copies of data. There's one copy of data where everybody from anybody in your organization can tap into. And you can build what this slide kind of says is as a data scientist or an ML or as an analyst, 80% of the time, you see it's trying to get the data in the right order in the right format. So snowflake helps you to have the data in a much more secured and a govern manner and once and to a place built. So as I said, I'm going to go on copies and then the device. So when you are trying to deploy these machine learning models. So for example, the presentation just before that, they were trying ML flow. They were trying a lot of stuff doing and deploying those machine learning models. The snowflake helps you with the tools like data IQ, which actually sits on top of it. And you will see it in the demo briefly. So again, this is again, you can have any set of data. Structured unstructured, there's an amazing thing called snowflake marketplace. A lot of data scientists right now are an analyst. They want to tap on to if you haven't looked into us, there's something called snowflake marketplace. The tons of data sets are available paid or unpaid where you want to leverage the data set and you want to enrich those data sets. You can ingest the data. So right now traditionally snowflake was working on sequels, but now with snowpark, briefly I'm going to talk about it. You can run those Python and Java jobs exactly on snowflake engine as if you're running a sequel job. And our best best is called snowpark, which is briefly I'm going to. And when you look at all these top tools over here, so of course data IQ is one of our key players over here. So you can run all the way from data ingestion to the model deployment onto the data I can implement. So again, so snowpark is our one of the latest development framework where you can run Python and Scala Java. We went GA last year. So the motivation of launching snowpark was to run a data frame APIs on a lot of data scientists are used to running snowpark jobs. Sorry, a data frame set of jobs. But what we did is let's build this Java Python and Java on terms of top of it and you can run those jobs and call in onto the snowflake engine over there. Again, it's pretty streamlined. You can build a scalable and optimized pipelines on that in the secure and govern fashion. So the data never leaves the data cloud. So that is one of the USBs on there. Which I want to talk a little bit. Now this is a busy slide. But I think the conclusion of this slide is really that data IQ has the best in class integration with snowflake. As you can see from all the recognition and customer feedback. And I also say that because I was as ex snowflake SC, I noticed that how deep integrated data IQ platform is being able to really make the platform more efficient, especially when you're doing data preparation as well as machine learning workload. Now as a data platform, data IQ allows you to accelerate your business insights and AI within your enterprise at scale. Now regardless of whether you're doing coding or doing visual programming, behind the scene under the hood, data IQ is actually translating those to efficient operation down to snowflake through push down as well as snowpark API. So snowflake is not just a data warehouse, but it actually has very powerful computer engine and data IQ really take advantage of that to benefit you so that you don't have to worry about trying to manage infrastructure to support your AI and for a little workload. Now whether you actually are doing coding as TITLE was mentioning or whether you're training model inside snowflake or outside snowflake or in data IQ, we can manage all that approach, different approach within data IQ so that you can actually have visibility and governance for all your ML project. So with data IQ, we basically impose our team members of your enterprise to be able to contribute to an analytics project, whether you are business experts or data experts that can code. We also have accelerated that focus on delivering value to vertical industries. So with a combined solution like this, you're able to deliver impact of business very quickly. Now even though we give you the flexibility of being able to create a project with the coding or visual recipe, we do that in a governed way so that you have a trusted environment where you can see how data is being transformed and then because of the well thought of data governance that snowflake put in place and we're not really moving data or making copies of those data, the governance and policy that you set up in place in snowflake will definitely propagate throughout your data IQ project as well. So preserving that eligibility for the benefit of compliance. Now we're going to this kind of architectural diagram. This is something that if you're a snowflake customer, you are very familiar with. So I'm just not going to take too much of time. So this is a classic reference architecture that we have come up for data IQ from left to right. As I said, I was pointing out of snowflake data clouds from we're ingesting the data. There are multiple ways that you can ingest data using snowflake and data IQ in the demo. They're we are actually ingesting it from an S3 bucket. But if you have a streaming data or the stuff of data, you can ingest the data. Once the data has been there. So there is something called a data prepper feature engineering and I think I'm sure as a part of the demo to a lot of demos today, you would have seen there's a lot of visual recipes. So these visual recipes have to run somewhere, the compute has to run somewhere. So that is running inside snowflake. So there are around 100 plus visual recipes. The underlying is a Java UDF. For example, I'm being a data scientist. You're collaborating on a project with somebody is an analyst. So there are SQL recipes over there. You don't have to or there is a you are trying to count the number of missing variables that you're trying to do the median or more. So those visual recipes can be used and that compute we are pushing down on snowflake. And the data is never leaving snowflakes. And the whole concept of snowflakes is we don't want data to data in should be no be influx on of the data and data should never leave. Again, on the as I said, the Python snowpark, we were going to show showcase as well. So you can record recipes inside the data IQ and snowflake. So you can run those snowpark jobs over here. Of course, on the training side, you would have seen that there you can run visual recipes and you can actually do the training inside using snowpark or stores procedure as well. On the on the extreme right again, like you can actually create a dashboard data data I could do you want to talk about the MLOps and so as you know, creating a model, training a model, deploying model is not one time event. You have to really monitor the model. So it's kind of the ML ops that data I could provide you as well as being able to create that last mile insights for your business user, being able to create web applications, security within data I could environment and with the latest stream let being able to do that efficiently without needing to wait until snowflake has that runtime engine. Now you could actually deploy stream let security with data I could leveraging all the interfaces such as talking directly to snowpark API. Yeah, so do you want to so we just going to quickly go through the training is going to it's going to start a demo and then I think going to talk through what the basic integration points over here. Yeah. So this is I'm sure you have really seen this in the face of quite a bit already. Now obviously in every snowflake customer has different points in their journey. If all your data is in snowflake great, we can just connect directly to it. But if it's not some of them are still outside, you can also be able to tap into through the connector that we have being able to start the business analytics project without needing to wait until all your data is migrated to snowflake. Thanks for data since snowflake you can see in this diagram the square boxes are the data sources whether you're doing visual manipulation of the data or code centric everything is all available through you. It what you whatever you create as an output can be fit into the next step whether it's code or visual. So here I have a visual joint recipe. Maybe you can just use the click clicking through the options and behind the scene data is actually converting this joint recipe into sequel. So what actually gets executed is actually being passed directly to snowflake in place of the database and doing this joint. So there's really no ways of data transfer that will aisle in order to execute that operation. So maybe can you just click on one of the samples all these blue items are saying is a snowflake data says that we big. So imagine a world where you have millions and millions of rows in a practical situation. So watch snowflake and data I give you trying to do is you can look at of course this data set the sample size of 10,000 let's assume a 1 million rows of there you can configure a sample let's assume 100,000 or from a million rows and you can build up those sequel joints and other joints on there and push down compute on snowflake. So those sequel queries are actually pushing down on the snowflake to compute. So you're actually using a cross crowd for the best of both the words that the security in the government is the data is not leaving. So if you want to go back on to the so on this data says if you see the one second on this so there are as I was saying there are close to 100 plus visual recipes which are pushing you for a non-corders but all these are underlining sequel codes and the Java UDS underlining it and you can push down compute on snowflake on that. Yeah, yeah, yeah. So imagine your business analyst able to do this join but then there's advanced data processing a feature engineering company to be done by data engineer who codes in Python. So naturally this platform will allow you to do that collaboration without going to different tools and this is leveraging a snow park all the Python code data frame operation here gets translated to under the hood into sequel through the API, snow park API. And on these codes if you open the code so yeah all these are snow park codes over there so if you're importing the snow park sessions on so this is like running the compute on that if you are familiar with data frames style APIs so these are as close as that you can get so this computer is exactly is running inside snowflake. Yeah, please. All right. So now that you have the data prepare when the time to do a visual other ML which you have seen before but in this scoring operation here as a user you're just going through this auto ML and and doing the scoring but the actual scoring code is actually executed in place inside snowflake. They like to actually automatically generate the Java code necessary for that inference and push it down transparently to snowflake platform and you get the result in snowflake table. So this is actually user doesn't it's not even aware where that code is being executed but we do that by selecting the most efficient engine to execute that engine code. Now when it comes to coding for example you know that snowflake does have a very powerful Python and Java environment for you to write custom code so as that was showing you you can actually do training outside of data cool and then be able to monitor that same thing with snowflake. Let's say if you were to write a very efficient code centric custom training code in snowflake we can actually take and log out those training session result and manage it from data cool so that you can compare results and be able to manage the lifecycle deployment of that particular model. Yes one point I just want to make so here you see the two ways that we are kind of depicting how the training can be training jobs can be done for machine learning. That's a for the first one is a classic case of an auto ML I'm sure you've got it sort of seen you you you you you throwing up the data splitting of the data so that's splitting up happening in snowflake all the computers pushing down on snowflake and when you scoring what Chen was saying when you when you score you export as a Java UDF inside the stage of snowflakes up imagine like you have tons and hundreds of models that you have trained there's a fresh incoming batch of datasets at a pretty many seconds you want to score so you can take the best of both the worlds and the scoring will happen instantly without the data ever leaving it there. On the second case what as a for those are demo purposes so there's something called store proc for a proc from snow from snowflake from from the snowbox so what we're trying to do is the training right now currently is not pushing down compute on snowflake on the auto ML but we are working on something in future this code over here we are creating an environment as a store procedure in snowpark and push and this is a simple I think random far smaller that you've been picked it up and that compute is kind of pushing down on snowflake as well so there are multiple ways that you can achieve in that yeah and following the ML flow framework you can start locking hyper parameter metrics and data I could pick that up and store for you all right so even though we're focused on the visual part of it people would think that data I could actually visual first but as you saw in Tyler's demo that coder is much very much a first class citizen so just picking off on that this is a visual code studio inside data I could almost server not on your desktop right being able to start letting you code and in this case we actually coding a stream that application using snowpark talking directly to snowflake and be able to publish an application that is UI driven this actually you can debug your stream that code side by side in data I could today and once you're satisfied with this application you can then deploy it publish it as a web app which I have here as a dashboard application here and the nice thing about this is that like Tyler was saying the back-end infrastructure that powers this stream that application is actually all managed by data I could so as a developer you don't even think about where you can put that code and then it's securely managed by data I could in terms of permissioning but the compute happens exactly so anytime you toggle up any of these fields over here it runs in in snowflake and so the it's kind of best about the worlds yeah so I think that that was there for today like well so there are we have enough guides actually one of the guides is there if we can link up in there so it's so we if you want to play around with snowflake and data IQ you can always go to this quick start guide on and we offer 30 days I think 30 days trial of snowflake you can leverage the partner connect over there and use data IQ and all the steps that we have talked about so you can see the best of all ultimately snowflake is doing a lot of work in data science and machine learning right now so data IQ and we have partnered up pretty well for the last they have been a data science machine learning partner for the last two years and we have seen a lot of successful implementation thank you thank you