 Hi, my name is Badge and I'm one of the engineers here at Observe. Today we're going to take a walk through the Observe solution and focus on some recent updates to the platform, particularly with respect to alerts and metrics. To begin, I'm actually not going to dive right into Observe. Instead, let's take a look at this Slack channel where I get alerts from Observe. Recently, an alert fired, telling me that some of our customers are facing errors. One of the key properties of alerts in Observe is that they leverage GraphLink to provide context around the notification. And in this case, even though we don't have a lot of errors, we seem to have a subset of customers who are experiencing a high error rate. And this could lead to some poor customer satisfaction. So let's dive in. Now I even observe. Here I get an overview of that notification. I can see that this is still an active incident. And for us, like any other SaaS company, when issues occur, a key question is who was impacted. This page helps me answer this question without diving further, so that I can evaluate the impact of the problem. For example, it looks like a few more customers have been added to the impacted list since we received the alert. Let's pause and think about what the root cause may be. Customer impacting problem could easily be pods stuck in pending state, maybe no memory usage over 90%, perhaps repeated fail logins through database, or really numerous other root causes. And in all of these cases, we want to get specific answers out of our investigation to understand the impact of the problem. By answering questions like which databases effected for which application in which AWS region? To do so, let's go take a look at the data that fired this alert. Now I can see the log lines that cause my alert to fire. I immediately noticed I have a lot of out of memory errors, which is a little alarming. If I scroll across, I can see more information. Like here, I have the stack trace for that error. But right now, still not really sure if this is a code issue or resource contention at the infrastructure level. I need to do a little more digging to figure that out. Because these datasets are linked together and observed in a relationship graph, I can jump to the Kubernetes pods that generated these log lines. Here, I see the pods where those error messages came from. These pods are currently active. And using the time scrubber at the top, I can travel back to an earlier time when these pods came alive first, which seems to be shortly before my alert fired. Now, what I'm really curious to look at is the metrics for these pods. If I scroll down, I automatically get in context metrics for these pods. And notice that I didn't have to hunt around following tags, carrying around names or IDs for my logs platform to my metric platform. Because I observe is a one stop shop for my logs metrics and other technical or business data, I can pull together metrics for my resources with ease. So because I was getting memory errors, it's really the memory metrics that I'm interested in. Scrolling down, I see a chart with my CPU usage metrics. And I can open it to see a more detailed view. And, aha, I see the signature so-to shape that is indicative of a memory leak. This is starting to look more like a code issue. The final piece of validation to check is whether these pods are restarting. To do that, I'm going to look at the notifications for these pods. So, observe alerts serve a larger function than just not finding you, be a slack or pager duty. They can also be used in context of an investigation. Here, I see notifications about pods restarting frequently, which is the last piece of the puzzle. So, I quickly got to a good spot in my investigation. I know which deployment is failing. I can open a ticket for the relevant team with this link so they can see the problem in context and fix it. But, I can take this one more step further. In my environment, I'm using continuous integration and deployment. My CI-CD data also comes into observe. By using Graphlink, I can ask Observe to find build events from Jenkins to help me figure out the exact change that introduces code issue. Let's click navigate to here and select Jenkins builds. Before we take a look at the build data, let's see how we got here. At the top of the screen, we have the breadcrumbs. We started out by looking at the error logs, then jumped to the problematic pods, and then Observe seamlessly took me from pods to the relevant Jenkins builds. There are actually several hops in our relationship graph we have to go through. But as a user, I didn't need to know what path to take. Observe knew that pods are composed of containers that are running images that are built by Jenkins. Okay, let's go back to our Jenkins build data. Here, I see the exact change that codes issue. Apparently, Tom made a small change to cache code, which calls a memory leak. I can now tell Tom that his recent change is impacting our customers and needs to be fixed. Well, there's more more thing. As you remember, we started this investigation with an alert telling us that our users were experiencing errors. Well, I wonder if some of them actually open support tickets related to this. Using Observe, I can actually find the Zendesk tickets that are potentially related to this bad code change. Again, leveraging Graphlink, I can jump to my customer tickets without worrying about the underlying path. Here, Observe is showing me all the support tickets opened by users who interacted with that problematic build. I can let our support team know that we identified root cause of the issue, and we can communicate our customers that the issue will be resolved shortly.