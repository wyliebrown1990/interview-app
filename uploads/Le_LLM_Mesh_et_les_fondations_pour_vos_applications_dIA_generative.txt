 Ils participent sur les samAcadreulas, dans les pays nich cassette et leur drewme dans les hesоминаAllah. Ça, c'est signed, on aplinés rapidement et toenillés. Normalement crèées et été occupées pendant la Alzou Ughman, nous sommes accelermes pour les gens erais à katouz, c'est Physiстро chocolate, et puis tous ceux qui sont Damenなんes, ont également attendies pointelели avec les « Helms à Ca», des fonctionnalités d'IA, de machine learning, d'opérationnalisation et de gouvernance. Mais aujourd'hui, je vais vous parler de ce qu'on a fait un peu plus, manière un peu plus pragmatique sur l'IA génératif. Donc, je vais commencer un petit peu par une petite intro sur, nous, qu'est-ce qu'on a fait dans DataEcoup comme application d'IA génératif, les fonctionnalités qu'on a implémentées et la manière dont on l'a pensé dans le produit, et puis vous faire un petit peu une ouverture sur ce sur quoi on est en train de travailler en ce moment, ce qui va arriver dans les prochains mois. Donc, avant de rentrer un petit peu dans le vivre du sujet, et ce qui va vous permettre de construire vos propres applications d'IA génératif, qu'est-ce qu'il y a une application d'IA génératif ? L'exemple que je vais vous donner ici, c'est AI-PRIPER, qui est une des fonctionnalités de DataEcoup qui va bientôt être public et AI-PRIPER. Donc, c'est vraiment un exemple assez parfait de ce qu'on peut tirer profi du l'IA génératif et faire des applications puissantes. Donc là, je vous montre un petit exemple. Donc, on a cette recette visuelle et la recette de préparation de données dans DataEcoup, il y a un nouveau bouton qui déclenche une fenêtre de dialogue, comme vous voyez ici, où finalement, l'idée, c'est vraiment de changer complètement l'expérience de préparation de données, au lieu de réfléchir aux différentes étapes de préparation de données que vous devez appliquer pour transformer la donnée, vous avez juste besoin d'expliquer avec du texte naturel, vous expliquez ce que vous allez faire. Là, c'est en anglais, mais ça fonctionne réagent également en français. Et l'assistant qu'on a développé va venir transformer ça, le langage que DataEcoup comprend et pour proposer les étapes de préparation. Donc, la manière dont on l'a développé nous, c'est que ça soit complètement transparent et que l'opérateur, par exemple, de data-analyses et en capacité de voir ce qui est proposé, de changer, de re générer. Et derrière, ça ajoute les étapes de préparation de données, vous pouvez les modifier. Juste ici, c'est illustré comment est-ce que nous avons essayé de jouer avec l'IA Generative et l'intégrer dans la plateforme et que ça puisse servir finalement à l'intégralité de nos utilisateurs. C'est comme ça que, à priori, tous nos utilisateurs vont toucher du doigt à l'IA Generative à travers ces fonctionnalités, ces assistants qu'on est en train de développer et d'implémenter. Et à être préparé, c'est probablement le premier que vous avez pu voir dans le produit. Mais on en a d'autres, ça a été un tout petit peu évoqué aussi avant. Globalement, on essaye d'en rajouter un maximum pour transformer complètement l'expérience de préparation de données et de machine learning dans une entreprise. Donc, j'évoquais AI-PRIPAR, mais on a aussi travaillé sur des assistants de code. Donc, pensez-cope-lotte, est-ce qu'il a été implementé déjà sur GitHub, sur VS Code. Nous intégrons des assistants pour générer du code, le documenter, faire des tests unitaires, un petit peu partout dans nos interfaces de code, donc, dans Code Studio et dans nos recettes visuels. Nous j'ai pitté en notebook. Et puis, le troisième, finalement, grand assistant sur lesquels on a travaillé, c'est la partie plus d'ocumentation, génération de métadonnais avec de liens génératifs. Donc, ici, la flow, donc ça va être des flows zones, ça peut être des data sets, ça va être des recettes de code ou des recettes visuels, générer de la documentation à partir de ça. Donc, quand vous pensez un petit peu, ce que Kenji vient de vous dire sur les différents patterns d'application qui se retrouve un petit peu systématiquement, là, ce qui est intéressant, c'est qu'on voit, c'est à peu près la même chose. On a trois assistants et c'est quasiment les trois un peu patterns qu'on voit, du Q&A pour et il y a eu pripère, de la structuration d'information avec la partie flow, documentation, flow, explication, et puis de la génération avec les codes assistants. Donc, on a un peu utilisé le même processus que ce qu'on recommande à nos clients et à nos prospect, aux gens avec qui on travaille. On a commencé par ces patterns-là et on a commencé à faire des choses de ce type. Maintenant, on est vraiment là pour parler plutôt de ce que DataEQ va vous permettre de faire vous et comment ce que ça va vous permettre de faire vos propres applications d'illégénérative. Et donc, avant de détailler les fonctionnalités, je rebondis un petit peu sur ce qui a été dit par les différents interlocuteurs avant. Mais on essaie vraiment de résoudre les quelques problèmes et les barrières qu'on a identifié à l'essor de l'illégénérative dans une entreprise. On envoie finalement de gros sujets. Le premier, c'est que ces détechnologiques sont quand même assez avancées et qu'il y a aujourd'hui assez peu de personnes qui sont en capacité de les maîtriser et d'entirer profils au sein de l'entreprise. Donc il n'y a pas assez d'experts pour utiliser et manipuler ces technologies-là. Et le deuxième, c'est finalement ce fossé qui existe entre ces technologies innovantes, excitantes, et une nombreuse et la réalité dans une entreprise. Et les gros freins qu'on est identifiés nous, premier, c'est que, voilà, avec les services déagénératives et notamment les API qui sont fournies, il y a un risque de donner son simple sorte de vos établissements, de vos infrastructures. Il y a un risque d'avoir des hallucinations. Ça, c'est les nouvelles problématiques qui sont en train d'emerger avec les ALM et l'illégénérative. C'est que ça invente du contenu. On a un risque aussi de propagation de biens. Ça a été entraîné sur les données de l'internet et donc ça propage jusqu'à ça a appris. Bref, un certain nombre de problématiques de sécurité, de confidentialité et de confirmité, qui nécessite de mettre en place la gouvernance et de la sécurité. Le deuxième frein, ça a été dit aussi à nombreuses reprises, mais c'est la technologie elle évolue tout le temps. Chaque GPT s'assortait il y a un an et depuis la deux semaines, OpenAI allons c'est le GPT4. Il y a un modèle qui sort tous les jours, des services qui sont en train d'emerger tout le temps et vous pouvez pas vous vous enfermer dès aujourd'hui dans une de ces technologies. Vous avez besoin de garder vos choix ouvert et une forme d'indépendance par rapport à ces fournisseurs. Vous avez besoin d'agilité de changer pour pouvoir utiliser les dernières technologies disponibles et les comparer entre elles. Et le dernier, finalement frein, c'est les coups. Par tourner un modèle de langue comme la MA2, un modèle open source sur ses propres infrastructures, ça coûte beaucoup d'argent et de manière générale, ça coûte de l'argent. Et donc, ça aurait percuté sur les coups de filles d'appays que les providers vont mettre en place. Vous avez besoin vous, que vous allez faire émerger ce genre d'application dans vos contextes de les contrôler. La réponse qu'on apporte à ces problématiques-là, ceci autour de deux groupilliers, le LLMMesh, ça vous a été présenté en ces en détail, je ne reviendrai pas sur les concepts qui ont été décrit par Kenjim et je vais vous montrer en pratique les fonctionnalités que c'est et les quatre niveaux, sur lesquels je vais rentrer un petit peu plus en détail pour vous expliquer aussi ce qu'on a fait aujourd'hui, ce qu'on compte faire dans les mois et qu'à rive. Premièrement, lLMMesh, c'est vraiment la partie fondation, c'est la plomberie, c'est ce qui va vous permettre d'utiliser ces technologies sans avoir à vous soucier de toute la partie infrastructure technique et c'est pas concept, c'est vraiment des fonctionnalités qu'on a implément tant que d'un d'adais coups et donc que je vais vous présenter. Ensuite, ça a été un tout petit peu vu ce matin dans la démo, mais globalement, tout ça puisse sur ce concept de connexion à lLM qui ont vraiment être cette couche qui permet de découper la couche applicative des services sous-jacents. Donc on a une toute toute une série de services, on s'intègre avec les derniers providers de lLM, les vectors sortes des modèles open source, des modèles privés et ça se traduit par toute la série ici, vous voyez dans les interfaces visuels de typologie de modèles, vous avez le choix entre du GPT3, 0.5, du GPTK, du coquillin ou alors un modèle qui fournit côté oeuvron face, ici vous voyez que c'est aussi accessible dans les interfaces de code. Et quand on regarde un peu plus précisément ce que c'est qu'une connexion à lLM, c'est un choix de modèles qui sont fournits par le type de provider et un ensemble de paramètres qui vont venir justement apporter cette couche de sécurité. Donc là vous avez la capacité de définir qui a accès à ces services. Vous pouvez aussi contrôler comment toutes les informations vous êtes logues, c'est-à-dire toutes les inputs, outputs, etc. et vous avez accès derrière à toutes ces informations là, donc vos audits log. Donc là c'est la forme un peu brut, mais data-écoupe permet aussi les utiliser sous la forme d'un data set structuré. Et ça c'est intéressant vous voyez ça comme ça parce que je vais vous montrer un exemple de monitoring de coup avec ces logues là. Est-ce qu'on a rajouté également pour essayer de rajouter cette partie gouvernante, sécurité et on va dire contrôle de filtre, c'est un certain nombre de choses que vous voyez ici, donc on peut regarder détecter s'il y a de la donnée nominative qui va sortir et bloquer les requêtes si jamais sans on est. Des appels et de modération qu'on peut rajouter, donc on détecte si la réponse du lm est oxie, ou même si ce qu'on demande au lm est interdit. On peut aussi rajouter des filtre qui vont dire, je ne veux pas que le nom de mes employés ou des clients sortes, donc à certains nombre de termes interdit. Et là, un exemple de comment est-ce qu'on peut faire du cost monitoring avec data-écoupe, on utilise justement toutes ces fonctionnalités, vous voyez ici, vous pouvez faire de l'agrégation par projet, par application, par typologie de service et vous avez un peu un monitoring détaillé de quelle sont les applications qui utilisent et à lm, combien ça m'a coûté et voir un petit peu la tour de contrôle de ce qui est fait avec data-écoupe et avec lm. Donc, le lm mâche, comme j'essaye de l'industrie là, c'est pas un concept, c'est vraiment des fonctionnalités très concrètes dans data-écoupe qui vous permettent d'utiliser cette technologie-là dans un contexte d'entreprise et sécurisé. Maintenant qu'on a finalement un peu clarifié ce socle et ces fondations, le deuxième volet de notre field de notre map ça a été d'ajouter les fonctionnalités qui vont permettre de rendre cette technologie-là accessible à tout le monde et c'est ce qu'on a vraiment, ce sur quoi on s'est focalisé au début. Donc, Kenji disait tout à l'heure, comment c'est avec des choses simples, comment c'est à des techniques simples, parce que exactement notre manière de voir les choses, on a commencé par s'appuyer sur ces premiers niveaux pour que ce soit accessible à la majorité et on va commencer à traiter les thématiques beaucoup plus techniques et beaucoup plus avancées par la suite. Donc, premier point, c'est d'HLM et même des choses très d'y automatiques du langage, sont maintenant des choses que tout le monde peut utiliser au quotidien et c'est ce qu'on a voulu faire dans data-écoupe. On vous en a recède visuel près d'allem poids qui embarquent du LM, qui embarquent des templates pour faire de la classification du résumé texte et je vais vous montrer un exemple de ce que c'est dans data-écoupe à l'instant. Ça c'est bien pour traiter quand même un certain nombre de cas d'usages, mais parfois on a besoin de mieux contrôler ce que va faire le LM, mieux contrôler ce que va faire liage généritif et donc on doit faire un prompt et ça c'est tout ce qu'on appelle derrière le prompt engineering et data-écoupe à fait prompt studio pour vous permettre de définir vos promptes, de les tester, de les comparer. Parfois encore une fois c'est pas assez, juste de faire du prompt engineering, on a besoin d'aller un grand plus loin, on a besoin d'ajouter ces propres données de connaissance pour adapter le résultat du modèle. Donc ça c'est la technique qui est un peu connu sur le terme de rétribulogment et de génération, que j'expliquerai rapidement tout à l'heure pour ceux qui ne connaissent pas cette technique là et dans data-écoupe on a voulu rendre ça accessible et plus facile. Et donc on a une fonctionnalité, on a des fonctionnalités qui permettent de faire de l'adaptation beaucoup plus simplement et puis pour finir les techniques avancées sur lesquelles je ne viens le présenter à rien aujourd'hui parce que c'est des choses sur qui l'on a actuellement en train de travailler mais je vous donnerai un petit peu les perspectives qu'on a sur ces sujets-là. Donc premier niveau des templates et des recettes visuelles pour faire du traitement automatique du langage de manière très simple. Globalement c'est assez facile, ça va être une nouvelle recette dans le flow qui va venir, donc vous vous mettez un data cet en entrée, vous pouvez opérationnaliser ça et par exemple résumer ce que vous voulez, des appels clients, des converses étiquées support, des résus des product reviews, ce genre de choses, de manière complètement visuelle. Donc on peut faire le tout sans une seule ligne de code en choisissant le modèle qui a été autorisé pour les cas d'usage par l'organisation et de manière très simple. Donc comment ça fonctionne en pratique ? Vous avez des nouvelles recettes qui vont apparaître. Là je prends un exemple de classification. Je peux utiliser des choses où il y a déjà les classes embarquées parce que de l'analyse des motions, c'est des choses qui sont incestandards. Le currence je choisis de faire de la classification propre, je vais moi-même décider de choisir la typologie de classification que je veux faire. Le cas d'usage que je vous présente ici, c'est de la classification de nouvelles Reuters. Donc là je choisis mon modèle, je choisis co-hear, mais ça dépend vraiment de ce que vous avez envie d'autoriser dans votre entreprise. Je décide de classifier les descriptions associées à chaque news d'articles de Reuters. Je mets mes classes, c'est un moment en tant que data analysis qui décide les types de logiques classes que j'ai envie d'avoir. Là je les rentre. Pour aider le modèle et l'appuier dans sa prédiction, je vais choisir des exemples. Je vais lui dire voilà un peu l'input que la description et la type de classification que j'ai envie d'avoir. Donc ça c'est juste après la joue d'exemple pour aider le modèle à performer. C'est ce qu'on appelle une contexte learning pour ceux qui veulent avoir les détails. Mais c'est aussi simple que ça. Je peux en rajouter un certain nombre et je peux aussi décider d'expliquer, d'arrerjouter un peu une explication. Pourquoi est-ce que le modèle a décidé de prendre tel class ? Donc ça me rajoute un petit peu d'explicabilité à mon processus. Globalement c'est tout ce que j'ai besoin de faire pour avoir ma classification. Je fais run, ma recette s'exécute et puis j'ai mes articles qui sont classifiés selon ce que j'avais envie d'avoir. Donc vous voyez ici la colonne prédictée de class et on peut faire derrière pas mal de choses, on peut faire des graphiques, on peut faire ce qu'on veut. Là je fais une analyse très simple, je regarde quel est le pourcentage d'articles qui parle de tel sujet ou non. Mais globalement ce que j'avais envie d'illustrer ici c'est que c'est très très simple de faire du traitement automatique des langues avec ces recettes visuels alors que c'était quand même restreint à une population un petit peu expert au paravant. C'est ce qu'on a voulu faire, c'est rendre ses cadusages assez standard, très accessibles. Le deuxième volet donc on rentre au niveau du dessus et je vais pas vous faire une démo de cette feature-là, vous pouvez aller l'avoir sur notre site internet si vous avez envie. Mais c'est notre prompt studio, globalement c'est vraiment l'interface qui a au permettre de tester des promptes, tester des modèles entre les comparés, regarder les coups. Donc vous allez pouvoir voir ma tête meilleure performance mais il est deux fois plus cher. Donc finalement vu les quartes de performance je préfère prendre, j'ai peut-être 3.5 plutôt que de payer gpt4. Vous allez pouvoir opérationnaliser ça, c'est à dire une fois que vous êtes satisfait du prompt que vous avez fait, transformez ça en recette et ça fait de l'opérationisation dans le flow. Et puis vous pouvez aussi enregistrer des templates, donc ça c'est cette idée de réutilisabilité de collaboration entre différents types d'utilisateurs. C'est un template de prompt, par exemple vous avez créé quelqu'un à travailler longtemps sur unquel usage de classification de ticket support. Une fois que le prompt à les bonnes informations, les bons exemples pour performer, ça ça peut être réutilisé par n'importe qui dans la structure pour faire la classification. Donc c'est idée un petit peu de l'intégrer dans les frameworks de datayku avec cette notion de collaboration et de réutilisation. Maintenant on passe au niveau 3, cette idée d'aller un peu un cran plus loin de ne pas utiliser le modèle tel qu'elle, et rajouter de la connaissance qui va vous être propre, ce qu'on appelle les techniques de retrieval augmented generation. Et globalement l'idée de ces techniques là, c'est de combiner de la recherche sémontique avec l'IA générative. Donc le cas d'usage pour expliquer un petit peu comment ça fonctionne, c'est j'ai une base documentaire, par exemple, toutes les règles de sécurité dans mon usine. Et j'ai envie d'en faire une base de connaissance facilement accessible. Donc cette document, je les coupe en petits morceaux et je fais ce qu'on appelle, je vais faire un vector store, donc je les restocque de manière structurée en ambéding, donc une forme vectorialisée de ces données-là, ce qui a un permettent derrière quand un utilisateur a posé une question, par exemple comment est-ce que je fais tel l'opération, l'opération, ce texte là et rechercher sous la forme d'ambéding, donc fait de la recherche de similarité, je ressort tous les bouts de documents qui correspondent à cette opération-là. Et le texte et l'algorithme d'IA générative va en faire une réponse structurée pour l'utilisateur et non seulement il a sa réponse à sa question, mais en plus il a les sources associées à la réponse, donc il peut aller voir les documents plus précisément qui vont lui donner les exemples concrets. Donc c'est ce qu'on appelle, Retrievellogmented Generation, qui combine l'LLM avec des bases de connaissance. Et ce qu'on a voulu faire dans DataEQ, c'est comme ce cas d'usage, je revenais tout le temps avec chez tous nos clients dans tous les secteurs industriels et dans toutes les zones du monde, le rendre, on va dire trivial à faire. C'est ce que vous voyez ici à droite, à partir d'une base documentaire avec une recette très simple de visuels qui permettent d'extraire le texte de PDF, HTML, de Doc, on crée une date à cette, avec ses données structurées, et on fait une recette d'ambéding, donc on crée un vector store et à partir de là, très simplement on enrichit un modèle, et une fois que ce modèle est enrichi, pareil on peut faire des pompes et on peut faire des applications et notamment là, où vous avez un exemple de template de web-able qui permet de faire notre interface du Q&A. Et globalement l'idée qu'on a là, c'est vraiment, en même pas une heure, on arrive à avoir notre Q&A sur une base documentaire et ça a été fait par quelqu'un qui n'est pas un expert de diagénérative. Pour aller un peu plus loin sur ce qu'on a aujourd'hui dans le produit, ce qu'on se verra quoi en tant, aujourd'hui on a, vous avez encore développé votre application de Q&A, on a des templates pour ça, mais ça reste quand même quelque chose qui doit être rajouté comme il ouai-bap. Ce sur quoi on est en train de travailler en ce moment, c'est vraiment compléter toute la boucle en version locale, mais aussi permettre des cas d'usage beaucoup plus avancés. Donc là, je parle vraiment de la première application simple, mais on peut faire des choses très très complexes avec ces techniques de rétribulogmentation generation, donc on travaille aussi à donner les outils et les moyens à des utilisateurs, des datations artistes, des développeurs pour aller plus loin encore et faire des choses beaucoup plus exotiques. Et je pense que c'est aussi l'ouverture que j'avais envie de donner aujourd'hui, c'est que là, vous avez vu dans le détail des fonctionnalités qui ont bien de vous présenter, qui ont été développées à reprendre, réaliser il y a quelques semaines. On a vraiment cherché à traiter ce premier niveau de complexité, parce qu'on pense qu'il y a énormément de valeur qui peut en être tirée, et que c'est ce qui permet de rendre cette technologie-là accessible à beaucoup de monde dans une autre prise. Maintenant, on va aussi aller s'attaquer au niveau plus avancé, donc on travaille sur des recettes de Feintuning, une fois qu'on a Feintuning son modèle, que ce soit aussi accessible dans le LLMX, et que ça bénéficie de tous les avantages que j'ai évoqué précédemment. On est en train aussi de travailler sur des méthodologies d'orchestration d'agents, donc tout ce que aujourd'hui va se faire par du code, mais l'idée c'est de mettre encore plus d'outils dans la main de vos développeurs et de vos datat scientifiques pour qu'il puisse aller encore plus loin. Et on travaille aussi sur pas mal de choses autour de l'opérationnalisation des LLM, donc comment une fois qu'on a, je ne sais pas moi, déployer et développer une pompe de récipie, comment est-ce que c'est le site qui peut être en souparmes d'API, comment est-ce que c'est ce qu'est ce qu'est-ce qu'allable. Tout ce genre de choses qui vont vraiment amener l'opérationnalisation à un cran au dessus, et ça fait un peu la transition avec la présentation qui a vous êtes fait un petit peu plus tard sur notre roadmap autour de LLMX. Si vous avez envie d'en savoir plus, n'hésitez pas à venir me voir, à venir nous voir une grosse partie de l'équipe produit de datatécou et présente aujourd'hui dans les centres produits là-bas. Il y a beaucoup de choses sur notre site internet, et puis n'hésitez pas à nous contacter via vos représentants, vos Ae, vos CSM. On aime vous parler, on aime avoir votre feedback, et moi particulièrement, j'aimerais savoir ce que vous pensez de nos fonctionnalités pour voir comment on peut les améliorer dans les semaines qui arrivent. Je vous remercie.