 Hello, my name is Christina Shao and I'm on the Product Team at DataIQ. As you know, there are many data and analytic solutions out there today. So what makes DataIQ different? First, we offer one single native product that brings together people with diverse roles and technical skill sets in a collaborative way. So whether you like coding in notebooks or prefer using visual tools or fall anywhere in between, DataIQ is for you. Next, we recognize that your tech stacks and processes are continuously evolving and that lock-in can be a major concern. Our platform is infrastructure agnostic, extensible, and integrates with all the major cloud providers, on-prem technologies, open source frameworks, and other enterprise software. And finally, DataIQ is designed for the governed and efficient scaling of all your data projects, whether that's automating your own personal data preparation workflows all the way to the most advanced industrialized AI applications. We're honored to be part of today's event and I look forward to our time together. Echoing what scientists are telling us, floods and extreme weather events are becoming more severe and frequent year over year. Imagine we're a small shipping company, looking to place a new warehouse somewhere in Florida. Our goals with this project are first to better understand different areas exposure to floods, and second to find the right insurance strategy for our new warehouse to mitigate risk. We'll approach these questions with the combination of descriptive analytics and machine learning. Our entire project structure is visually represented in the flow, and here's where the team came together to contribute their unique skills using a combination of visual recipes and code elements. First, our analyst source data from OECD, the Coastal Flooding Information, as well as from the National Flood Insurance Program. And with DataIQ, it's really easy to connect to wherever your data lives, maybe SQL database or cloud, data warehouse, or object storage, without having to copy it or to write code. In our case, we stored our data in a snowflake database. Now let's start with looking at some of the flood insurance data. No matter how big the table is or where it lives, you can see here two million rows, we can still explore it easily in a spreadsheet like manner. And for initial exploration, we can do data profiling, even see here some data quality indicators that show us we have missing or invalid data. And for every column, we have quick analysis, which shows a summary stats and a distribution, and this really helps inform our data preparation. So the first thing the analyst did was they used a prepare recipe. And this is a real powerhouse. There's over a hundred different pre-built processors to do the most common data manipulation and transformation tasks. And again, we do have many tasks that are suggested to us by type. So we did do a lot of date parsing and extracting date parts. We took location data and transformed it into geo points, and did many other types of basic cleansing and transformations. Now another thing that we wanted to do was cross the information between the past floods and our potential locations for our new warehouse. And to do that, we used a geo join recipe, which is quite flexible. We could choose for our geo points or geometries how we want to combine these two different data sets. And we also can choose where we wanted to run. If we have a large job that we need to run, maybe from maximum performance and efficiency, we run it in database. And even though I'm creating this type of join using the visual interface, it's really easy for anyone to go and view the query that's automatically being generated behind the scenes. Now an expert might choose for this point to take this code and modify it programmatically from here. And in fact, a lot of coders tell us that this visual to code type of approach saves them a lot of time versus hand coding large transformations. Now in fact, along these say inlimes, if we go out to our flow and we zoom way out, we can see other places in the pipeline where coders did do transformations instead using SQL or Python recipes. And it's not really an either or decision at any point you can choose to use code to do something custom if that's your preference. And oftentimes an analyst will get stuck or they need something quick, complicated done. A coder will come in with a solution and then the analyst can keep on running without breaking stride like we see here. So coders can write their scripts in this editor or in the built-in Jupyter notebooks, or they might even choose to pick their favorite IDE like VS code or our studio. And this way they can use their favorite languages, their favorite tooling without ever having to leave the data IQ project. Now as the team went through this initial data prep and exploration phase, it's quite common to want to do visualizations on the fly to better understand the data and know what step to take next. So we have built in charting here that we've done as well as some deeper statistical analysis. And this started to help us pre-inform our feature selection for our machine learning model. So you can see here again, data IQ will help us find interesting relationships in our data and cool ways to visualize it in order to tell other people what our findings were. So for the most insightful types of charts or statistics, we published those two dashboards so that we could collect all the insights in one place for stakeholders later. And important to note that at this point, we could have also promoted this data set to our feature store so that the next time somebody's working on a flooding or kind of geography type project, they can use this data set from here without having to start from scratch and do all those transformations again. So in the next section, we're actually going to talk a little bit more about how we built that machine learning model. Now we're ready for our data scientists to build a machine learning model to predict the potential insureds payout that we might receive in the case of a flood, helping us ensure both the warehouse and its contents have the right level of coverage. So we build models in the lab and this is a sandbox environment that holds a bunch of different types of modeling tasks supervised and unsupervised as well as some specialized tasks like forecasting or computer vision. For our prediction, we could have chosen, you know, to follow the expert path and code our own model, but instead we chose one of the autoML templates to help expedite the process. So we can prioritize between algorithms for rapid prototyping or more interpretable algorithm types or those that are geared towards type performance. So let's go ahead and look at the analysis we built. Now the autoML made a lot of initial design suggestions, but of course the data scientists can go in and review and override anything, modify what they like. For example, it detected from our target that this was a regression task and it made a suggestion of metric to optimize for. We also turned on debugging and this helps us spot hidden pitfalls like overfitting or data leakage and raise a flag if it detects them. And we're also able to add model assertions, which are informed statements that we make so that if after we train the model, if it breaks an assumption that we think should be true, it's like the canary and the coal mine. We know some things wrong. We need to go check immediately. Now when it comes to features handling and selection, again, we can have full control over how to handle the feature and we give plenty of options here, depending on the type for how to rescale it, handle it, handle missing values, and so forth. Lots of best in class ML libraries and algorithms are here for you to turn on and off for each model experiment. Or you can of course, custom code your own Python model and add that as an option. So here, we're able to turn these on and off to see what performs the best. And finally, when it comes to running big models, most often we outsource those to kind of cloud compute. And here's where we might say, as a data scientist, well, I want to run this in a container. I want to be able to use my Google Cloud or my Amazon or Azure resources to distribute that search across maybe for Kubernetes containers. And I don't have to be a cloud architect to do it. It's as simple as this. Let's go look at the results. Each of these sessions represents a model tournament. And we can explore them one by one or at the end, maybe you want to look at them all from a high level in this handy table. We can dive into any model and dig further. And every single time we hit train all of these performance and explainability charts get populated for us. Now note as we go through this result section, these blue boxes you'll see here, these are reading tips and they're really useful for citizen data scientists, maybe IT operators to tell them how to interpret the chart and maybe what good looks like. Let's look at some results. For this model, we can see of course that coverage is the most important set of features for defining how much we might get paid out by an insurance claim. But we also see that location and seasonality play a large role. And we can dig into any feature further to understand its marginal impact or its influence. So for example, we can see here that at about $20,000 worth of total content insurance, we start to see a more positive impact on the payout. And this will be important for us later. For bias detection, we run subpopulation analysis to see whether the model performs equally well, you know, for different states or even different zip codes inside Florida. And we also can investigate row level predictions. Maybe we want to look at failed claims or unusually high payouts to understand what's driving those. So we're going to hold off on covering what if until later, where we look at, you know, what to expect if we make changes to the input. So we deploy this back to our flow to be part of our production pipeline. And at that point, we can score data, you know, that we've held out to see how the model performs on data it's never seen. Now finally, let's revisit the idea of participation by different roles. Up to this point, we've seen how our data architects or engineers can figure access to our data and our data analysts or citizen data scientists data prep built initial insights. And our data scientists did feature engineering and more advanced modeling tasks. In the next section, we'll talk about delivery and how to get insights into the hands of business users. In this segment, we'll discuss model delivery and model management. And actually, we're going to start on the dashboard, even though I could have shown what a analysis in the lab as where I was before, I've actually published it to our dashboard so that decision makers and business stakeholders can do simulations of their own. So we'll start here and be able to answer kind of interactively questions like, well, if I build higher than they're required two feet above the base flood of elevation, do I see an impact to the prediction of how much we might get paid out in the case of a claim? And I've actually run some simulations and I think I've found some interesting insights. What we discovered was that by increasing the amount of insurance coverage we have on the warehouse itself, it doesn't really change materially the predicted payout we might expect. But by increasing the amount of coverage on the contents, the inventory, we do see quite a large jump from 15,000 to almost 90,000. We could even optimize for the outcome that we want. Maybe that's the max value or some other specific dollar value. So we'll freeze any non actionable features and we'll let data I could prescribe the optimal mix for the remaining inputs to meet our specific goal. Let's say the business wanted a customized application. Well, it's really easy for the team to expose this model or lots of other types of elements actually as a restful API. So we can create API services with just a couple clicks here, add enrichments, maybe run test queries to develop our API. Now, instead for a batch approach, they might choose to kind of have a scheduled run. Let's say every quarter, we know that we want to update based on new weather events or based on a data set being modified like our insurance coverage, for example. So we can set up the trigger to rerun our pipeline and the steps might look something like this. Go pull the new data, check it for data quality or potentially run metrics to see if there's been material drift in our data or our model. If needed, we might spin up those resources to retrain our model and then create a new snapshot, a new version, push it to production and send a note to update the team. That would be kind of a classical batch scenario approach. Now, regardless of whether we use scheduled runs or an API approach, the employer is the one-stop shop where operators and teams can see all of their deployments in one place. So the employer connects to your dev, your QA or test or your prod infrastructure. And here's where they can really monitor. They can review call or event logs. They can do rollback to previous versions and all those important management tasks. Note that although many types of users appreciate the ability to manage deployments and do ML ops through a visual interface, as I'm showing, ML engineers can also leverage RAPIs to programmatically perform these same types of operations from external DevOps management systems. Let's talk a little bit more about ML ops. Each time our machine learning model is run against a new batch of data, data IQ automatically evaluates and stores the results. So operators have a visual gauge of how it's doing over time and can detect whether the model's degrading. If we see a downward trend in performance for a given model version, we can investigate different types of drift. For example, input data drift. Is it changing because the data out there in the world is changing and our model no longer fits it? So prediction diff performance drift, it helps us find all the root causes of our degrading model. And let's say we do retrain a model or refactor it. The next logical question is, well, is this one better than any version we've produced before? And for that, we would use model comparisons. And so we can side by side do a comparison not only of the performance metrics, but also how did we handle the features? What we're training details. And this way we can do our champion challenger type analysis quite quickly, not just for operators, but even data scientists wanting to compare candidate models to see which ones really the best. Finally, it's worth noting that for that highest level governance and oversight view, all these models and pipeline versions are also being captured in a central model registry as well as a bundle registry for the the projects. So as organizations scale from tens to hundreds of models, it's important for them to be able to govern individual projects or workflows and to be able to quantify business values and risks in a more standardized way. This way they can make better decisions and gain better control over the AI portfolio as a whole. And this is all built in again to that one single platform for data I could. To recap, we use descriptive analytics to deepen our understanding of how flood risks might impact where we put our warehouse and machine learning model to help us make informed decisions to mitigate potential costs and damages. Let's go to our dashboard now to look at some conclusions. Looking at the OECD data as well as the NFIP past flood claims, we decided to narrow down to about 10 potential locations in Southeast Florida. Now looking at the past claims, we can see that there's kind of a hot spot around Miami, a lot more claims. And so we wondered how we could look at all of this together in one view. So one of our data scientists with application development skills built us a web app, which we've embedded into our dashboard. This allows decision makers and business stakeholders to really explore the data. And again, we do see there have been more claims in that southern region. So what we'll do is we'll actually reduce our locations to just maybe the more northern ones. Let me pick a few here. And from here, we can zoom in, kind of see where those floods are and maybe pull in our driving radius. You know, this is kind of how far a truck can go from our warehouse in 30, 40 minutes. And now we can make a better recommendation about either our steward location or maybe even our hobby sound location since no floods have occurred anywhere kind of in that radius. Now, one thing to note, everything I've shown you today, so far from DataIQ is generally available and is all included with an enterprise license. One product for simplicity, remember? I do want to show you one thing before we end that's about to come out in our next version. I'm pretty excited about its impact on model explainability and trust as it pertains especially to our project. So we have three new, shapley based visualizations for feature importance. And if we dig into something like feature effects, we can see that yes, as we saw in what if increasing the contents coverage does increase our potential payout. And we do see that again, consistent with what we saw in what if increasing our building insurance doesn't actually have the same effect. So we can use the feature dependence plot with the total building insurance to understand what's the number we should recommend a leadership. And that inflection point happens at about $100,000 of coverage. So now we know where to recommend and how much coverage to recommend. Even better, these shapley based visualizations are available for all types of models. Tree based or not, and maybe not even those developed inside DataIQ. For example, you might have developed a model using MLflow and imported it and will still do these visualizations. In the future, we'll be able to show the same types of comparisons for cloud models, maybe developed in Amazon SageMaker or Google Vertex or Azure ML. So very exciting stuff. To conclude, let's summarize what we've covered today. DataIQ makes AI accessible to everyone, from creators to consumers. For this project, our team of five members representing different business functions and technical skill sets came together to pool their collective expertise. Since the platform's technology agnostic, we are able to connect to diverse types of data and infrastructures and use a combination of visual tools and code based on need and preference. And finally, we designed our model, deployed it and reported findings to business consumers all in one single shared environment with special attention to WhiteBox explainability and AI governance. Hope you enjoyed this presentation of DataIQ. Thanks so much for your attendance and your participation.