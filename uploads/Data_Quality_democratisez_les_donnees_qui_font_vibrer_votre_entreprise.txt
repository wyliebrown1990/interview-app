 shame et lui il n'entraîne que la ma nad, la immunisme néansידant pushing plus traite pitched, il faut lifted du rang. C'est un concept qu'on va revenir et revenir et l'objectif ici, c'est de vous montrer comment on le traitre chez Datayuku, quels sont les problèmes qu'on va chercher à Aesut. Et potentiellement, vous donnez une petite visibilité sur la roadmap à court terme qu'on va avoir là-dessus. Donc aujourd'hui, la partie apparente. Il y a ma photo, j'espère que je ressemble encore à ma photo, normalement elle est plutôt récente. Donc on va avoir trois aspects sur la qualité de la donnée. Les défis, quels sont les défis que vous rencontrez, qu'est-ce qu'on entend après des clients là encore. On ne fait qu'en assembler ce que vous nous dites, ce que disent les analyses, ce que disent le marché. On va voir comment est-ce que dans Datayuku, on peut déjà traiter un certain nombre de ces défis. Et ensuite, quelle est le next step qu'on voit et tout ce que vous pourrez faire au sein de Datayuku sur la qualité de donnée. Donc aujourd'hui, là, c'est une tendance marchée. La qualité de donnée est un problème sur un sondage qui a été fait. On voit clairement qu'il y a une grosse pression entre le datan ginière, le datan en liste et potentiellement le business user qui va consommer la qualité de la donnée. Et donc potentiellement, le chiffre qui est assez impressionnant, c'est que seulement 7% des problèmes de qualité de donnée sont résolus à priori. Ce qui veut dire que des problèmes de qualité de donnée qui sont vu trop tard et qui potentiellement agissent sur la confiance qu'on peut avoir dans la donnée. Donc, confiance dans le report et confiance dans la décision qu'on va prendre au strahvère là. Donc, c'est forcément plutôt problématique. Les grands thématiques qu'on peut voir, quels sont les problèmes qu'on identifie ou quels sont les gros points. Il y a définitivement la complexité de la donnée, ce qui rentre, comment est-ce qu'elle est intégrée. La vitesse d'intégration, c'est-à-dire qu'il y a des données qui arrivent potentiellement tous les jours toutes les semaines, comment est-ce qu'on arrive à les mettre dans un format de standardisation et dans un processus de standardisation des données. Il y a la responsabilité. Ce qui est important, c'est qu'on a dit qu'on a dit qu'on a dit qu'on a dit qu'on a dit qu'on a dit qu'on a dit qu'on a dit qu'on a dit qu'on a dit qu'on a dit qu'on a dit qu'on a dit qu'on a dit qu'on a dit qu'on a dit qu'on a dit qu'on a dit qu'on a dit qu'on a dit qu'on a dit qu'on a dit qu'on a dit qu'on a dit qu'on a dit qu'on a dit qu'on a dit qu'on a dit qu'on a dit qu'on a dit qu'on a dit qu'on a dit qu'on a dit qu'on a dit qu'on a dit qu'on a dit qu'on a dit qu'on a dit qu'on a dit qu'on a dit qu'on a dit qu'on a dit qu'on a dit que tout le monde a dit qu'on avait dit qu'on avait unhappy la paix qui a travaillé avec la música, comme ça, comme ça, comme ça, comme ça, comme ça, comme ça, comme ça, comme ça, comme ça, comme ça, comme ça. Contrapson tego le micro en 1 intermediate là-bas, qui a 10, qui a cooperé avec nos sprinteurs qui ont140 initiatives, qui ont des500 Panda et qui apparaitnt dans un Sammy, qui a parce qu'on a des kidевes et donc wifi qui a 805, j'ai agreement avec les consonعدations lupip erste d'ailleurs, parce que j'aiprompé au mondeisphere province, mais vo穊 allé Willie innocent. très utile mais qu'il n'est partagent pas. Donc c'était un peu, on peut revenir un peu sur le concept qui était présenté ce matin par Michelin, c'est dire j'ai plein de monde partout. Alors chacun fait ses petits trucs dans son coin, mais comment je profite du réseau pour essayer d'arriver quelque chose. Donc ça c'est vraiment la partie silo, c'est plus départementale que technique on va dire. Parce que technique aujourd'hui, normalement on va dire qu'il y a une certaine standardisation sur les architectures de donne. Voilà petite allusion à Guirou, parce qu'on parle d'argent à la fin, c'est toujours une histoire d'argent. Ce qui est intéressant c'est que Guirou il a vingt ans, il était au top. Mais donc il y avait déjà des problèmes de qualité de données. C'est pas quelque chose de nouveau, c'est pas quelque chose sur lequel on se dit qu'on a découvert, c'était déjà un problème. Et en fait le problème était, on va dire, connu, et on se dit potentiellement entre un problème qu'on n'a reglée pas à la prévention, il y avait un facteur de 1 à 100 qu'à la fin, si on le réglait à la fin, ça nous coûte plus cher. C'est certes, ça veut dire que le sujet n'est pas nouveau et qu'il a toujours été là. Orvanche, ça c'était il y a 20 ans. Aujourd'hui, c'est mil... Ah bon super, c'est multiplié par 10, normalement il y a un 1000, on va. Donc là vous voyez, il y a le 1000 de là, c'est qu'aujourd'hui, ça coûte beaucoup plus cher. Puisqu'il y a plus de données, plus que la plus d'utilisation, puisque elle est plus démocratisée, ça coûte beaucoup plus cher. Donc on en revient tout de suite, c'est-à-dire le problème de qualité donnée, reste un peu prééquivalent, enfin, il n'y a pas de nouveaux problèmes qui sont arrivés, c'est juste qu'il faut les traiter. Et ne pas les traiter, ça coûte plus cher. Et bien sûr, on n'a pas envie de dépenser de l'argent pour faire des choses qui sont inutiles. Et si on pouvait récupérer l'argent, j'ai d'être écout, ça nous arrangera aussi. Donc comment est-ce qu'elles sont les défis qu'on va avoir aujourd'hui, les trois principaux défis, c'est trop de données, et donc c'est ce que j'ai dit, c'est qu'il y a beaucoup de données qui arrivent. Beaucoup d'utilisateurs qui veulent consommer la donnée, même des gens d'un expert, qui veulent se dire, je veux faire des analytics parce que je veux être data driven, je veux que mon club de rugby régionale, ou voir même demain commune, puis s'utiliser des données soit data driven. Donc on a plein d'utilisateurs qui veulent utiliser des données, ça a une problématique. Le problème de data qualité, c'est un problème pas seulement pour reste dire que c'est le data engineer qui est responsable et qui l'a mal fait son boulot, mais en fait, c'est pas du tout ça. On va voir que c'est pas du tout un problème, un problème, un personnage de la chaîne de valeur, parce qu'au final, il y a un business user qui peut voir un problème de qualité de données qui sera jamais vu par data engineer, parce que pour la simple et bonne raison, qu'il ne voit pas de la même manière. Et le point, le dernier point, c'est quand on commence à se dire, «ouais, non, la table, j'ai des lignes vides et tout ça, le data engineer se dit à la tension, je vais juste checker le nombre de lignes vides dans ma table, peut-être que l'utilisateur d'après se dit à la norme, mais mon problème, c'est pas les lignes vides, mon problème, c'est les lignes qui sont vides où il n'y a pas zéro. Enfin, il faut avoir le même, on va dire, socle technique et le même environnement de discussion pour parler à d'en. Donc on est vraiment, on faut avoir une approche à la fois collaborative, parce que tout le monde a potentiellement son nom à dire sur la qualité de données et l'utilisateur finale. Et aussi, proactif, dans le sens où il faut essayer d'identifier au maximum les problèmes en amont. Et pour ça, il faut communiquer, et il faut communiquer bien en amont avec les bonnes personnes. Donc, on essaie de ce qu'on essaie de voir, c'est qu'aujourd'hui, on a le donné brut, donc c'est un peu l'image de gauche, c'est qu'on a plein de données. Elle est là, on est plein de tables partout, on a plusieurs data sources, c'est très bien. Ok, c'est parfait. Ce qu'on aimerait avoir, c'est potentiellement quelque chose qui est classifié, codé, documenté, accessible, et parce que ce n'est pas seulement une bibliothèque qu'on cherche, mais c'est qu'il y a des gens qui l'utilisent. C'est des gens qui l'utilisent pas, ça sert à rien. Donc, c'est vraiment cet optique qu'il ne faut pas oublier, c'est que sans l'utilisation, la donnée n'a pas forcément besoin d'être mis à jour ou à la fin, on peut juste la dépresser et pas l'arande visible. Donc, il y a vraiment l'histoire d'utilisation qui est aussi important à prendre en compte. Donc, comment est-ce qu'on va gagner ? Comment est-ce qu'on va faire aider ? On va aider les équipes d'atta, à gagner la conscience et d'organisation. Donc, comment justement les équipes qui sont en charge de la donnée, on peut pouvoir participer et être assuré que ce qu'il livre et à la valeur ? Donc, on va avoir trois étapes là-dessus. Il y a, il y a, il y a, il y a, la découverte. Donc, ça, c'est comment au travers de la taille, où on va faciliter la découverte de la donnée, comment on va fiableiser. Donc, fiableiser, ça va être vraiment cet objectif de le faire ensemble, et comment on va une fois que c'est fiableiser. Comment est-ce qu'on va s'assurer que ça reste fiable ? Et donc, c'est pas, parce qu'une fois que c'est fiableiser, la donnée mise à jour, on a besoin de s'assurer d'avoir des contrôles qui permettent de s'assurer que ce qui, la valeur qui est générée, reste toujours la même. Et à la fin, partagez. Donc, le partage, en revient, c'est quelque chose d'assez important pour justement ne pas refaire ce qu'a déjà été fait potentiellement par d'autres équipes d'atta, et pouvoir enrichir au fur et à mesure tout ça. Donc, sur la partie découverte, quels sont les questions qu'on voit, justement au milieu de cette espèce de bibliotex en nom dans lequel on ne se retrouve pas ? C'est comment détecter les anomalies, que sur les données que je viens de récupérer, quels sont les problèmes ? Qu'est-ce que je vois, est-ce qu'on peut m'aider à profiter les données pour savoir qui sont les problèmes ? Comment est-ce que je peux aller plus loin dans l'analyse ? C'est-à-dire, j'ai fait une première analyse, je vois que la table, elle contient des transactions, ou elle contient des résultats. Qu'est-ce que je peux faire de plus sur les années statistiques de cette table pour en savoir un peu plus, pas forcément en regardant ce qu'elle te donne ? Et comment est-ce qu'on peut... On va dire appliquer des composants stodards à cette donnée ? C'est-à-dire, il y a des choses qu'on a déjà été faites, par d'autres personnes, sûrement, qui sont disent qu'il y a une table transaction qu'il a même, qui était similaire, j'avais des problèmes similaires, donc est-ce que je peux réutiliser un composant plutôt générique pour appliquer les mêmes règles et pour essayer de fixer les mêmes choses ? Ça, c'est quelque chose... Pareil à la réutilisation, quelque chose d'un important, de parler mari à chaque fois du début. Donc l'actu sur ces trois plomblomélnatiques, forcément, on a des solutions. C'est non, on n'a pas des problématiques. Le premier, ce qu'on va chercher à faire au sein de la taïcou, c'est vraiment dans la découverte des données, c'est à partir de mon moment, elles sont inclus dans la taïcou, elles font une autre façon pour les faire du profiling. Et surtout, identifié de manière statistique et potentiellement les indices de datacotille, la ce qu'on peut voir sur le screenshot, c'est qu'on a une petite barre vertérouge. Alors, ça reste, on peut se dire, que c'est simple, mais c'est déjà une première indication qui permet de savoir à quel point, qu'on va dire le pourcentage de données qui ne sont pas forcément dans les tas, dans lequel on voudra qu'elles soient. Et ce qu'on va chercher à faire, c'est pas forcément faire des lignes vides ou pleines, mais aller dans ce qu'on appelle des minings, c'est potentiellement identifié à cette colonne, se dire cette colonne, ça doit être un numéro de téléphone. Et donc, si le numéro de téléphone, il est mal formaté, potentiellement ça ira dans le rouge. Allez un peu plus loin dans la découverte et on va dire le profiling pour identifier tout ce qui pourrait être nécessaire, la première fois qu'on va les donner. Ensuite, c'est ce que je disais, il a parti analysant profondeur, donc là c'est toute l'analyse statistique des tables et des données qu'on a auquel on peut avoir accès au sein de data écout, à partir de la colonne ou dans le RITAN panel. Donc, c'est comment est-ce qu'on peut avoir pas forcément une idée du contenu et de la distribution du contenu et comment est-ce qu'on peut être, on va dire plus à même, de comprendre la structuration interne de la donnée. Le dernier point et je pense que c'est un point qu'il ne faut pas oublier c'est qu'au final, chez data écout, notre objectif de la donnée, elle a potentiellement plusieurs fins et il y a une fin qui est faire du machine learning ou de la AI. Donc, dans les données qu'on va voir, potentiellement, il y a des data scientists qui vont venir voir la donnée. Donc, est-ce qu'ils peuvent aller récupérer des features au travers de ces données? Donc, des features qui vont être utilisées dans un modèle. Là encore, on croit énormément, en fait, que les données ont un cycle de ville, un cycle de ville, c'est que le divin ne s'arrête pas juste potentiellement la donnée et dix mais qu'elle pourrait être utilisée pour faire des modèles, pour faire de la prédiction. Donc, on incluse ça dès le début, en fait, dans la partie de la qualité de données, parce que la qualité de données va être vital à laquelle il est désigné au modèle derrière. Donc, ce qu'on cherche au travers des features, c'est comment est-ce qu'on peut générer des features automatiques à partir de ces données? Donc, on a une recette qui a été livrée à la, je crois, dans la dernière version, en douche, qui permet justement de générer des features de manière automatique pour être réutilisé dans les modèles? Donc, on est toujours dans la découverte de se dire, ok, j'ai des données, qu'est-ce que je vais pouvoir utiliser, qu'est-ce que je vais pouvoir réutiliser? Donc, ça, c'est la partie découverte. Après, une fois qu'on a découvert, on se rend compte qu'il y a des problèmes, donc il faut fiableiser. Fiableiser ou rendre ces données de confiance et comment est-ce qu'on va arriver à faire ça? Donc, ce qu'on entend, parce que là encore, s'abiliser, ça peut être notre définition chez DataEco, mais on essaie d'écouter les clients et d'écouter ce qui cherche à faire et qu'elles sont les problèmes qui cherchent à raison. Donc, une des questions, c'est cette histoire de le faire ensemble. De le faire ensemble, pas parce qu'ils se disent, parce qu'ils sont, parce que nos users sont plus romans altruistes et qu'ils disent, ils veulent régler un problème avec tout le monde. C'est juste qu'on fait la communication aujourd'hui sur les problèmes de data quality, généralement, pas se parler, Slack, Teams, Messages, en disant, «J'ai un problème, mon dashboard marche pas, c'est pas la bonne donnée. » Et donc derrière cette communication parfois agressive, ou on va dire pas forcément en ligne, enfin pas forcément agréable, il y a la nécessité de se dire, « OK, il y a un problème de data qualité, qu'est-ce que je vais faire et comment est-ce qu'on peut le régler ensemble ? » Donc, il y a vraiment ce concept de comment est-ce que je fais intervenir les besoins des business users en amont et comment je communique là-dessus ? Après, la partie de data préparation, alors la partie de data préparation, c'est un exercice qui est plutôt long, enfin, qui peut être long, en fonction de la qualité de la donnée. Donc comment on a simplifié, comment on la rend plus efficace ? Ça, on va voir que là-dessus, avec tout l'aspect LLM, génératif AI, il y a une opportunité assez incroyable qui s'ouvre à nous, de pouvoir simplifier ou accélérer, on va dire, la data préparation. Et après, il y a toute la partie, comment est-ce que les données que je vais fiable d'utiliser, comment je peux être sûr de les fiable d'utiliser pour qu'elle soit utilisée dans des modèles et donc qu'elle rend le modèle plus efficace. On en vient, on a toujours cette optique-là, même si mon focus est principalement de la tamalagement de chez la taïcou. On a quand même l'objectif de se dire la donnée peut être utilisée dans un modèle. Donc, faut pas publier ce cycle, qui est écompousse, qui est la prédiction et quelque chose qui est très facile à faire, donc pourquoi s'en empêcher ? Donc, là-dessus, là encore, on a un certain nombre de fonctionnels. Et après, ça, c'est pas nouveau, mais tout le concept du projet et du flow dans la taïcou, qui est l'environnement de collaboration des différents utilisateurs, qui permettent, en fait, de les tous les mettre sur un même canneva, et pour voir ce qui se passe sur la donnée, pour pouvoir préparer la donnée ensemble et pour pouvoir, à la fin, opérationniser ça. Donc, le flow va être vraiment là où les codeurs peuvent faire des recepes de code, si ils veulent faire des recepes de code, les non-codeurs peuvent faire des recepes visuels, mais à la fin, chacun peut faire sa recette sur sa partie du flow et combiner l'ensemble, l'ensemble pour avoir un résultat de haut de valeur. L'idée n'est pas forcément de se dire, on va séparer les codeurs et les non-codeurs parce qu'à la fin, on va venir en fouleur qu'ils rassemblent et qu'ils fassent un pipeline qui soit personnalisé. Donc, là, ce qu'on cherche chez DataEQ, c'est vraiment au travers de ce flow, c'est de mettre tous ces utilisateurs qui n'ont qu'un objectif, je dirais peut-être pas une envie, mais qui n'ont qu'un objectif, c'est d'avoir de la donnée de qualité pour générer de la valeur, de les faire collaborer ensemble, pour pouvoir opérationnaliser. Et l'opérationnalisation reste, on va dire, la valeur intrinsèque, c'est que si on n'oppérationnée, ce n'est pas, potentiellement, le flow, il sera obsolète, ou personne va l'utiliser. Et si c'est cet objectif, de se dire, la valeur va venir, en moment, ce sera opérationnalisé. Le deuxième point, c'est la partie de DataPréparation. Donc, c'était un peu ce que je disais, c'est qu'aujourd'hui, on a beaucoup de recettes visuelles, on a encore en tête, on a plein d'ilets, on est très créatifs, et on a aussi des recettes de code pour faire de la data préparation. Le petit plus, où vous allez sûrement du voir des vidéos, où vous pouvez aller sur le site, voir les vidéos, c'est ce qu'on appelle le AI-PRIPER, donc c'est l'aspect génératif et aé, qui aujourd'hui nous apporte, on va dire un accès, l'érateur à ces fois, dans la data préparation. Les recettes qui sont là, pour des utilisateurs, on va dire, non-techniques, mais qui connaissent les structures des tables, enfin, qui se connaissent déjà un peu, c'est très efficace. En manche avec l'aspect et AI-PRIPER, on va pouvoir commencer à interagir avec la donnée, ou avec ce qu'on veut faire avec la donnée, en langage naturel. Alors, là encore, on ne promet pas la magie de même et donner sécurisme et donnée, ou fiableisme et donnée, et tout va se passer tout seul. En revanche, le produit est enrichi, c'est proposant un certain nombre de fonctionnalités pour aider à préparer la donnée. Ça va permettre de découvrir d'une meilleure manière, donc, de se dire, est-ce que tu peux me prendre cette colonne, enfin, enlever les lignes vides, est-ce que tu peux me faire l'âge moyen des gens par rapport à leur date de naissance et de pouvoir, on va dire, interagir et générer les steps. Donc, là encore, l'idée, c'est pas forcément que tout se passe magiquement, mais c'est que générer les steps de préparation qui vont permettre de vérifier que ça correspond mieux, en résultat qu'on va voir, donc ça nécessite quand même une certaine connaissance technique, et donc de pouvoir aller beaucoup de rapidement dans la génération de ces steps. Et c'est là où on voit une espère, enfin, qui aura une amélioration, on va dire, de l'expérience, c'est que, au travers de cette espèce, on pourra générer plusieurs steps en même temps. Donc, facilité, donc, à l'est plus vite dessus. Et je vous encourage, pour le moment, c'est encore en privé de prévu, mais je vous encourage à l'utiliser, parce que plus on verra l'utilisation de cette histoire-là, plus on sera capable d'identifier quels sont les requêtes qui sont demandées à la donnée. Là, c'est encore un... Enfin, je peux le dire, mais au niveau de product management, c'est encore un grand inconnu de si on vous donne ça tout de suite, qu'est-ce que vous tapait dedans ? Est-ce que vous tapait des choses très précises en disant, prends-moi la colonne en dollar et convertit-la en euro, ou alors, c'est, je ne sais rien, identifie les problèmes de qualité et résoulez. Et en fait, en fonction de ce que vous allez demander, nous, on va forcément essayer d'adapter et comment on peut s'adapter à tout ça. Donc, pour nous, c'est une grosse accélération. C'est pas de la magie, je répète, il faut savoir ce qu'on fait, mais on y voit, on va dire, un axe d'amélioration et d'accès aux fonctionnalités de datayku de manière plus facile. Elle dernier point, j'en viens, toujours, il ne faut pas d'oblier, c'est, comment on peut faire du feature engineering sur les données. Donc, là, on passe vraiment dans le mode larbe et comment on peut tuer les features dont on va avoir besoin dans le modèle. Et tout ça, dans le même platform, tout ça, dans le même flow, avec la collaboration du data engineering, du data analyst locode, du data analyst codeur, du data synthesis qui fera un modèle. Et à la fin, générez, on va dire la valeur ultime du flow et qu'on pourrait documenter au sein de data product ou au concept de ce style. Donc, là, on a fissabilisé les données. Normalement, on peut se dire que on a pas mal travaillé, on a des flows plus ou moins complexes, on a un pipeline qu'on est capable de personnaliser. Une fois qu'on a fait ça, on va bien falloir qu'on s'assure que ce qu'on a fiableisé le reste et commencer à partager. Donc, pareil, là, encore, on n'a rien à inventer dans l'histoire. L'idée, c'est une fois qu'on sait comment est-ce qu'on va opérationniser la livraison de données de qualité. Donc, en continu, quand il y a des updates, quand il y a des mises à jour de table, quand il y a des nouvelles données qui rentrent en compte, qui rentrent dans le pipeline, comment est-ce qu'on s'assure de ça ? Donc, ça, c'est déjà en premier point. Parce que l'exercice a été fait une fois, on a peut-être pas envie de leur offert. Comment est-ce qu'on peut partager les données fiableisées ? Alors, il y a deux histoires. Là-dedans, il y a le concept de partage et il y a le concept de indiquer qu'elles sont fiableisées. On va dire deux choses certes qui sont dans un même phrase, mais le partage, c'est facile, remplir un data catalogue de centaines de tables. En disant, mais vous inquiétez pas, il y a le schéma qui est juste là, c'est super facile. Malheureusement, ce n'est pas très utilisable pour quelqu'un qui ne connaît pas les tables. Donc, la partie, c'est comment est-ce qu'on communique, qu'elles sont fiableisées, comment est-ce qu'on communique, comment est-ce qu'on peut être utilisée ? Et ça, ça va être un axe, différents seteurs. D'où se dire, je ne cherche pas les données par rapport au nom de la table ou au colonne qui sont dedans, mais je cherche les données par rapport. Potentiellement l'usage en lequel elles sont utilisées, potentiellement le pipeline, potentiellement le projet. Donc, avoir un peu de contexte là-dessus. Et donc, justement, il y a aussi le côté de j'ai fait un super boulot sur mes données. J'ai un data set que j'ai utilisé, puisque là encore, faire un data set pour le plaisir de faire un data set, ça n'a pas forcément de sens. Mais que j'ai utilisé pour mes analyses sur Janserien, moi, sur les ventes du dernier quartet, je suis prêt à le partager et sur le presque qui soit réutilisé, voir enrichi par d'autres personnes. Donc, comment est-ce que je m'assure que d'une certaine manière, les forcogémies et l'opération des açons que j'ai pu mettre derrière, ce soit réutilisé ? Donc, ça, c'est trois problématiques, bien sûr, parce qu'on a trois problématiques, on a trois réponses. Aujourd'hui, on a ce qu'on appelle dans un data-écout, ce qu'on appelle les Matrix and checks qui sont au niveau du data set. La bonne nouvelle que j'ai, c'est que d'ici quelques mois, on va, comment dire, les améliorer, même plus que les améliorer, mais on va passer au concept de data quality rolls. C'est-à-dire qu'on va vouloir vraiment avancer sur ce sujet de qualité de données, donc avoir des règles de qualité de données, qu'on puisse appliquer aux data sets, toujours au niveau de data sets, et qu'on puisse monitorer tout ça au niveau du data set du projet et potentiellement de l'instance. Donc, on va aller à un niveau supplémentaire, qui est de se dire, les Matrix and checks, aujourd'hui, sont très techniques dans le sens où on peut aller inspecter la colonne, on peut même faire du con, on peut faire plein de choses. On va garder cette flexibilité là, mais la même plutôt dans un concept de des cumonitoring ou de des curules, donc avoir des règles potentiellement réutilisables et partagables, et surtout avoir du monitoring dessus. Donc, ce rende compte que, alors pas surtout les data sets, puisque vous pourrez en avoir des milliers, mais sur les data sets les plus importants, comment est-ce que vous allez savoir ou garder la qualité de ces data sets. Et si y a un problème, remonte un warning, remonte une heure, remonte un alerte. Et ça, ça va être vital dans le partage, parce que, à partir du moment où on partage, on a envie d'être sûrs que ce qui nous a été partagé, reste de bonne qualité et comment est-ce qu'on le sait, faut qu'on ait une certaine manière, une information dessus. Donc, c'est là la deuxième étape, c'est la partie partage. Sur la partie partage, aujourd'hui, on a plusieurs systèmes en fonction de l'approche, on a le système de Future Store ou un système de data catalogues. Les deux sont d'une certaine manière, 6 000 intègres de fonctionnalité technique, mais l'approche est différente, donc, sans sous aujourd'hui, dans un data catalogues, je cherche un data set de bonne qualité. Comment est-ce qu'il a été généré, ça m'intéresse, mais c'est peut-être pas forcément l'anément vital. Dans un Future Store, je vais avoir besoin des features qui ont été généries et j'ai aussi besoin de pouvoir reproduire la manière dont ils ont été généries. Au final, ce sera le projet, mais on va dire que l'approche est un petit peu différente, c'est pour ça qu'on a deux approches là-dessus. Donc, ce qu'on veut faire au travers du catalogues, c'est autoriser un steward, à publier ces données au travers d'un data catalogues, dans data e-coup. Alors, je pense que certains ont en tête déjà ici me, mais j'ai déjà un data catalogues, j'ai pas besoin de ça. Je suis entièrement d'accord avec vous. Notre objectif ici, n'est pas de construire le data catalogues de votre système d'information, parce qu'il est bien plus grand que les data sets qui a dans le data e-coup. En revanche, notre objectif, c'est des utilisateurs qui sont dans le data e-coup, et qui potentiellement ont besoin de réussiser des choses. Ils passent plus rapidement par le data catalogues, ils changent de repas d'interface, ils se partagent ça de manière plus facile. Donc, c'est ce qu'on cherche à viser. Il y a des possibilités faites par un certain nombre de clients avec les entreprises de ta catalogues du marché et qui est très efficace. Mais là, juste pour préciser notre objectif, ici, c'est les utilisateurs qui collaborent au travers de data e-coup, comment est-ce qu'on peut les aider à découvrir ce que font les autres et les réutiliser. Elle dernière point, c'est la partie partage, ou la réutilisation des choses qu'on a fait. C'est comment est-ce qu'on peut réutiliser d'un projet un autre, ce qu'on a déjà été fait. Donc, on a l'un encore, tout un système de partage au travers de data e-coup, avec des requêtes où on peut demander le partage d'un data sètre dans le projet et avoir cette visibilité là. Donc, l'état pultime serait de se dire, je vais dans le data catalogues. Je vois les résultats de data quality du data sètre, le data sètre m'intéresse, je vois les décourules qui sont dessus. Je me dis, il est pas mal. En plus, il a un niveau de data quality qui m'intéresse. Et je demande son accès. Je l'importe dans mon projet. L'honneur, le Stuart, c'est que je l'importais dans mon projet. Donc, le jour où il veut casser une colonne ou autre, il est râme à vertir, c'est les suffisamment sympathiques. Et moi, je peux continuer à surveiller la qualité de données que j'ai nérées. Et donc, pour mon pipeline qui suit après. Donc, c'est vraiment ce qu'on cherche à faire et aujourd'hui, le challenge, enfin. Là où on voit de la valeur, c'est dans cette collaboration. Et potentiellement, pourquoi pas, puisque dans le data catalogues, la qualité de données, là, que l'utilisateur qui va peut-être le consommer, se disait que tu pourrais rajouter quelque chose ou alors rajouter lui-même des data quality rules, mais reprendre la qualité de données au fur et à mesure du pipeline. Là encore, je sais que sur le marché, il y a des outils data observabilités qui sont généralement complémentaires aux outils data catalogues qui sont très bien, mais qui sont positionnés un tout petit peu différemment dans le sens où eux, ils font un snapshot, ils font un clock et ils regardent ce qui se passe. Et ils voit aussi en est. La manière dont on positionne la qualité de données chez DataEcos, c'est on veut pouvoir agir si il y a un problème. Donc c'est complémentaire de ces outils-là, mais ce qu'on cherche à faire ici, c'est de se dire, ah, il y a un problème d'attaque, j'ai le fixé. Où est-ce que je le fixais dans le pipeline? Donc être le plus rapide à pouvoir fixer tout ça. Donc pareil, un peu comme les data catalogues, c'est pas quelque chose qu'on est avec lequel on est compétiteur, mais plutôt complémentaire. Et vous l'avez vu et je les plutôt répétais, on cherche à améliorer l'expérience utilisateur, des users qui sont dans nos dataEcos. On attaque peut-être tous les autres après, mais ce sera une autre histoire. Mais on cherche vraiment à améliorer ça et à rendre la livrée de la valeur le plus possible à ces utilisateurs. En résumé, normalement, j'ai été assez clair, j'avais pété plusieurs fois les mots. En résumé, est-ce qu'on considère vraiment, est-ce qu'on cherche à faire? C'est la qualité de données en équipe, donc la collaboration, l'opération des aînisations. Et après, il y a toujours les trois aspects. C'est découvrir les données, la découverte de la donnée, la fiableisation de la donnée et le contrôler le partage. Je vais pas revenir sur ce qu'il y a dessus, mais aujourd'hui, à plateforme, offre beaucoup de choses, comme je le disais dans la roadmap, ici quelques mois, en cuin de l'année prochaine, on aura un nouvel étape sur la partie data quality rules et data quality monitoring, qui, je pense, contribuera à cette effort sur la data qualité et sur tout ce que vous pouvez faire, comme valeur, autour de data eco. Et j'arrive à la fin et j'ai 5 minutes d'avance qui vous permet de prendre une pause plus longue. Si vous avez des questions, je suis...