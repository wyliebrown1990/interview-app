 Hi everyone, my name is Christopher Peter McGriss. I'm the lead data scientist here at DataIQ. And I have the absolute pleasure to be joined by these wonderful gentlemen here, Alexi Metal and Robert. Together our names for CRAM. We're going to try to cram in as much as possible in the next 20 minutes to cover this absolutely vast topic on data model and quality. So Robert Metal, Alexi, I'd love for you to take a moment to introduce yourself. So the audience knows a little bit about your background. So Robert Stevenson and I work for Ben Weimellen in the investment management side. And do you want me to keep going with the intro? Sure, yeah. So I get to run the investment management implementation of DataIQ. And have done that since 2018. So it might be a little biased today. But we've had a lot of fun over the years and that's what my main focus is. And I just want to make sure everybody has the right tools, the other job, and look for opportunities for cross-pollination and collaboration where possible. And we'll dig into that a little bit deeper. Cool, thank you very much. My name is Metal Parach. I work at Convex Insurance, which is a specialty insurance and re-insurance company based in London in case you couldn't tell for my accent. So I've been an actor for 20 years. And Convex is quite a new company from 2019. One of our key visions is to improve data-driven decision-making. So looking forward to talking to you a little bit about my role as an actor and also the journey that Convex is going on. Thank you. Hi, my name is Alexi Sirkoff. I'm a partner of Deloitte. I've been with this my 25th year with the firm. I wear a couple of hats at Deloitte. I run our model risk management practice that deals with matters of model risk. I also, over the past couple of years, have been leading our Trustworthy AI services, which is an area of practice specifically dealing with the risks, controls, and governance around artificial intelligence machine learning. Deloitte is an alliance partner with, you know, a state IQ. We collaborate quite a bit, and so I'm happy to be here. Wonderful, wonderful. So I think through our discussion today, I think we want to tell a full data story. A beginning and middle and end, because I think that data and model quality pervade throughout the entire data analysis pipeline, and something that is very important. But to set the stage here, I think that there is a little bit of subjectivity to what quality means to each of us. So I'd like to throw it to metal and learn a little bit more about how you unpack the deficit of quality. There's a little bit of a paradox here, because I think we all know it when we see it, but everybody might define it slightly differently. So it's your definition. Well, I think there's not a single definition for data quality, but I think it's important to think about the product of good data quality, which is confidence in your models and the ability to make better decisions and better outcomes. And within that data quality is made up of not having many errors in there, no material data gaps, consistency in the way that it's being filled out, it's being used, making sure it's well defined so that people know what it is and know what they're using and how to use it, and also joined up and easy to access. And I think all of that means that we spend less time manipulating and cleaning the data and more time for it to be useful. And so, Alexi, I know that you really have a big focus on the model quality as well. So how does the definition expand or change a little bit when talking about the model itself? Absolutely. Well, so models consume data. And we talk about data being the new oil, but most of us don't consume oil directly. So we use gas, we use energy, we use plastics. And so models are probably one of the more advanced ways in which we consume data. And it is appropriate to talk about model quality in this context and the risks that are in models. We need to make sure that the models get their right inputs, which is the data, that the processing that goes on inside the black box is doing what it's supposed to do, that the outputs flowing downstream are interpreted correctly. There is a broader concept of quality of AI and ML models, some of the more advanced models that we have out there. And there we touch upon subjects such as bias and fairness, explainability. Can we look inside the black box and tell what it's supposed to do? And then we look at the black box and the black box is enabling privacy. Is the model using the data that it's entitled to? Rebusiness. Is the model robust to either inadvertent or maliciously supplied incorrect data? And then also security and cyber risk elements that is the model appropriately protected from a security perspective. So there's an entire spectrum of things that all go into the definition of model quality in this broader sense. Yeah. So Robert, I know we just heard a little bit about robustness and consistency and all those types of things. I know you work very collaboratively among very different teams among your organization. So do you find that there are any consistencies or inconsistencies among the way in which the different teams sort of highlight these different elements of quality, both for the data and the models? It's definitely important to, I mean, we've spent the whole day looking at all the cool new features of DSS-11 and all of this advanced technology that's available. And if the data itself doesn't support that, then we can't really go forward, can't let you. So I feel that there may be a lot of opportunities to do some manual intervention. And so that means you likely can't automate a process. So every team may define data quality a little bit differently. It could be timeliness. Like a report needs to go out and they're trying to look for opportunities to get that out faster. Or there may be just a manual patch that they do. With investment management, we all have a different, there may be different definitions of the word revenue. That's a common issue. And so if you're looking for that value somewhere, you need to know what calculation might go into it upstream because that might not actually be what supports whatever work you're trying to perform. And so I think it kind of depends on the definition of the work that a team is trying to do. And yeah, I think it's a moving target as well. And ideally, there's a lot of transparency around the process. So I understand what goes into the upstream data. And you create a transparent process around the consumption of that in the pipeline and so forth. Yeah, actually I want to dig a little bit deeper into that moving target sort of concept. I know earlier Metal Uni, we're talking about the process of data cleaning itself. And there's so much subjectivity that goes into that. If we gave the same data set, messy data set to 100 different data scientists, they all might do something slightly different, even though they do have that common goal in mind. And it's not necessarily something that is bad because I think we do want to enable data scientists to bring their creativity to the table. But we also want to have those boundaries for the guardry else to make sure that they're actually focusing on getting the job done, the rational thing they need to do. So I wonder from your perspective, what's your thought on increasing a team's confidence that we're focusing on getting that job done, while not sort of constraining them, especially when talking about the subjectivity that comes to the process of data cleaning? Yeah, definitely. So I think as an actually, I'm at the frontline of data quality because we're having to project future claims payments far out into the future. And using data models to do that, we have to make recommendations based on millions of dollars. And so if there are data errors that can cost the company quite a lot of money, traditionally, the way that we would do it is really checking data consistency between quarter and quarter, so we would have to present results at every quarter and be able to explain the differences. A lot of that would involve data investigation, understanding, however, that takes time. So what we're doing with data IQ is we're building out an automated process so we can look at it much more regularly and try and get these data issues fixed as quickly as possible. And that increases people's confidence. But also at Convex, we're trying to enable database decision making. And so that involves processes, culture systems. And so we're getting a message from the top that data is really important. And so we're having to be quite deliberate about increasing that confidence in data if we're going to try and increase this decision making. And so there are many elements of that. One of them is a message from CEO saying, well, it's everybody's responsibility, data quality. So users, producers, managers, all need to be responsible for that. That's a cultural thing, but also there's a systems thing where we're trying to automate data quality checks. And what we're trying to do is measure data quality and visualize it. So if there's a data error, those tickets go out to people to fix those. But also we visualize that error rate so that you can look at it over time and you can compare different teams. When different teams can see their own data quality on a dashboard, they don't want to be bottom. They want to try and do better. So that in itself drives a culture. But also we're building it out of data universe to try and create that consistency. We also got a data culture team, which is helping to catalog our data so that people are using it in the right way. They know what they're using. And to your point about being creative, as users of data, sometimes we can compensate for some of the issues that we see in data. And we don't necessarily communicate that around the business. And this idea of everyone is responsible means you've got to collaborate with people. So even though you're making adjustments yourself, what we're trying to do is we really explicit about those adjustments we're making and sharing them around the business so that we can get that consistency of adjustments. So you haven't got 100 different answers from data. Everybody knows what's going on. You're trying to fix it at source. So I think that's been really important in what we're doing. Yeah. I know Robert, you have some thoughts on collaboration. I know one element here that we want to find a balance between is having the individual being able to do their own job. Whether it's the data engineers who are loading that data, the data scientists who are running experiments and building models on that data, the data analysts who are deriving new features, or even our data managers who are ingesting those insights and trying to actually take action. And I think it's quite beautiful that a persona can have their individual lane, the string that they're pulling on to get their job done. But collectively put together woven together, they actually create a much more beautiful tapestry. So Robert, with all of this collaboration going on, so many cooks in the kitchen, it's beautiful to think about when things go right, but realistically speaking, something may go wrong. So when the pipeline inevitably breaks in our everyday AI processes, who's really accountable for fixing those issues, maintaining that we still want to work together, but who's fault is it, how do we sort of solve the problem, how do we work together to again reach that common goal? I think it depends on the age of the process in a way. So if it happened to be developed in a modern platform, then there is that transparency and the team worked on it. Hopefully a diverse team that you have data scientists and data engineers working together. And so if something breaks, ideally you have that transparency in the first place and anyone can hop in and fix it. And in my head I'm picturing the flow, hopefully some of the rest of the people in the room are doing the same thing. Being able to see what broke and look at the reason why and either fix it there or escalate it and be able to show the problem in context instead of just be like, okay, it was late or we're missing data or data completeness issue or something like that. If you can show the problem in context, then ideally the producers and the consumers come together and can work to solve that problem permanently. And what you don't want is a one-off fix. And if somebody is doing something manual and this is kind of like maybe a more legacy process where somebody just does something in Excel and they've always done that way. When they leave the company or something changes, then it's terminal. And if you have that process around it, then it continues to scale, obviously. But then it's more easily supported. Yeah, so I feel like at this point we've kind of discussed like the beginning of the data cleaning, the middle, the data pipeline, the process. And Alexia, I'd love for you to talk a little bit more about the ending, you know, to tell our complete story here. So from your perspective, I know you're really in experience in the model risk management space. And your team really has a big focus on what happens next. What happens downstream once the model actually takes that data in and what do we do with it? There's more beyond the finish line. It feels like. It seems like your team has really shifted the reference frame between the moonshot and mundane. The mundane has become now the model building because the moonshot is what we do with it afterwards. So I'd love for you to tell us a little bit more about how we can focus on that impact, what we do with the results of a model. And also a little bit more about the groundbreaking Trustworthy AI initiative that's going on at Deloitte. Thank you. So, yeah, the topic of model risk management does not a new one. It's been around for a long time. It's also another regulatory driven topic in and of itself. So if I were a CEO of a company and I was told that these numbers come out of a black box and I'm about to tell investors about it, tell the world about it, rely on those numbers. I would want somebody to take a look at that black box. I would want somebody who's other than the guys who build it. Make sure that they didn't forget to divide by two. That everything is good. That they picked the right data. That they fed it into the right place. That they did the right processing. That they picked up the right outputs. So, there is this inner need of managing model risk that exists no matter of what the regulators might tell us. Now, it so happens that in some parts of the universe such as in banking, the regulators are very prescriptive. They've issued very prescriptive guidance called the Sauer 117 that says, that will shout manage model risk. In other areas though, insurance, investment management, even outside of financial services, the same concepts apply. It's good practice. It's something that everybody should be doing all along. Now, fast forward to today, when we talk about models, we're not just talking about statistical models or more simple things that we studied in school, but new AI machine learning types of applications. And not just used for risk or pricing or even business forecasting, but AI and machine learning applications that are all around us, like chatbots and decision making tools, recommender engines and so on that are all around us at every institution. So how do we manage risk there? And there, this concept of trustworthy AI comes in where one has to really build a comprehensive holistic governance structure around these models as well as we sometimes call AI machine learning objects, because sometimes folks get stuck on the definition of, okay, is a chatbot a model? Well, maybe or maybe not, but that doesn't matter. The chatbot has logic and you want to make sure that it does the right things, that it doesn't go rogue on you, it doesn't start spooing obscenities, it doesn't start saying things that shouldn't be. And so how do you manage that risk, right? You have to look at it from the beginning, make sure that you understand how it's actually being built and trained. The data that goes into the model is key, because no matter how good the model is, if the data itself is, for example, biased, if it doesn't, if it excludes a certain constituency, there is nothing that you can do at the model side to not have that bias carry in, right? So you have to look at the way the model itself is constructed and along the dimensions that I mentioned earlier, robustness, explainability, privacy, fairness, and so on. You have to look at the model, not at the big, not at just at a point in time, but put in a monitoring process in place, because it's not enough to say, oh, I checked it on day one, it was fine. Well, how do you know that it still works on day two? How do you know that it doesn't go off track, especially with AI machine learning models that are teaching themselves, that are evolving over time? You know, that has to be a monitoring process in place to make sure that the model continues to do what's right. Privacy is an important element, and again, the data and models are going hand in hand, because AI and ML models are notoriously data-hungry, what we say, and they will look and either with the help of the researcher and sometimes by themselves, find every piece of data they can find to get trained upon. What if they don't have right to the data? What if there are privacy issues, what are there consent issues that need to be considered before that data can be consumed? So a comprehensive governance structure with specific technology enabled controls is an answer to a lot of these issues and risks, and something that we see a lot of companies are putting in place today at the same time as they're putting in place their data governance programs. Yeah, it seems like this is quite ubiquitous, like whether it's, you know, the consumption of these models and the results that we end up distributing, or whether it's the culture of our data team to top those charts everybody's involved, or all the different teams, the different teams across an organization for collaboration. It's something that matters ubiquitously across the teams. So I know in our last minute here, I'd love for us, we talked about the beginning, middle and end of the data analysis pipeline, but we have a little bit of an epilogue, and I'd love for each of you to just say what's the one big thing that you'd like the audience to take away from our discussion today about data or model quality? Talk to each other, talk to your teammates, and be transparent about the process, just collaboration in general, but I think the more visible a problem is, or, you know, project or whatever it is you might be working on, the more obviously the more attention you get, but the better results you get, building diverse teams, I think that's critical, and different perspectives, and some of our most successful projects that we've seen have been produced by teams that have a wide range of participants on, and that's what I would say, just talk to each other, try to go as far upstream as possible if there's a data issue, and no more band aids. Yeah, and eat your avocado toast. Yes. Yeah, I would say really be deliberate. Data quality is not going to improve by magic, and so you need to think about the types of initiatives you want to put into the organization to try to improve data quality, and make sure that everyone is having a say, and everyone is responsible for making sure that we're using good quality data, making the right decisions, and being responsible. So, yeah, definitely. I guess my message is, you know, to build governance and controls in from the start. You know, the rest of this conference is about building the best SkyNet possible. You know, stronger, faster, more potent, you know, more advanced with the best data. This panel here, if you don't, if you forget everything else about it, this panel here is the only one that tries to make sure that the SkyNet doesn't take over the world. And you have to do that by putting these controls and governance in from the beginning until it's too late. Well, just top it all off. My one big thing. It's a little bit of an inside joke with the gentleman here today, but one thing I'd like to say is thank you to all of you. You are all absolutely brilliant, if you know what I'm talking about. And thank you all for joining us for this discussion on data and model quality.