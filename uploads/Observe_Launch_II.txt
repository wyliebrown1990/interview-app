 Welcome. Deeply innovative metrics. That's DIM for short. The metrics you never expected to see until at least a few years ago. The first time I've ever seen a new one, I've never seen a new one. I've never seen a new one. I've never seen a new one. Welcome. Deeply innovative metrics. That's DIM for short. The metrics you never expected to see until at least 60 years in the future. If you're a crack-adjac technologist, then you rely on metrics on a daily basis. Any modern metrics platform needs these key components. My capable technical assistant, Young Jason here, will help me illustrate these for you on our giant. Easy to comprehend, dashboard. The DIM platform includes all the key requirements you crave. Must have for every route you've been to. The DIM platform includes all the key requirements you crave. Must have for every rockstar 10X full stack engineer. First, you'll need a dedicated, WonkaBite scale in memory database. You'll need real-time processing because the main company's too silly. So make sure you have wonkaBites of memory on hand. Remember, it's VC money you're spending so cost should never be an object. To analyze your metrics, we don't use a modern, popular language. There are no where near, expensive enough. So we invented a simpler language just for you. DIMQL. How about that? How to break the correct end so simple? A fifth grader can use it. Well, that's all the time we have for now. But tune into our next show when we'll talk about dashboards. Complex tagging systems. And alerts more and more DIM. Deeply innovative metrics. Hello. My name is Annette Mulaney and I'm a software engineer. I know I don't look it, but that's because they paid me to shower. I'm just kidding. We're all very cool and hygienic. I'm here today to talk to you about observability. What are my qualifications? Am I a 10X rockstar ninja engineer? God no. No. Absolutely not. Not even a little. Honestly, I'm more of a 1X. 1.5 on a good day. 0.7 if I'm a little hungover. Work hard. Play hard. Point is, I'm a remarkably average and deeply adequate engineer. Like most employees. But that's exactly who needs to troubleshoot apps. Normal humans. Ideally. The first stack of various products and in-house things is usually very concentrated. I'm talking relying on one person to solve the tricky issues. Which is terrifying. The power can really go to their head. I've already promised a firstborn to get help before. And that's just not a scalable solution. Plus, what if that person leaves or dies? I mean, I'm sure I'll be fine. I'm sure our stack is completely and thoroughly documented and easily grasped by whoever needs to fill in. What is observability? Other than a buzzword I throw around in meetings and attempt to sound smart? Open telemetry. High cardinality. Cloud native. Tail sampling. Are you even going to put those into sentences? No. Observability is ultimately about answering simple questions. What's happening and why? Or, as we might phrase it during an incident call? What the hell is happening? It's 2 a.m. Why? In my experience, I usually try to answer these questions by desperately parsing too many logs. Because I never actually got good at the proprietary query language of our service. And the logs themselves are deeply unhelpful. The tags return everything. I'm like, who set this up? Oh. I didn't. Huh. I should make that better. What if I added more tags, more logs? I probably had metrics and tracing to those logs. And you know what? More tags to those metrics. Okay, now I definitely need more dashboards and more alerts. But are there enough tags? And not to brag. But all this is costing the company a lot to ingest. Even though I don't even need most of this info anymore. But like, what am I going to do? Remove it? I mean, who does that? Oh. Excuse me. Don't worry. It's just another alert. I'm going to get them all the time. There'll be more of something's really going wrong. Plus, I'd have to wait for everyone to hop on the call before we can make any progress. I have time. Okay. I know it's 2AM. Let's get this over what swing go back to bed. Okay, what's the issue? Bob has joined the meeting. Hi, Bob. Welcome. Mary has joined the meeting. Thanks for joining us, Mary. Bob, you're muted. Bob. Bob, you're muted. Bob, no one can hear you. Plus, everyone has their own tools, and so everyone has their own data. Since our tools don't speak to each other, we have to. And if there's one thing I love, it's stressed out troubleshooting. Although actually, it has been one of my more reliable forms of socializing during some of the bleaker parts of the last year. Oh, restart the note. Can't believe it and think of that. This is the third time this is happening this week and that. Crazy, right? This didn't need to be a call. Sure, Bob. Wasn't it kind of nice, but it was? Okay. Well, just try restarting it next time before hitting me up. Totally. Don't you get a dog? Bye. Bye. I also tried to fill the void by bettering myself. A year ago, when I stopped commuting and my climbing gym closed, I resolved to dive deep into Kubernetes and finally understand it instead of silently praying to myself. And within a few weeks, I had completely given up on that resolution. But others have fared better. Observe, for example, has some pretty exciting updates to share. Thanks for watching the Observe launch. Now, this is our second launch, and we're going to focus a little bit less on the company today, and more on new product features and some of our amazing customers. If you're new to Observe, the first thing that you should know is that we're taking a very different approach. Recent research by the 451 group shows that organizations use seven tools to troubleshoot and monitor their applications. They've got many tools because they have fragmented data. We believe that observability is fundamentally a data problem. If we can solve that, the tools problem will take care of itself. Now, you'll often hear vendors talk about observability as three pillars, logs, metrics, and traces. When we look at the customer experience in using these tools, we find that the DevOps team is spending all of their time doing ops, tasks like tagging and archiving data. There appears to be not much dev in DevOps these days. Too much effort is going below what I call the value line, and the SRE and engineering teams don't see enough value. There's got to be a different approach. With Observe, we flip the iceberg. We use a commercial database snowflake as our underlying data store, so a lot of those operational tasks just fundamentally disappear. Our engineering team focuses their effort like above the value line. This enables more users, including roles such as customer success, to do what they really want to do, which is to understand and analyze their application. Now, what's our approach to delivering all of this value? The most fundamental step is putting all of the telemetry data in one place. After all, it's 2021, and cloud storage is practically free. If your data isn't in one place, it becomes really hard to relate the piece parts, which is critical to providing context, which is critical to observability. Now, the magic in the system is that the raw data, it's messy, and generated, it's gobbledygook, it's not understandable by humans. Conventional wisdom is to provide users with a search bar and have them go looking for breadcrumbs. We think that that's a terrible starting point, and so we transform the machine data into something called the Observe Data Universe. Event data is curated into things called resources that users understand. These are things like customers and shopping carts and pods and containers. No other product has this abstraction layer, and it allows us to do something pretty amazing. Most importantly, Observe establishes relationships between these resources. This allows users to quickly locate additional contextual information for the problem that they're looking at. The user can navigate the graph using our grappling feature without knowing the exact path between the nodes. Let's now talk about some of the new features in Observe. When we looked at the state of the art in systems that analyze metrics, we found that to become fusing, often presenting hundreds or thousands of metrics and tags to the user. If you knew the metric that you wanted to look at, you were all set, but if you didn't, you were never going to find it. Users get lost in their own tag soup. We wanted to change the game, Observe curates metrics, so that users only see the metrics that are relevant to the part of the system that they're looking at. In this example, a view of the customer was seeing average response times, error counts, and the number of tickets that they've raised. If we want to add something more to the dashboard, we just drag and drop it from a curated list. And because the user only sees relevant metrics, they're not overwhelmed. Let's turn our attention now to alerts. Alerts aren't new in concept, but it's amazing how unwieldy they are to deal with, even today. Too many inboxes are filled with too many alerts that contain too little information. As systems become more complex, something has to change. Like metrics, Observe's implementation is a game changer. Our alerts feature relevant contextual information, so the user knows exactly where to start their investigation. Now we can do this because of the data universe, the graph of connected data sets that exist behind the scenes. In this example, we're alerting on errors in application logs, but we also know exactly which customers are affected. Why? Because the customer's resource is linked to the application logs. Now, speaking of customers, we've made great strides with our early customers since last October, and pleased to announce that we have over 20 paying customers that use Observe on a daily basis. They're all part of our founding customer program and are helping us define the product roadmap. I want to say a big word of thanks to all of those customers for their trust in Observe at this early stage. And finally, a word about our pricing model, which I believe again is another game changer. Our pricing is usage-based, so customers only pay us when they're using the system. Even better, we itemize bills right down to individual data sets. Imagine if you got an electrical bill, and it itemized the top 10 appliances that were consuming electricity in your house. That would be a beautiful thing. That's exactly what we do. We believe in usage-based pricing, but we also believe in being fully transparent with where the money is going. Thanks so much for your time today. I really appreciate you tuning into the update. After this break, we'll be back to hear from the Observe founders on what they were thinking when they were implementing many of these new features. Welcome back to DIM, or deeply innovative metrics, the 1961 edition. In part one, we covered walk-a-byte scale databases and algebraicly correct query languages. In this show, we'll cover many more must-haves for every Rockstar 10X full-stack engineer. First up today, let's cover dashboards. That is a UI or user interface. Remember, you can never have enough dashboards. If you see one you like, you just copy it and make it your own. Contrary to popular belief, dashboards are state-of-the-art, incredibly useful, and never ever break. Oops. Next up, tagging, lots and lots of tagging. Pack, Jason, you're it. Jason, do you know what a DIM tag is? And last but not least, we need to cover alerts. And there's one now. Let's walk over to our alert operator, Ms. Phyllis Stein, to find out what's going on. Phyllis, what is that particular alert for? Is it an application down? Phyllis? Is the CPU being plotted by server instead of by process? Is that pizza here yet? Oh. Looks like Phyllis has a bad case of alert fatigue. Sorry about that. We'll address that in our next show. And speaking of our next show, tune in next week when we'll take an in-depth look at logs. We'll show you how to determine the true age of any log by examining timestamps in our cross-section. But that's for next time, so don't miss it. In most metrics products, you're dropped into a sea of often cryptically named metrics, with little knowledge of how to get the right metric for the situation you're in, or even how to get that metric drawn correctly. All right, so typical situation. There's a problem with X. What metrics can I find that might tell me about X? You're scrolling through pages and pages of dashboards trying to find something as relevant. You don't find it now browsing through thousands and thousands of raw metrics of how you're looking for. The metric naming is all over the place. Taxory consistent, it's a mess. The way observed stores and processes metrics data takes advantage of the unique capabilities of snowflake, and is radically different from existing systems. In observe, the way you find metrics is dramatically different. When you look at a resource, we can find and surface the right metrics about that thing without any need to search. So we're trying to flip this on its head. So instead of paging through all those metrics and dashboards, it just goes straight to our resource page, and we'll show you any metrics that are relevant. We'll even auto-generate dashboards based on those metrics. This metrics experience is almost disturbingly simple compared to similar products where you weigh through thousands of pointless tags and metric names. Our perspective, I guess largely, is that you don't even need those tags anyway. If you can just correlate data sets together, if you can follow these transitive relationships in the data, you can answer these super nuanced questions. One thing I'm particularly excited about is our solution to the cardinality problem. Cardinality is the number of unique combinations of metric names, such as CPU utilization, memory utilization, et cetera, and tags, such as the application name, Kubernetes pod name, and so forth. Existing systems require users to carefully plan and watch the cardinality of their metrics data to find the right balance between cardinality, cost, and performance. Observe does a way with all of that. In observed cardinality is largely irrelevant for cost and performance. Applications may emit as many metrics and unique tags as they please, with no need for users to play with tuning knobs such as indexes. Metrics data can be retained and queried potentially forever at full fidelity and with high performance. So when we were thinking about what to do with our learning feature, we wanted to make sure that we did more than just like fire and forget a learning. So at first it was some servers on fire, and then it was these servers on fire. And now we need to know what services run on those servers, and what customers are using those services. Alerts about things or resources completely changes not only the experience of configuring alerts and monitors, but also how to make use of them when they finally trigger. The alert you want to see as a business operator is a customer is not having a good time. Oh no! Because we do both time and relations, we can deliver that. We put a lot of effort into making sure there's somewhere meaningful to go when you click on our alert generated by observe. So instead of looking at random log entries or metric chart or whatever, you quickly see a list of like which users do this impact. How often has it happened in the past? What other issues might these users be experiencing? If you have a roster of things you care about, and those things relate to each other, you would be fooling out to use observe. You should use observe. Now that all sounds pretty sick, but what does it look like in practice? Let's have Belgium, a better engineer than I, walk us through how freaking simple it can be when all your traces, logs, and now metrics are in one place, mapped on to concrete concepts that a human can navigate. But don't worry, even if you are that rockstar engineer that can stare into an abyss of logs and see the matrix, there's still a view with those raw logs that you can use to scare the interns. Hi, my name is Belgium, and I'm one of the engineers here at Observe. Today we're going to take a walk through the Observe solution and focus on some recent updates to the platform, particularly with respect to alerts and metrics. To begin, I'm actually not going to dive right into Observe. Instead, let's take a look at this Slack channel where I get alerts from Observe. Recently, an alert fired, telling me that some of our customers are facing errors. One of the key properties of alerts in Observe is that they leverage GraphLink to provide context around the notification. And in this case, even though we don't have a lot of errors, we seem to have a subset of customers who are experiencing a high error rate. And this could lead to some poor customer satisfaction. So let's dive in. Now I even observe. Here, I get an overview of that notification. I can see that this is still an active incident. And for us, like any other SaaS company, when issues occur, a key question is who was impacted. This page helps me answer this question without diving further, so that I can evaluate the impact of the problem. For example, it looks like a few more customers have been added to the impacted list since we received the alert. Let's pause and think about what the root cause may be. This customer impacting problem could easily be pods stuck in pending states, maybe no memory usage over 90%, perhaps repeated fail logins through database, or really numerous other root causes. And in all of these cases, we want to get specific answers out of our investigation to understand the impact of the problem. By answering questions like which databases affected for which application in which AWS region? To do so, let's go take a look at the data that fired this alert. Now I can see the log lines that cause my alert to fire. I immediately noticed I have a lot of out of memory errors, which is a little alarming. If I scroll across, I can see more information. Like here, I have the stack trace for that error. But right now, I'm still not really sure if this is a code issue or resource contention at the infrastructure level. I need to do a little more digging to figure that out. Because these datasets are linked together and observed in a relationship graph, I can jump to the Kubernetes pods that generated these log lines. Here, I see the pods where those error messages came from. These pods are currently active. And using the time scrubber at the top, I can travel back to an earlier time when these pods came alive first, which seems to be shortly before my alert fired. Now, what I'm really curious to look at is the metrics for these pods. If I scroll down, I automatically get in context metrics for these pods. And notice that I didn't have to hunt around following tags, carrying around names or IDs for my logs platform to my metric platform. Because I observe is a one-stop shop for my logs, metrics, and other technical or business data, I can pull together metrics for my resources with ease. So because I was getting memory errors, it's really the memory metrics that I'm interested in. Scrolling down, I see a chart with my CPU usage metrics, and I can open it to see a more detailed view. And, aha, I see the signature so-to shape that is indicative of a memory leak. This is starting to look more like a code issue. The final piece of validation to check is whether these pods are restarting. To do that, I'm going to look at the notifications for these pods. So, observe alerts serve a larger function than just not finding you, be a slack or pager duty. They can also be used in context of an investigation. Here I see notifications about pods restarting frequently, which is the last piece of the puzzle. So, I quickly got to a good spot in my investigation. I know which deployment is failing. I can open a ticket for the relevant team with this link, so they can see the problem in context and fix it. But, I can take this one more step further. In my environment, I'm using continuous integration and deployment. My CI-CD data also comes into observe. By using Graphlink, I can ask Observe to find build events from Jenkins to help me figure out the exact change that introduces code issue. Let's click navigate to here and select Jenkins builds. Before we take a look at the build data, let's see how we got here. At the top of the screen, we have the breadcrumbs. We started out by looking at the error logs, then jumped to the problematic pods, and then Observe seamlessly took me from pods to the relevant Jenkins builds. There are actually several hops in our relationship graph we had to go through. But as a user, I didn't need to know what path to take. Observe knew that pods are composed of containers that are running images that are built by Jenkins. Okay, let's go back to our Jenkins build data. Here, I see the exact change that calls the issue. Apparently, Tom made a small change to cache code, which calls a memory leak. I can now tell Tom that his recent change is impacting our customers and needs to be fixed. Well, there's more more things. As you remember, we started this investigation with an alert telling us that our users were experiencing errors. Well, I wonder if some of them actually open support tickets related to this. Using Observe, I can actually find the Zendesk tickets that are potentially related to this bad code change. Again, leverage and graph link, I can jump to my customer tickets without worrying about the underlying path. Here, Observe is showing me all the support tickets opened by users who interacted with that problematic build. I can let a support team know that we identified root cause of the issue and we can communicate their customers that the issue will be resolved shortly. Damn, girl, that was both impressive and given the ability to navigate so quickly to an offending line of code, quite terrifying. Makes for some great, blamful problem-solving. Now, if you're like me, this information isn't quite enough to be convincing because there's not a human demonstrating social proof. Well, you're in luck. Let's hear from some actual customers who are using the product. We've got two customer videos. The first is from LineData. There are financial services company that is rapidly moving everything to AWS and has security and compliance challenges. So this is like key for them because they need their customers to trust them. My name is Andre Butsar. I am the director of Cloud and DevOps at LineData. I went to college for computer engineering. I actually started my career in help desk, moved on to system engineering. I play a lot with Raspberry Pi, Arduino, and automating things around the house. LineData is a FinTech company. We provide front-middle and back-office products for hedge funds, wealth managers, and banks. One of my responsibilities is to manage the Cloud Platform team, which is responsible for building as-code frameworks to be reused by the various business units and engineering teams, as well as develop DevSecOps and GitHub's processes to improve developer velocity and quality of life. A lot of organizations, they might have monitoring, but monitoring is only a piece of observability. Our ability is what brings all of the different pieces together, tracing metrics, monitoring, logging. Security is always top of mind for me. We have dozens and dozens of AWS accounts. But with Observe, we can create linkages not only across resources but across accounts, which makes it very easy for our engineers to isolate issues, especially when you have dependencies on different services from different accounts. So we can troubleshoot an issue in our multi-account AWS architecture in Observe without ever having to log into an actual AWS account. It's not just security of the application, but it's also the security of the infrastructure, it's the compliance, the auditing. When we developed our GitHub's process, we had SOC 2 and Mon. We're able to link CloudTrail to the pull request information in Observe so that every single CloudTrail event has links back to a pull request. So in the Observe UI, I can look at a CloudTrail event and I will see who created the pull request, what the comment is exactly what's happening, who opened it, who merged it, who closed it, what had happened. Observe stood out to me for the simple fact that you can link disparate data sets together. You no longer have to copy and paste and remember what it is that you're looking for because one data set can lead you to the next and the next like a trail of breadcrumbs until you find your way to the issue. The impact on line data has been tremendous, providing our engineering team all of the same resources that our SREs have in order to manage their engineering environments. That has allowed us to develop more secure applications and release better quality code into production. And then we've got TopGolf. They have over 50 locations in the US and the last thing they want when people are drinking beer and playing Angry Birds. Yes, with a golf ball is a problem that they have to wait for their IT guys to sort out. My name's Ethan Lilly and the engineering manager over here at TopGolf. I've been at TopGolf for two and a half years now. When I was growing up, the stars was the 90s. They were just making it big here in Dallas and they ended up winning the cup in 99 and that's when I really got into hockey. I used to just play out in the street with some of my friends. It's one of the few sports I'm actually decent at. I mean, I always grew up around computers. I only remember the first computer we had. So I've been heavy into technology my whole life thanks to my dad that we're going to rate things on and be into it himself. Our main mission is creating moments that matter. People do things like gender real parties or they do proposals at TopGolf. We need a matter of big life event. We also have top-tracer technology which traces the flight of the ball and we can use that to do all kinds of things. This year we put out an Angry Birds game. You can actually play Angry Birds with a golf ball which is pretty crazy. Things that I've been working on since I joined is moving things to Kubernetes, more modern Docker orchestration engine. When I first started programming I was on. We were maybe pushing updates to production six months to a year. That was pretty much our cadence and whenever we did it it was very painful and then over time I've seen ships do quicker and quicker really cycles. Smaller and smaller chunks getting pushed out to here at TopGolf were now on a weekly cadence. We push updates every single week. The four we're using mostly elastic search for centralizing a lot of our logging information. We're limited to kind of what Kavanaugh would allow us to do which they have some capabilities with dashboards and visualizations but we mainly use it just to dig through logging. Kavanaugh uses a different search syntax called Lucene which is very annoying to use. It's like regular expressions but it's not which is really annoying for people. I would never really advertise it to anybody. Observe helps us to monitor the game system and the integrations with our POS system for handling the checks. We send a lot of data and we send over 1.25 billion events a day and all of that data is anything from a ball going into a target or a pendant light changing for a day or a new reservation coming in. What we've really gained with Observe is the ability to link our data from different data sources and ways that we never could before. Being able to link the data better between just our microservices alone and including different things like infrastructure with service now, with all these different platforms that we use that we wouldn't have even thought of linking before. We've also been exploring different ways to get more data into the platform before where previously we were trying to figure out ways to stop sending data because it was costing so much money. It's saving us time, it's allowing us to more quickly resolve escalation which is better for our guests. Nobody wants to be sitting around waiting for some IT guide to fix your problem. Before it was like a cat, we had this cat that had its own mind and wanted to do what it wanted to do and no matter what I wanted it to do, it didn't matter. But now it's like having a dog because your buddy will help you out when you ask him and he's not too complicated. I love simple and dog-like. That's why I date men. Well, that's been the Observe product update. I hope you enjoyed it moderately more than the average tech product show. We'd love to hear from you, so thank you so much for tuning in. You know, at the end of the day, we're all just humans, standing before a monitor, asking it to please give us something meaningful. I'm not going to be a cat. I'm not going to be a cat. I'm not going to be a cat. I'm not going to be a cat. I'm not going to be a cat. I'm not going to be a cat. I'm not going to be a cat. I'm not going to be a cat. I'm not going to be a cat. You