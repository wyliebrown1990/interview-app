 So let's go around, introduce yourselves. And could you also let us know a little bit about your team, the skill sets and their focus? And it would be great if you could also, you know, describe it in the context of the maturity. Renee, you want to go first? Yeah, hi. I'm Renee Canfield. I work at Experian. And for the last seven years, I've been working on a lead generation product, now called Marketplace. And we brought data, Iku, and a little over a year ago. But we've been working with them for a couple of years to kind of prove out of use case. And so we have a team full of data engineers. What would you might call a business analyst, the terms of change so much over the last five years. And a few data scientists. I'm not even sure I'll qualify myself as a data scientist. So we have people that build models. We do a lot of tableau work. And we have a lot of engineers that put all our data in the snowflake, which we connect to data Iku with. Great. James? Sure. So I'm James. So I'm a from the Commodore Science Background. So my PhD is focused on mushroom learning or computer vision. I've been working in data science for years about 15 years across different multiple industries. So I'm leading the data science team in the IT or in parallel networks. We've been working with data Iku, I think, since 2017, even before I joined parallel networks. And we have fully operationalized data Iku as the fundamental data science infrastructure for the whole data science team. So we build up team. I'm leading the team with the same team members, including Princeton, Post-Signars, and Engineers. And we try to build our team as a product team. So we develop an entire solution of the AI and ML to support different business organization functions in company. So including the finance, the customer supporting, or the product management. So yeah, I think for our team, I think we already passed the establishment stage and in the journey of expanding. Julia? Yeah. I am Julia Koshalivako. And I lead our Salesforce Marketing Data Science function. We focus on various problems that Salesforce marketing team is trying to solve. So about the half of my team is focused on the problems of marketing effectiveness measurement. So we build models like multi-touch attribution, marketing mix models, causal inference, trying to disout the true impact that marketing has on the business. And about another half, good half of my team is focused on developing recommender type of systems, propensity models, natural language processing models to help us personalize customer experience. So it's basically models that are built at an individual level. So I'm very, very proud of my team. It's a very strong team that has not only technical background in statistics, machine learning techniques, machine learning engineering, computer vision, optimization problems. But it's also the team that has tremendous experience in the main expertise in marketing, B2B and B2C marketing. Great. We'll get into our first question. So balancing efficiency cost, governance, and innovation are critical factors when implementing AI. Given market fluctuations and evolving technology advancements, although your primary metrics may not change, can you share a unique perspective on the approach your organization has adopted to maintain this balance of efficiency cost and governance? James, do you want to go first? Yeah, sure. So I think there's a great question, because it's not easy to balance the ODs, multiple factors in product initiation, AI, ML solution implementation. But in my two sense, I can share with everyone here is, I think this is a some purpose that we are trying to conduct in our daily work. I think from the two perspectives for data science project initiation and AI, ML model implementation, two perspectives. For project, when we kick off some of the new projects, we like to include the business to hold to end users as early as possible, so that we can fully understand what the business pain points and what the targets or expectation from the business. That will help us to learn the dummy experts and also to learn the business problems and so that we can transfer to technical problems. There will still be a lot of efforts or the cost from data science team. So, and also if we can include the business to hold an user into the project execution, I think that will be easy for us to educate customer or maybe the end user or business to hold to understand how the data science works, how the ML model works, and also we can get the feedback immediately when we see so that we can adjust or do some changes for our technical role map. And move to the project execution perspective, I should say based on the mature data science infrastructure, for example, data IQ, we actually practice the agile development of any data science model development so that we can provide enterprise level into an solution. So, as I mentioned before, we include the business to hold in the loop. Once we got some deliveries and we can share with business to hold us and to get the feedback. Then based on the feedback, we can adjust ourselves. So, we can make sure that we are targeting and we are on the road track to solve the business problems and we can deliver on time and to maybe do it to the adjustment as many as possible. So, on the milestone deliveries or some of the planning risk of. So, this is something that we can share. I think that's super important bringing the business into the process. Julia, you want to go next? Yeah, absolutely. So, for us, the balancing act that we're talking about really depends on the problem and who's the end user of the model. In my mind, I separate the users into two major categories. They're humans, they're gonna use your models and there is systems that gonna use your models. So, how you set up and how you balance that development process was, process would really depend on that. So, in my team that focuses on marketing effectiveness, the end user is humans, the marketers, they need to make optimized decisions. If I have a budget, how do I invest it in which marketing tactics should I invest to optimize that investment? If in the middle of the year, I got an incremental dollar, where do I put it? Where do I get the biggest bug for my, for my, the biggest bang for my buck, right? As they say, so in this type of problems, where we have to spend, it takes longer. We spend more time with stakeholders understanding their problems, understanding, sort of their use cases and how we can generate business value for them. We might have to use simpler techniques because sometimes marketers that I don't understand the black box, you know, explain to me how you arrive to this number. So sometimes, you know, the most complex techniques might not be the best friend, right? So this is where innovation also, not necessarily can be your best friend, even though, you know, sometimes it does make a sense to try. On the other hand, my other team focuses on developing models that are consumed by systems. Guess what? Systems don't care about the black box, they don't ask you a lot of questions. All we care about is the model fit, model performance, accuracy, are we able to show in a AB test that the model, let's say the process that embeds AI decision making is doing better than, you know, business as usual. That's all that the business stakeholders care about. You know, as long as you kind of get them on board with the goals and objectives, you're good to go. You don't need as much engagement from them. So again, this is where speed to market matters. You can innovate a little bit more, you can run quick AB tests, you can evolve and iterate on your model to make it better if you're not happy with the results. So really kind of in my mind, it depends on who's consuming your model. I love that distinction. The humans, right, in the systems. Very different how you explain who you bring into that process is going to be different. Renee, and thank you for coming all the way from the East Coast and the Netherlands. I think for us, I think it's such an important thing to really think about who's going to ingest that information and what are they going to do with it. So we spend a ton of time not just like showing what a model can do and showing the performance charts, but we have to explain that too and say, yes, one metric could go down, but that doesn't mean they're not working. And you kind of have to expect to drop here, but that's because we're strengthening the model. That's been a real challenge. Coming from a risk background, moving into marketing, marketing moves are really, really fast. And they like to throw things into a product, right? They don't always give us the ability to do something like an AB test or whatever, because they're not analytical driven people that are driving everything by data. And so there's a lot of education around why don't we just throw in a model and 100% of the population? Well, we want to test it. And that's the only way we can make sure it's working is if you throw two populations in at the exact same time, so they're kind of operating at the exact same time. And they have the same conditions. We don't unfortunately get to do that all the time. So there's a lot of education that goes into the back end of that. And what we do is we try to err on the side of being conservative, not taking too much off the top with a model, but just kind of saying, hey, we take 10% and look at the impact of this, right? We take a little bit more and we watch that volume shift from one lender to another, because we're kind of closing down where we don't think people can get approved. There's just a ton of education. We do a lot of that. So we just have to not just sit there and crank through data, which we love to do as analysts, but we have to really tell a story. And it's difficult. It's difficult to get people to adopt it, because everyone does have an bias. They think, oh, well, a consumer acts like this. It's like, no, you act like that. So you just think every consumer acts like you do. But when we're dealing with a more subprime population, and you can pay your bills and you don't hold your balances on your credit cards, you don't really operate like somebody who has a subprime living paycheck to paycheck. And so just showing that through data kind of gives them a view and to, hey, this doesn't operate the way you think it does. And you need to let data tell you the story. We work together to put those two ideas together. We don't do everything by just building model, because it's cool. But we also want to make money and drive business. But we have to work together. There's a lot of education. That impact analysis is very important to understand that. So next question. According to Deloitte's most recent state of the AI enterprise, AI adopters tend to buy more than they build. Can you share your experience in your organization, whether choosing to buy or to build and discuss the outcomes and benefits of the decisions? Do you want to go first, Renee? Sure. If you guys love hearing me talk. Yes, so I think, OK, so experience is a very large organization. And it can be very what they call siloed, right? So a lot of people are like, hey, I'm using this tool. Hey, I'm using this tool. I'm thinking about writing this. I think where some of the organizations fall short is that they realize that, yeah, you're like you can go build something, but who's going to maintain it? And what if that one person who wrote the majority of the code leaves? Do you have anybody that's going to support that at that point? Who's going to, there's no strength to that tool. There's no longevity to it. That person leaves and the tool leaves with them. And then what do you have? How are you going to deploy these models? If you pull on a tool externally, you've got a whole company that's supporting that tool. It's making it stable. It's supporting you on the phone, right? And you have a problem. People on a call, on a web call. I've got great people at DataIQ making YouTube videos for me to show me how to click around on the screen and do something that would have taken me hours to figure out. That's the part that's really, really valuable about it. And you know that somebody's going to support it, right? I think internally it's a little tough, because unless it's fully adopted and it's been around for a long time and proven, I think a lot of times you see people leave and it goes with them. So we went the route of buying. And it's hard to get them to adopt like, to that idea. Like, yes, it is worth the money, because you're getting a full staff supporting it too, right? You can't just, you know, people like to think that data scientists can do everything, not right? Your data engineer, your program, your statistician, your everything, not right? It's really, really difficult to find. So me that can do all of that, right? And support the background of a tool. So we bought, yeah, pretty happy. Great. We love to hear that. James? Yeah, sure. So I think from my personal point of view, I don't think which either buy or build, which one will be the best strategy, because I believe we should find a way with the best strategy to feed us. So what's our requirements? So for example, maybe if we came back to the data science to mature curve, which stage we are inside? If we are in the early stage to grow a data science team, or we already have the mature infrastructure or the architect to support data science departments or deployment to support the end-of-life cycle solution, right? If we in the early stage definitely the buying will be the better strategy, because we can save a lot of the efforts, a lot of the supporting cost, and we can definitely improve the efficiency or the data science AI implementation cost, right? But if we already move to the mature stage, like the company, we can get full support from the engineering team, or maybe from DevOps, or infrastructure team, like Google, Amazon, definitely sells for us, right? So okay, we can build our own two in-house tools, and but it depends. So, and also, when we either buy or build, we need to make sure, so what taxes that we are using cause the different functions, because we want to build up, if we want to build up the enterprise solutions, we need to think about how to enable our model and integrate our model into business pipeline. If we isolated from the business infrastructure, I think that would be the issue for us to show the AI value through the model in the end of the month. So, let's see all our considerations, one, we choose what we would be the best strategy for us, or which platform we should choose. And definitely, eventually, we choose the data I could, because I think the best strategy, or the best platform for us, and I think it's a moment, and so that we can coordinate all the different resources, all the business functions, and to show the AI, ML, and the data science value, so that it different cases. That's great. And depending where you are, right, on your journey, what projects you have going on, those are all factors that organizations have to consider. Julia, I know when we talked, and we're prepping for this, you had an interesting perspective on this. Yeah, I just wanted to share slightly different perspective, because obviously, we bought Data IQ, that's why I'm here. So, for similar reasons, it's already been covered. So, I want to talk a little bit about bi-versus build of the actual models. Do you go? Let me give you an example. Marketing mix modeling, medium mix modeling, multitouch attribution modeling. There are so many well-respected agencies out there that can build those models. They have a lot of expertise. They have access to the data that you might not have. They have benchmarks, industry benchmarks. They have, again, experts they've been doing this for many, many years in different industries. So, there's a lot of benefits in going and actually buying an algorithm, right? And somebody else is developing that algorithm for you, obviously, using your data, possibly augmenting with the data that they have access to and you don't. So, that's a big consideration. So, do you build or do you buy? So, of course, there is obvious cost. Do you have the expertise in house, you know, speed to market? Those are the obvious. But what I want to share is, don't forget about the strategic perspective on this. So, for example, for Salesforce, I even jot them down. So, developing and keeping domain expertise in house. Again, you know, once the company develops your model, they leave, you know, you have to maintain it and you actually haven't developed, you know, the depths of understanding your data, your marketing, your business model, right? Your marketing model. So, that's a big consideration. Another one, in the process, can we leverage Salesforce tools? As you guys know, I think a lot of you probably use Salesforce. We have very powerful, some of the state of the art tools like Data Cloud that helps us to bring data from multiple sources, you know, unified, merge it, you know, synchronize it, clean it for you. So, is that if we buy versus build with allow us to use our own products? Data security and governance, do you get to share the data? This data leaving sort of your firewall? Are you bringing consultants on using, you know, your laptops in your space? That's an important consideration for us. And finally, but also very important is technical debt, right? How much system modifications do you need to do just to incorporate what this third party builds for you, right? How much throw away work? How much rework? And thinking into the future, you know one day somebody's gonna come from the leadership and say, why are we paying for this? Why can we do this internally? How do you unwind? What is the cost of unwinding like all this integration that you have done? So those are the important consideration for, you know, do you buy a build, an AI or machine learning algorithm? The technical debt is a great, great point to bring up. I think that's something that not all organizations can say. So get about it. Yeah. Because it's so far in it, you know, you have to think so far in advance and we forget about it until, you know, that executive is, why are we paying for this? Let's build this in house. Okay, here we go. Great points, thank you. I love this question. This is something that I talk with my team all the time about is how do you measure and report the value created by your AI initiatives? You know, and are you past the stage where you're actually having to prove out internally the value of AI? So tell us a little bit about measuring and reporting out that value. James, you want to go first? Oh, sure. I think because I mentioned that we build up our data design team as a product team and we deliver N2N, AML solution to support different business functions. I think the metrics for us to measure any projects or any use cases, if it's successful or not, is the business impact. So we want to make sure as an NA model, we develop and deploy it, can be enabled, will be enabled in business function pipeline and to contribute the business to the stage and to turn the data into actionable insights, to help the business to make it final decision. So this is, I think this is, we will be, I think it definitely depends on the company strategy. So the second criteria, I will say is the metrics, I will say is about team growth and even also the individual team member growth because, personally, I believe the human resources is critical for any engineer team, including the data science team or software development team. When you want the team or each individual team members can consistently use their strengths to grow themselves, to advance their career or to build out their skill sets through the projects or the model they've been building. So how to help them to get familiar with the state of the arts, techniques, the text tags. So in the data work, I think there will be another things we need to take into account in any projects or any use case at the model in implementation. So definitely, I'd like to see more feedback because this is also important to me and to try to maybe do some more strategy refresh. So yeah, what's your feedback from? Yeah, I think for us, the leaders of the, the lead generation product, they always ask about revenue. Right, everything is how much revenue will this make us? How fast can we get some revenue from this? And you have to really kind of reorient their mindset on that is that that's not really the end goal. The end goal is not like day to day, we're making X percent more. It's about making everything that we're doing better and more efficient, right? And whether that is people by allowing me to bring in a team that a lot of people had never built models before and now all of a sudden, I've got five people who, if I go on vacation, can step in and build a model and they never did it before, that's huge. Right, because you're not wasting all that time until I come back from vacation to build that model and understand what it's doing, be able to implement it. That's really, really big and it's often overlooked. And then the other part is just kind of like reorient and their minds around what metrics are we looking at, right? The better that these offers perform and that the consumers are happy and they're getting access to credit, which is so important when they're struggling and they need money and to pay their bills and everything. That's so, so important and it's so valuable and we can't just think about dollars and what's really important is that we also, sometimes they forget that like the product does so well and because of the data that we have access to, that we have lenders who actually come in and they develop products just specifically for our marketplace. Well, that doesn't happen everywhere, right? Not everyone's, well, I won't say the competitor, but not everyone's going to one of our competitors saying, hey, we're gonna develop this credit card for you because we know you can target exactly who we want. And that speaks a lot too because that might have been just the subset of the population that never came in the doors, never clicked on a credit card, but now it exists there because of the modeling that we did and the way we drove the right people to the right offer. So it's really, it's just more education about what are we doing? Why is it important and why not everything is very easy to measure with a dollar and a sense attached to it? It's really, really difficult though. And like you said, building the skill set too, because it just makes the team, like it just makes everyone stronger, if they can explain everything and you don't have to rely on one person and it's really important. Julia? Yeah, so again, you probably were picking up on the pattern. I have two different sort of teams, marketing effectiveness and customer modeling. So how we measure the value of both teams is actually a little bit different. Remember, I have one team, the main consumer is marketers and another team, main consumer system, right? So of course, measurement of the value is a little bit easier in the system side, right? We can build models relatively quickly. We get enough volume of customers and contacts visiting on our website, so we can actually do quick AB tests. We can measure our key KPIs. And of course, there is immediate KPIs like click through rates and if it's an email unsubscribe rate, open rates. And then of course, we're trying to tie it to sort of all the way down the funnel to revenue whenever it's possible. So from that perspective, it's relatively easy, I would say, to show the value of the models we build. It gets a little bit more complicated on marketing effectiveness side. Again, the value of those models we build is a strategic decision making. So I hate metrics like, well, how many subscribers do you have in your dashboard, the users? We all know that those numbers are no easy. Don't really tell you anything. Half of those users could be your own testers, who is UI developers, right? UAT type of folks. So that's where it gets tricky, right? So I think this is where the value of the team becomes more qualitative. How many in-depth discussions do we have with marketing? How much sort of, you know, how much do we, are we able to influence in terms of, again, the budget spend? How, you know, well-hour, let's say ROI, ROI scenarios, optimization of ROI tools are helping the marketers to make decisions. The support from the leadership, support from the stakeholders that we get. The depth of the questions we get. And then, of course, the only quantitative sort of, a key quantitative measure I can think of is the investment into the team. So when the marketers feel that there is a lot of value, we definitely see influx of funding, headcount. So that's a big measure of success for us. Yes, value, dollar and cents, but also on the people side as well. So I know Jed brought this up. We couldn't go very long without talking about gender of AI, but so when evaluating emerging tech like chat, GBT, or gender of AI, what factors do you consider? How do these technologies influence your AI project roadmaps? You wanna go first, Julia? Sure. Well, definitely the fit for our use cases, the fit for this company strategy, business strategy. As you probably know, once, I think a couple of weeks after chat, GBT kind of took us all by the storm, Salesforce announced Einstein GBT and the partnership was open AI. So now we have actually the feature that folks can interact with CRM, via chat GBT. So there is definitely a lot of sort of strategy around it. So when the new technology is come, of course, there's a lot of interest around them. I would say that we still want to do, make sure we do diligence before we just jump on any sort of shiny object out there. So typically the immediate impact on a road map, can we fit a POC? Can we run some quick tests? Can we find the use case and see if there is, we can actually prove at the ability to generate business value? And if we prove successful, okay, let's go back to the drawing board, let's reprioritize and adjust our roadmaps if we truly want to pursue that new technology. So again, this is kind of the balance, right? Let's not just jump and rearrange world until we really know that this is where we want to go. Yeah. Still keeping focused on your main priority is, but bringing new innovative ideas and techniques into the pitch. As a POC, yeah, that would be my approach. James? Sorry, yeah. Totally agree with Julia. So for any new techniques, definitely like the general, generated for AI or chat GBT. So we need to align with our priority of the use cases or projects, or maybe the business request. If this is the new techniques is mature, or can truly turn the data into the actual insights to support the business. So I think there'll be the one things we want, we will start to think about. And the second thing I think I will say, because of how the other networks is the separate security company, we do have the uniqueness when we think about it, if we can, how we should move on with any new techniques. So for example, if the technical techniques like generative AI, and not mature in terms of the data privacy, or maybe the cause of the resources we need to spend, is there any potential issues we need to solve? Because we are trying to provide an enterprise level solutions. If we, if you're saying any, if we are not aware of any potential issues, and this is maybe, you should maybe propagate it to the different staff of the end-to-end lifecycle of the solutions. So, and also, the third thing is, as you say, is what we would a long-term goal or the short-term goal for the tech stack we're using in data science implementation. Are there any emergent requests for this kind of the new techniques before we want to start with any new techniques? I think we'd better have the good idea of the other strategy. How we want to use it? And what would be the best position for this thing of techniques in the whole model repo, or in the whole tech stacks? So I think that would be the three main points I would ask you to share. I think that's, you hit on a very good point of, does it align to the long-term strategy? And sometimes we can lose sight, right, with a new shiny object. But I think it's important to align and making sure our teams are aligned on that as well. Renee? I don't think I have much to say about that. I would say, I don't know if I'm a lot to say, but experience a little bit slower. I think maybe a lot of it's about privacy and governance, and they're a little bit slower to adopt things like that. So I don't see us using it anytime soon, but I would think that I'm sure that our data lab, that I believe it's down in San Diego, is probably already doing some work with it. Yeah. Great. So can each of you share a valuable lesson or insight from your experience and how you've maybe navigated some of those challenges? Renee, you want to go next? In data science, you mean? Yeah, I think just setting expectations for the team who you're working with, and then the business, what do they want to get out of, the analytics, what do they want to get out from data, and just making sure that you're always setting them up with reports, right? Just always having data at their disposal, and we try to do that ahead of time. So before we launched a series of models, we basically had all the Tableau Dashboards sitting there that you could ever want to say, hey, this metrics moving up, this one's moving this way, and this is kind of the expected impact we believe here. This is our best guess at how we could estimate revenue from this. The more you get them comfortable with that ahead of time, the better, and just be prepared to explain it, right? If you're not working with just data people all the time, which we love that, right? We love nerding out and just sitting there and talking about data all day long, and hey, wrote this cool piece of code and look what it does, but you have to really be able to communicate that, three levels up, and kind of get them excited about what you're doing, but also prepare them for what that solution is going to do once it deploys. And then, actually, one of the data scientists, John at DataIQ, was helping me kind of get some of this, like some buy-in to what we were doing. And he said, let's just forget the API for now, right? Because I was trying to get that implemented, and there's a lot of moving parts in places that has to be installed and all that. And he said, let's just batch them. Like, let's just batch our models, and let's prove the worth from them first. And once you get the buy-in, then it's like, OK, let's do the full solution. And that was a really, really good piece of advice. It was just kind of like, let's show a little bit, right? Let's make a small step and show what it can do, and then just say, what if I did five instead of one? Look what it does, and then what if I did all of them? I think that's a really good piece of advice, so I'll just steal that from John, and just say that, just like, prove a little bit of what you're doing, and then show them what you can do. And then once you get buy-in, then just say, like, analyst can do this, data can do this for you. Let's run with it, and it's been really, really valuable. James? Yeah. I totally agree with your name. So I think the challenge I experienced in my work is how to educate a customer and the end user to understand how the AI and ML algorithm model to help them. We're not replacing their work, but we are trying to turn the data into the insightful actions to help them, excuse me, to help them, just to part them. And then we need to let them know what we can do, and what we can fulfill based on their request or to fulfill their expectation. So I think I mentioned multiple times in this session to include business holders and users in the project execution. I think that's the critical things I'm trying to practice in my work. That will be help us to understand what the pain points are at business side. What's the best solution? Maybe we don't need to come up with more complicated model or because they don't care what the other advanced version of the Azure Boostware using or how complicated the neural network structure will build out. So they just care if our solution can solve their problems. So I think the customer business in your moment and the feedback, I think there will be the critical things or some things we should keep in mind in any products. So that's just my partner, you're turned. Well, believe it or not, I'm going to echo exactly what both of you said. Am I sort of less than learned is engage your stakeholders early in the game, keep them engaged consistently. In my experience, frankly, it's relatively easy to develop a model. It's so difficult implement the model to make it, again, be part of the system or part of the human consumption of everyday operations is extremely difficult. This is where my team faces the most challenges. It's like, oh, we have limited three-store engineering resources to integrate the solution into this process. We face issues like priority. This is not my priority. Also, if you know difficulties on implementation side of things, this is when you have to go back to your stakeholders, get their support. This is the frankly the only way to get the road blocks out of your way. So but again, you need the engagement alignment on the objectives, so support, and keep that going. As soon as you fall off of the radar, they're off on doing other things. So you have to keep that engagement and excitement going. So that's great. Well, I wanted to thank Renee James, Julia, for being here. It's a balance, balancing efficiency cost, bringing the right stakeholders in. When we look at value, value, dollars and cents, but also the productivity of our teams, emerging technologies, how do we do that? It's all about a balance with scaling AI. But thank you for your perspectives, and thanks for being here today. Thank you. Thank you for having us. Thank you.