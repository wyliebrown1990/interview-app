 Data Science can sometimes seem like an opaque process, especially if you're on the outside looking in. How does DataIQ help builders and project stakeholders stay aligned to their organizational values for Responsible AI? In this video, we will cover core transparency and explainability features that help teams maintain pipeline quality, enable data consumers to understand and trust model outputs, and provide stakeholders and managers with overall project visibility. Responsible AI practices include inspecting your data for potential flaws and biases. DataIQ's built-in tools for data quality, and ExploreDroid Data Analysis help you quickly profile and understand key aspects of your data. A smart assistant is available to supercharge your exploration, suggesting a variety of statistical tests and visualizations that help you identify and analyze relationships between columns. Specialized GDPR options mean you can also document and track where personal identifiable information or PII is used, yielding better oversight of sensitive data points from the get-go. For projects with a modeling component, DataIQ's visual ML framework offers a variety of tools to fine tune and evaluate machine learning models and confirm that they meet expectations for performance, fairness, and reliability. For example, to incorporate domain knowledge into model experiments in the form of Common Sense checks, subject matter experts can add model assertions, basically inform statements about how we expect the model to behave for certain cases. In this project, where we're predicting the likelihood of a patient needing to be readmitted to the hospital, let's say we have some assumptions that juvenile patients or patients who spent less than one day in the hospital will not be readmitted. A full panel of diagnostics helps us perform other sanity checks by automatically raising warnings, if any issues such as overfitting or data leakage are detected. Later, when evaluating our trained model, we can see that the test results in fact contradict our assertions and some diagnostics triggered warnings. So these are some instant signals that we should investigate further and possibly rethink our assumptions. Model error analysis is another view that delivers insight into specific cohorts for which a model may not be performing well, which enables us to improve the model's robustness and reliability before deploying. Speaking of cohorts in the data, subpopulation analysis is a powerful tool to ensure models perform consistently well across different subgroups. For example, after examining performance metrics broken out by race, we might choose to sample our data differently, or retrain our model with different parameters to ensure more even performance across racial groups. We can further ensure models perform consistently across different risk groups and are aligned to fairness thresholds defined by key stakeholders. We can use the fairness report. This report computes four different fairness metrics that help you assess whether positive outcomes for a sensitive or at-risk group are predicted at the same rate as for advantage groups. Finally, it's important for teams to be able to explain the influence of specific variables on global model results so that end consumers can trust the outputs. Feature importance quantifies the relative importance of each feature to the overall model. In this case, the number of patient diagnoses is the most important variable. Other views such as feature effects and feature dependence plots provide other ways to explore feature importance. Meanwhile, partial dependence plots display how the model's dependence on a single feature varies across this range of values. Here, we can see that as the number of medications increases, so too does this feature's influence on the prediction. Up until about 17 medications, at which point the model's dependence on this feature levels off and then declines. For AI consumers who are using model outputs to inform business decisions, it's often useful to know why the model predicted a certain outcome for a given case. In our example, this translates to why was readmission predicted for this particular patient? Role level individual explanations are extremely helpful for understanding the most influential features for a specific prediction. Or for records with extreme probabilities. For business users wishing to understand how changing a record's input values would affect the predicted outcome, did I accuse interactive what if analysis is a powerful tool? To use our hospital readmissions example, you can not only test out different scenarios for lab results and treatments and assess impact, but also use the outcome optimization capabilities to discover the optimal set of values that would lead to a desired outcome, in our case not being readmitted. You can even do what if analysis on image data? Notice how the activation heat maps help boost understanding of why an image was classified a certain way. Finally, for overall project transparency and visibility, zooming back out, you can see how data Icus flow provides a clear visual representation of all the project logic. Wikis and automated documentation for models in the flow are other ways builders, reviewers, and AI consumers alike can explain and understand the decisions taken across each stage of the pipeline. As you've seen with as many features oriented towards transparency, explainability, fairness, and reliability, data Icus supports teams in building responsible and accountable AI systems. To go further on other platform benefits like collaboration, extensibility, and AI governance, check out the rest of the key capabilities pages on the data Icus website. Thanks for watching!