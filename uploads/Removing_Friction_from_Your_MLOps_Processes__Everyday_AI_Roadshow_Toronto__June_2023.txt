 The flywheel has a lot of different sort of pieces of the puzzle. One of them is definitely how do you actually build and deploy your models, not just once, but do it repeatedly as quickly and efficiently as possible. And so here to talk about that is Kirsten Puganacker. She's going to talk about how we do that in Data Accu. Great. So sometimes it's discussed as a tooling or a technology builder by four the engineers, but ultimately it comes down to culture. So how are people working together to actually make AI a reality? So ML apps can be described as a framework, a blueprint, a mindset for moving models from prototype into production. And the ultimate test of what makes ML app successful is whether or not you know when your models are drifting and you know the moment that they're drifting, not three months down the road when there's disaster from that model drift. So we're going to look at this from three different standpoints today. The first we're going to talk a little bit about why and the labs can be such a challenge. I want to assure you you're not alone. A lot of organizations are feeling the same sort of friction and struggle. And then we'll talk a little bit about what I'm hearing in the field. So as a solutions engineer, I get to talk to a lot of different organizations across different verticals within different groups. And then the last step is to kind of make this a reality, right? So what am I hearing in the field? How do you fix it? And then what does that look like in action? So we'll walk through what that looks like in data. I could. So we'll start with those challenges. And I see this common tension oftentimes when I'm talking to different organizations and leadership is that they understand the need to scale with AI, but they struggle to control it. And it's kind of this this plus or minus. So we're going to start with a few reasons why AI is great for that scaling, right? The first is that you want to be able to have provide better customer experiences. So maybe it is better understanding your MPS surveys that you're sending out. Maybe you want to build better recommendation engines for your marketing. The second piece is about improving that productivity. So we know that you're being asked pretty constantly, can you do more with less? Can we actually use the skill sets that you have at the table? And then the third piece is knowing that that innovation is imminent, right? We need to stay in the market. You need to stay relevant. And the only way to do that is to keep up with that technology and innovation. When you actually are able to implement an ML process, what we see at least according to this McKinsey study is you can expect a 30% increase in your productivity efficiency brought process efficiency and up to five to 10% gains in your revenue. It's pretty big. So all this is really great, but it really is a struggle. It's not easy. And there are three reasons why this isn't easy. The first is a people problem. So we know that there's a shortage of full stack data scientists out there. And they're really hard to come by. They're really smart. They're really hardworking. But only one out of the 10 days that they get to work on machine learning projects are actually on revenue generating projects. So they might be busy, pitman-solving packages or building Docker images. Maybe they're talking to the engineering team to try to make sure that they have the right data that they need to actually train their models. They might also be having conversations with the business to explain things like what is a confidence interval? Why should I trust your model? Right? So they're spending a lot of time doing that. The next thing that they're doing is actually looking at the technology itself. So only when those DSS can actually work on these projects, only 11% of those models are making it into production and generating revenue. And that's not a lot. Right? So we're getting that they're working, that's assuming that we're getting everything into production. And then the third piece of this is the process. On average, it takes organizations nine months to go from prototype into production. And this doesn't even include the latency that comes alongside gathering that data, right? So you have to collect it. It has to land somewhere. You have to be able to get the right analytics before you can even start that model. And so when we actually push that model into production, it might be 12 months, not nine months down the road. And so we're lucky if that model isn't drifting immediately when it goes into production. So we've identified where we want to be, why we're not there. But I want to look at it from an organizational level. So how do these challenges actually occur within an organization? We're going to start with your standard model lifecycle. We've got this infinity loop. We've got the building side of that model, the deployment side. And I think it's pretty common to focus your ML ops framework to the processes that you're looking at implementing into ML ops on that deploy side. So you end up focusing on the right side of this infinity loop. And I want to challenge you today to consider the left side of this infinity loop as a really important part of that ML ops framework. And what happens on the left side is where you're actually asking that business question where you're getting that data prepared. You're running the analysis. You're running through the model iterations, right? So you can go through that build side many, many times before you get to the deployment side. But then you end up going build deploy, build deploy. So hopefully this process sounds a little familiar. We're going to add a little more complexity to it, right? So this is just the analytics and the tooling that goes into the model lifecycle. And then you start layering in the people, the cast of characters that's responsible for actually promoting these models. For the business subject matter experts who are asking those questions, helping with that data acquisition. And we get into the actual modeling and the risk managers who are assigned the task of approving models, making sure that they've got the proper documentation should an audit need to come about before you can actually deploy those models into production. And then you layer on the additional roles that it takes for the model monitoring to make sure that we can start the sleep all over again. And what you notice here is there's a lot of handoffs that have to happen at each step along the way. You have a lot of different skill sets represented in this full model lifecycle. And that's where a lot of the breakdowns start to come in this process. And I'll let you sit with this for just a second. Because this is also just for one model. This is to get one model into production. So when we think about the one out of 10 days that it takes the data scientists is actually able to work on it 11% or only are making it to production nine months of time for a single model. And the kicker is that the more you scale, the more challenging this becomes. So at first, it's really, you know, it may be a challenge to get these models going, but then they start rolling into production, the stakeholder seat of value, and the models that are being produced. And it just piles on to these ops teams who are then spending all of this time trying to fix the models that are in production. They're busy monitoring them. And it becomes a lot of work, right? So we need to actually fundamentally change this approach to regain that control. Okay. So I'm feeling pretty solid that we have established the challenges. I'm sure a lot of these points resonate with what you're seeing. So how do we start solving for them? And hopefully you've already picked up on some of the themes that I'm going to hit on today. We're going to pull that same build deploy ML labs framework together. But we really want to take a holistic approach to the ML labs framework at DataIQ. And if you remember nothing else from my talk today, I want you to know. That at DataIQ, we really wholeheartedly believe in collaboration as a tool to remove the friction from your process. So we want to make sure that everybody is coming to the same table, that there's a collaborative platform everybody's using the same tool. So as an analytics workbench for data scientists and the business alike, DataIQ also provides a really robust ML labs framework within that. So this solution is going to cover that entire life cycle. So from that data preparation to that model building and experimentation where the risk managers are able to come in, sign off, approve, we've got the model deployment and then monitoring those models once they're in production. So last week I had a customer come to me and they said, wow, DataIQ is really great. And of course I love hearing that. But the reason why they were mentioning this to me is really important. He said, this is the first time that our data scientists have actually been collaborating with our data engineers in the same platform. They were able to more accurately predict when these projects would make it into production and they were able to work through the conflicts that they were having a lot more effortlessly. And it's really cool to see this collaboration, right? So we're removing that friction. We're able to actually have that left hand physically see what the right hand is doing, right? So when you look at this diagram, that left side is seeing what the right hand is doing and vice versa. So the other part of this is we're able to pull in all of these different roles into this process. And it only is successful if we can follow three major pillars to be able to iterate through these cycles. We need to make sure that the process we're using is unified, that it's operationalized, and that it's repeatable. So that data science engineering collaboration, right? So they were in the same platform, it was unified. We're going to talk a little bit about the operationalization and how do we actually start scaling that in a repeatable manner. So a little bit more about what I've been hearing in the field from a unification standpoint. Everyone hates documentation, right? Those handoffs are really hard because you actually have to track things. You have to communicate with people, and that's hard, especially when you're not speaking the same language sometimes literally, right? So if you've got data scientists who are programming in Python versus R, you've got analysts who are working in SQL, you've got all of these different roles speaking different languages. So how do we actually have them go from one step to another and have it be seamless? The second is with the tooling complexity. So you might have business analysts who want the results in a Tableau dashboard, or business stakeholders who are requesting it in PowerPoint. And then you've got the data engineers, right? And they're running these orchestration flows through something like airflow. So they're not using the same tool set. And then the third piece of this unified loop is being able to innovate without throwing away the work that you've already done. You shouldn't have to start over from scratch every time you begin a new project. And I'm not just talking about the machine learning models that you're throwing away, right? I'm also talking about the data pipelines and also the logical models that your teams are building. So all of that is really important to have this robust unified loop. And it's a crucial first step. So analytics projects are going to need constant refinement, right? They need that iteration. And if you don't get that thing, start to go downhill really quickly. So you have to be able to unify the steps alongside the people themselves. And the goal is to really streamline that work from the prep processing to the data prep, the testing, the deployment, the monitoring. And it needs to be easy and natural for those teams who are working on it. And the first step of this is in that end to end environment with no gaps. So it's going to get rid of some of those tooling frictions that your teams might be feeling. And then this is going to happen in data, who with things like automatic tracking, and you're able to see what changes have been made. You can run through the experiment tracking itself. There's auto documentation that you can use within data, I feel. Keeps you in control on every step of that journey. And the second piece is that key, right? That collaboration key. How do we get people speaking the same language? And that allows you to harness the domain experts for things like making sure that you're choosing the right model. And the business experts are able to give their input to make sure that you're actually answering the business question you are seeking out. And the third step is that it's kind of like death in taxes. You know that new tooling and new technology is going to come along. So how do you make sure that your organization can grow within that? DataIQ isn't going to limit you and what models you use, right? So wherever that modeling is happening across your enterprise, dataIQ is going to be able to build that into the open platform. So we've got an easy to deploy loop. We're collaborating. We've got this unified loop going. But how do we operationalize it, right? So we want to be able to make sure that we've got the guard rails on the train. So what I'm hearing. One of the biggest time vortexes when we think about pushing something to production is the handoff between the data scientists and that data engineer where a lot of recoding has to happen. Sometimes in the same language, sometimes it's not nine months to production. A lot of that can be spent in that recoding process. And pushing those projects into production can also raise a lot of questions about quality. And I think organizations are getting better and better about that data quality side of things. So I want to make sure that you're also thinking about the AI quality that comes alongside that. And then the third piece of this is to automate so that you can actually feel. So less time is going to be spent on that monitoring. And I think that it's important to say that automation should happen where it's appropriate. Right? So we're not talking about automating just for the sake of automating. Sometimes it is important to have a human in the loop. How do you make sure that that happens? So again, we've got that unified loop happening. We're talking about this. That's going to help with the streamlining, but isn't going to prevent those inconsistencies from happening. And that's where that operationalization is going to come in. So if we want to get the MLObs fundamentals of that control, that repeatability, that scalability, we need to be able to operationalize those models. And so unlike other blows, what you design is what you operationalize in data IQ. And this means from day one, you are planning for production. The flows that you see in data IQ are actually what's going to get pushed into production. That's what's going to be bundled when you send it to the employer. And it's the model objects. And we can also then automate those data prep steps and make sure that we're collecting the ground truth and taking that into account at the same time. And the more we start automating, the more we need guard rails, which puts us in this middle bucket, right? So we need to make sure that we are keeping things safe and standardized along the length. So data IQ offers some built-in model validations to help you make sure that you've got the trains on the track. And then within those guard rails, we also have the ability to sign off and approve different models before pushing them into production. And those guard rails aren't going to work if you aren't able to have the visibility, right? So this is the third part of this slide. Now, who's going to centralize those models and project registries in something that we call govern? Okay, again, it's beyond just the models because it's important to also take a look at those projects. We aren't taking a look at the projects. We don't actually have the context, the broader context of what business initiative does this belong to. Who are the project sponsors? Who do I go to if I have questions about it? Right? So it's about more than just tracking those models. It's also about tracking the projects themselves. The final piece is about making sure that you can actually repeat these steps and start scaling. So what I'm hearing is that it turns out it's really hard to enforce different compliance risk, get all of this into a single place, right? So at scale, this bigger picture, how are we in control? And the second piece here is that we want to create AI value without starting from scratch. So how do we allow those team members to find what they need to use and then build on the project that have already been started or pieces of it that have already been started? And then the third piece of this is we want to make sure that those domain experts actually have a say in what they're doing, which can really be a struggle, especially with organizations who work in really hard data silos, right? It can be hard for those domain experts to then have a voice in the process. So we've done the hard work of building and operationalizing this ML apps loop. We have created an AI project that's going to deliver value, but now the organization is asking for tens, hundreds, thousands of models. How do we actually scale the work that we've already done to make that useful? And I like to think sometimes of building these models, like building a subdivision, right? So some of the models are going to be the same, some of them are going to be a little different. And the way that you really standardize this process and make it fastest by using something like a blueprint, right? So you've got all the architecture there. It's a lot easier to tweak certain rooms. Maybe you want to move the kitchen around in the house. And so data IQ is going to allow you to do that with blueprints that you can build out in govern. And those are going to be the flow templates that also allow you to plug in the right people at the right time. So you've got a similar risk team on similar business initiatives that you're delivering the same types of workflows is going to make it a lot faster. And if we go a level deeper, data IQ has a lot of different artifacts within each project. So this might be things like datasets, models, code, plugins. And all of these assets can be tagged and cataloged so that they're searchable, discoverable by different groups across the enterprise. So let's say that you are a marketing data scientist and you want to be able to run some NLP models on customer reviews. And you've noticed after searching through data IQ that the finance team has done a similar project. Well no longer do you need to start over from scratch. And in addition to that, you have somebody to reach out to if you get stuck or need help. Finally, govern is going to provide you with a streamlined registry of all of those projects and models across the organization. So it's going to provide one bird's eye view, a centralized watch tower of all of those assets and where they are in that model lifecycle. So in this case, if you're a risk manager, you can log in to earn, you can see the 25 projects that you're responsible for and decide where you need to invest your time. So it's unified, it's operational, it's repeatable. And before we go into the data IQ demo, where you actually get to see this in action, I want to share with you a quick customer story. And this particular story is from a US manufacturing company. Their challenge was to take 24 seven video streaming data from one of their factories. It needed to be processed, sent to an API, a live API endpoint for scoring. And then it needed to be assessed and then sent back to the factory floor to make decisions about the anomalies in their line. All of this was able to be wrapped in automation. So when the model began to drift, a notification was sent to an ML engineer who could then take a look at the project, take a look at the model, go ahead and push the new model into production and then take care of those results really quickly so that the line never had to stop. And fundamentally, this saved the company a lot of money. But it was built in a way that it could also be reused, right? So they didn't have to redesign this for multiple factories. So it was deployed not just in one factory, but across dozens. So again, it's unified. We're able to grab data. We're able to utilize the experts and the skill sets that we have. It's operationalized. We had the automated notifications, the retraining, and it's repeatable, right? So they didn't have to rebuild this use case across multiple factories. So these concepts can be really abstract. And especially abstract, if this is the first time that you're encountering what an ML ops framework can look like. So I wanted to run this through a project that would be relatable to everyone in this room. So I welcome you to close your eyes if you'd like and imagine yourself at your favorite coffee shop. And you've just bought your favorite drink and you go to either swipe or tap your credit card, right, to pay for it. And in this credit card broad case, that credit card transaction is sent to a live API endpoint. It's scored and it's determined whether or not that particular transaction was fraudulent. Sometimes maybe you get a text message saying, I think this is you, but I'm not really sure. Sometimes you'll get completely declined because you stole your best friend's card and now you're in a different country. So this particular story is going to be from the viewpoint of a model that's already in production. So it's a credit card broad alert model. And we're going to look at it from four different personas. So we'll take a look at it from the business stakeholder, the ML engineer, a risk manager. And at the very end, we will touch on IT operations. So you're going to see how all of these roles collaborate together. So we're going to start with the business subject matter expert in the place that they feel comfortable in Slack. And you'll notice here that they have two notifications that have come to them from Slack. And it's not that a model is drifting, right, because the business stakeholder doesn't care about the model. They want to know if they're missing their fraud. So the first one looks okay. The second one is that the fraud has been missed. And the business stakeholder wants to take a closer look. They've got a link directly in this Slack message that will take them into a data echo dashboard. And once they get into this dashboard, they can see additional context, right? So they can understand a little bit more what's going on with this model. And if they have additional questions that enables them to go ahead and reach out to the machine learning engineer to say, I'd like you to prioritize this in your list today. There's definitely something funky going on here. And so we can go ahead and take a look at what the ML engineers sees. They have their own notification that's already happened. They can see additional metrics in their own dashboard about what's going on in this particular use case. And they actually can do deeper dives, right? So they know that there has been an automatic retraining on this particular project already. And they want to take a look at what we call the model evaluation store. This is going to give them a look at some additional metrics. So they can take a look at things like, some of this low accuracy precision. They can see additional data drift metrics here. And really see how this model is changing over time. And they can double click down into particular models. And here they are able to get some explainability metrics. They can take a look at things like the input data drift. So in this case, we want to see if the data that the model was trained on is different than the transactions that are coming in. And we'll notice that this card age is actually drifting with new applications that have potentially come in. So we want to see what that retrained model is going to look like with those new applications embedded. We've got this V2 model in the middle here. This is the active version that is drifting. And we also have V3 below. So this was automatically retrained and machine learning engineer can head straight into this retrained model and start validating. Is this the model that I actually want to push into product? So that machine learning engineer feels confident that this is the right model for them. Or they can make the changes that they need to accordingly. So we'll see this V3 over on the left hand side. It's waiting to be pushed into production. And we see here we've got a model in development. We've got one in production. And we're almost ready to introduce our third character here. When this engineer goes ahead and hits that deploy button, we're actually going to hit a governance blocker. Okay. So what that means is we are actually trying to push this into production, but something isn't finished in this process. So we're waiting for some sort of approval. Maybe that entire ML ops process hasn't been gone through in the proper way. So here's that governance status. And this is going to trigger an alert to our third character, that risk manager, who spends most of their time in the governor. And this is what that looks like. Again, we're thinking about blueprints, right? So on that left hand side, we can see where in the workflow this particular project is. We can get some additional information about that project. We know what business initiative it belongs to. We can see the different sponsors of the project, the description that that follows that. And this is just one project of many that this risk manager is trying to keep track of, right? But it's all in one place. It's really easy for them to see it. So they got the notification. They want to pop into the model real quick. Take a look at that V3. And when we click into that V3 on the blueprint side, we can see that it's waiting for signatures, right? It's in review. This is a place where different personas might come in, might have a risk manager, IT. Making sure that that model is hitting all of those compliance pieces that it needs to be foregoing into production. And this model is definitely high risk. It's very customer facing. And so all of those steps along the way could be fully automated. But for the credit card fraud, we had a lot of human and loop. So what you're looking at here is how you would automate that full process without having all of those handoffs, depending on your application. And that's really going to streamline some of those data quality checks, the AI quality checks. And make things really streamlined. So as you've seen today, data IQ is taking a fundamentally different approach to ML ops, where we're really trying to cover that full model lifecycle and address multiple stakeholders at the same time. We're unifying things. And on top of that, we really want to be able to remove that friction through automation. And we want to make sure that you have a really simple path to deployment. And it's repeatable. So it's not just a one and done. You're building for production from the beginning, not just with one model, but with many models that have additional reusable components. So when we take on this holistic approach, we can build trust with the business. You can reduce that time to value. And most importantly, you're creating a culture of collaboration. And that's going to turn your CDO organization and your ML ops teams into an intelligence engine for the entire company. So why do we care about it? Why are we going to invest the money into creating an ML ops framework like this? And the thing is that we don't want to just add value with AI, ML, and analytics. We want to be able to multiply that impact across the organization. Oh no, we're not ready for pristine yet. To make sure and making sure that we're going to maintain that control. Because it's the only way that you're going to be able to scale effectively. Okay, now it's going to go. Great. I think you are.