 Hello and welcome to the Observability Trends webinar series. I'm Grant Swanson your host and today we will dive into the topic of navigating regulatory compliance with observability. Now I'd like to introduce our guest speaker, Senior Sales Engineer Keith Buzwell. Welcome Keith. Hey Grant, thanks a lot for having me today. Yeah so just getting right into it, navigating regulatory compliance with observability, a modern approach. So, you meet it here, there's three things I really want to dissect just from the title of this webinar today. So, regulatory compliance, what is that? You know, what does that even mean? Set a little of a baseline there. Just make sure you know we're all on the same page what we're talking about but then doing it with observability and a modern approach. You know, so what are all those things mean inside of here? So, just getting into the first one is that, you know, what is regulatory compliance? And really these are the rules that a company should be following to make sure they're taking care of their employees data, their customers data, partners data, and it varies. You know, so if you're in banking for example, there's going to be some differences on some of the things that you should be doing there. For example, in things like HIPAA, NERC-SIP and with critical infrastructure. So, it really depends by the industry that you're in and what type of regulatory compliance is and kind of measures that you're going to have. And it covers much more things outside of just logging and you know where workload should be running and things like that. But it even goes down to like HR type of policies and stuff like that. But it goes very, very in depth. But for the purposes of today, I really want to discuss what it means, you know, to do it with observability. And it can even depend on geography. So, for example, if you're living in the EU and GDPR is going to be important to you if you're living Australia or even California. And you know, there's going to be different laws that you would need to adhere to on what type of data, how long you keep it, what you do, and you know, how are you securing that data as well. So, there's a lot of variations there and which kind of leads me into this next portion is what makes it so difficult. And I just, I've been in the logging realm, I would say for around 10 years now a little bit over. But in every time compliance comes up, it's always, you know, it seems like it's a bit of a difficult thing. And some of the reasons are is that it varies so much, you know, depending on your industry, you know, what type of things should we bring in about your customers. Matter of fact, your own customers themselves can have an impact on what regulatory compliance is that you should be adhering with. You could have multiple regulations, you know, you could have a different type of a tool stack in your product stack could mean that's like, hey, the products we're offering means that we have to have multiple, we align with multiple regulations because of the type of customers that we serve to. And then of course, location, but another thing that makes it difficult is that traditionally it's been, it's been pretty expensive to adhere to as well. And some of the reasons for that being is things like long retention, you know, I think, you know, like PCI for example, you know, like 90 days readily retrievable, but one month or one year of, you know, kind of backup or an archive. And then so you know, just saying 90 days readily retrievable, and that can be achieved in various ways, but essentially it's saying, you know, at any point, I need to be able to go ask this question about the day that is, you know, in the PCI scope and make sure I could treat, you know, retrieve that data quickly. And that, you know, anything with that first 90 days, but then a year after that, you could have it in something a bit of a colder state. So that's complexity as well, because you know, for example, just like that is that well under the first 90 days is here after that need to go over here. And how do I interact with that data? Do I query it the same? Does everybody know how to access that? So there's some, some complexity that gets added to some of those things. And sometimes it can be a lot of data, depending on how large your environment is or what your business is, and the services that you offer up, it can be very expensive and to do some of these things. And then sometimes, you know, I've even seen like hey, we're just hosting something internal to just logo this data. So, and that's fine. However, now you have infrastructure costs. Somebody needs to maintain that environment. You got to make sure it's up to date. And if that has data coming into it that has maybe PCI or HIPAA related data as well. Now that's in scope as well. And so it has to adhere to the same kind of things that we're, you know, we're trying to log from in the first place. And then if you're not doing things right, sometimes there's some very large fines that are wrapped around this as well. So just some, just some things that I would say that have come up over the past 10 years that I've seen that have just made this kind of a difficult problem to solve. And then it even comes down to give a, hey, I know my industry, I know my customers, I know the regulations. And then really kind of getting done. Okay. So now, you know, exactly what data is it that I need to be collecting. How am I going to get that data in and, you know, where am I going to store it? How am I going to store it? So there's a lot of questions to ask there as well around some of these steps. So some of the things that I've seen as options of that have tackled this problem in the past, you know, so one of them is just using your existing logging or observability tools. So I just I have a logging background myself. So I've seen this attempted, but I would just say previously, you know, so I was at some of the pros around this is that, you know, a lot of the different platforms that I leveraged had their own agents. There was an existing pipeline going into that logging platform. And you could just kind of piggyback off of that, you know, more often than not, that data was already being collected. Maybe there was some additional things that you needed to bring in or remove some filters to allow that type of data that meets your regulations to be to come in. And then often, you know, you had to have it some type of option to archive that data. You know, I could say that you see this data go archive over here and, you know, we don't we're not going to use it. And if we do need to use it, maybe it's like a rehydrate or re index type of function that you would use. So there was some of those pros on the cons is that most of the platforms that I've leveraged in the past. In some fashion, the ingest that the data that you were bringing in that was tied to directly your licensing cost. You know, said, it's like, okay, you're bringing in, you know, say 100 gigs of data per day. There's a cost to that. So your costs would go up to bring in this type of data. And it could be very expensive. Sometimes it just bringing data. It's like, hey, I just simply need this for compliance. You know, I don't feel like it should be that expensive. I don't even really plan on interacting with it that much. So it created some some very expensive scenarios, depending on what you were trying to do. And then also like if, you know, the data was archived off, you know, which was sometimes an option. It would become cumbersome to use it is, okay, where is that data? What's the process of going about interacting with that data? Can I just query it where it's at? And it's slower. Do I have to re-injust that data and put it into an index and then make it available inside of there. And then sometimes even bringing that data out of the mechanism that it was in became very expensive as well. So it's just like with the complexity and so it's never been a very easy problem. Another option that I would see is kind of one of the things I already mentioned was building a appliance logging pipeline. So it's like, hey, you know, whatever team has their own logging observability tools, we're going to go build our own thing. That's just for compliance on make it super cheap storage. It could be, you know, obviously more cost effective than commercial off the shelf. You're not locked into vendor collection pipelines and stuff like that. So you have something that's very, you know, repurposed and kind of easy to, you know, take from one scenario, one environment to another. But some of the cons, you know, once again, now we have somebody that has to kind of maintain this environment. Some of the upfront costs and building a pipeline, you know, a lot of times I would see it something like backed up into like S3 buckets, for example, or just building some type of routing pipeline that would, you know, dump into some type of blob storage. But there's an upfront investment to that because you know, some of the things that you have to understand is, okay, how are we going to collect that data? What tools are available to grab that data? How are we going to build the pipeline? How do we route it? And you know, there's also costs associated with that as well. And a lot of times that I was interacting with those type of scenarios is like, hey, we're just going to go put it in like this S3 thing and then, you know, maybe use Athena or, you know, some other mechanism that will allow me to search my blob storage. And you know, a lot of times it was slow. You know, I think the query language more often that wasn't terrible. It was kind of SQL based on a lot of times, which I think is a very common query language and a lot of us know it. But also it wasn't very intuitive, I would say. It was just kind of is what it is and you just kind of dealt with it. And then one of the big downsides for me is that, you know, when that data was collected and it went into somewhere else, you just really didn't have an opportunity to correlate that data with anything else anymore. It lived on an island. It was in its own silo. And for the most parts, that was okay because maybe the data didn't have a ton of value and correlation capabilities inside of there. But, you know, there was everyone's thoughts like, it would be interesting to see how this is tying into the things that are in my observability or my logging platform. But really just didn't have an opportunity to do that because it just kind of lived on living on an island there. So those are some of the things that I've seen. Just, you know, kind of throw my 10-year with just logging and trying to, you know, see these different compliance and regulatory use cases in some of the different ways we went about trying to solve it. So, you know, taking a bit of a modern approach to this. Now, what does that mean? So some of the things that I'm looking for nowadays when I think about, hey, I have all this data. I want to bring it in. But, you know, there's lots of scenarios where I want to leverage cloud native architecture. There's a lot of economies of scale being able to dump stuff, like I mentioned, like into S3 storage and stuff like that. And have stuff that's very elastic compute. You know, if I want to go interact with it, if I'm interacting with a lot, be able to scale up, be able to scale down, have unlimited storage, be able to have very cheap storage as well. Also, I don't really want to think too much on how I want to ingest that data. And so, schema agnostic, what I mean by that is, you know, for a lot of examples, some of the tools that I've leveraged is like, oh, well, you know, as that data comes in, it needs to pass through this pipeline, it needs to hit these parsers, these parses need to extract these fields. And if you don't do that, then really it's difficult to interact with the data once it's inside the environment. Sometimes you just, you can't even really interact with it all. So, it's like, I just don't really want to think about that too much. I really, I just kind of want to throw the data in. And if I do need to create some type of schema or extract some values later, I want to do something with it. I could do that at any point I want. So, I think this is just kind of a more of a nice to have just considering some of the things that I've leveraged in the past. A flexible data collection pipeline. I don't really love being married to proprietary agents. You know, some, the agents typically work well, I would say. But the problem is, is that if for whatever reason you need to, you kind of break up with that vendor. And then now you're left in a scenario, it's like, okay, now we need to go redo that work for putting on new agents and configurations. And so, I really want something that's flexible that can leverage in existing pipeline that I'm building. So, for example, if I'm leveraging like FluentBit agents or Telegraph or Grafone agents, kind of really, you know, anything in that open source realm, I really want to platform that can just consume the feed from those agents. Just like I would be doing if I was building my own open source pipeline and putting it into there. So, I would, I want to be able to reuse the pipeline that I built. And then of course, cost effective. You know, that's a big one, right? That's one of the biggest reasons that I've, or one of the biggest things that I've seen and a lot of the architectures is that, hey, you kind of bring in this data, it's attached to ingest. And it's going to be very expensive. So, something that's cost effective to do this as well. Now, what does this mean for Observe? And doing it inside of Observeability Platform, such as Observe. So, really, one of the key components that we've been able to make things very, very cost effective, but also very performant, is the concept of separating storage from compute. You know, we really, we're reusing both when we're interacting with data and we're ingesting data. But really, they're kind of serving two different purposes. And I, in one really shouldn't necessarily from a licensing perspective, be associated with the other in some scenarios. And I think compliance logging is a great use case for that. Because if you think about it, I have all this data that I'm bringing in. I really probably not going to interact it with a whole bunch. So, I'm not going to be using a whole lot of compute. But I might be using a lot of storage. Well, I really maybe only just want to get it charged, you know, mainly for the storage and not so much the compute. So, being able to decouple those two things from each other is one of the key architectural components that Observe did when we design the solution. So, what does that look like? So, starting with compute. Now, on the left, this is kind of what I'm used to in the past. So, data is going to come into your platform. It's going to get put into these storage tiers, for example. We're going to talk a little bit about storage here in a second. But everything was tied together. So, if you, if you brought in more data, the compute, the storage, it all scaled at the exact same, essentially the same size. And that build, the license that you got charged, it was all tied together. And then sometimes there would be tiered storage, for example, something to reduce some of those costs. But now, it also created complexity. How do I queer that data? Do I need to do it in a different way? Does the query language, even the same, do I have the same capabilities? I'm going to dive in that a little bit more here in a second. But we really wanted to decouple those two. So, starting with compute is Observe is hosting several data warehouses on the background. So, when you go run your queries and one of the key architectural components is that we partnered with Snowflake to deliver the back end of our product. And so, with that, we have really low cost storage. And we also have very elastic compute in the fashion of these data warehouses that we're all running in the background for you. So, now, when you go run a query, we can look at the data that you're interacting with and go pick the appropriate size data warehouse and a sliver of that to go run your query against. And as we're hosting these resources, if some of these resources aren't being used, we can scale them down. Or if we say, oh, yeah, there's a lot of queries happening. And maybe I'll only have one Excel large available right now. I might want to go back it up with another one. That way that everybody's getting the exact compute they need at any given time. But we can scale those resources up and we can scale them down. But we don't need to tie the compute and the storage together and also be very performance anytime you go run a query as well by doing this. Now, what's, you know, so the way that we're achieving this is that we have adopted the concept of a data lake. Now, that data lake for us is essentially going to be S3 storage that's going to be feeding into Snowflake tables. And I'm going to go, we're going to show that a little bit in the UI here. But one of the, one of the key foundational things that we've done is we've aligned with an open source collection pipeline. So as you go deploy your telegraph agents and your beats agents or whatever it is, you can keep using those. You can repurpose those pipelines that you've already built or partners with curvil as well. Great tool to be able to pipe data into us. So it's a, it's a, it's just a really flexible platform to be able to get that data. And then also with things like AWS and as you could deploy our apps in our apps, not only is it going to have dashboards and things like that free to use, but the entire collection pipeline is also built into our apps. So when you deploy the app, you're also going to get all the tools to get in that data as well. So it becomes very easy. It removes that burden of having to think about, hey, how am I going to get this data into the platform? And you know, kind of how do I interact with that data? So a lot of nice things baked into that. But as that all that data comes inside of here, it's going to go live inside essentially S3 buckets in our platform. And it doesn't matter what it is if it's a log, a metric, a trace, everything business contextual data, all lives inside of that data lake. So it reduces some of that complexity that you might have like, okay, well this, I have to put this type of data in this type of index after put this type of data in this type of index or it goes into this type of back in database. So all of our data lives in the exact same spot. Also all of our customers get 13 months of retention by default. So just out of the gate, that's what we give everybody. And a lot of times like, oh, wow, that sounds expensive. But the way that we can decouple storage and compute and since we've partnered with snowflake to deliver our back end, one of the things that is really nice about our platform is that we average about a 10x data compression. So if we're talking in terabytes, if you figure for every one terabyte that comes into our platform, we're going to compress that data 10x. So a matter of fact, if we even leverage like a number like 10 terabytes because you kind of use a nice number that we know will compress nicely. If we use a 10, if we have 10 terabytes of data coming into our platform and we compress that data at 10x, now this is where our monthly storage charge would come from. And this is just piggybacking off a publicly listed S3 storage rate. So 23 cents a gig per month. So 23 dollars for every terabyte that you bring in. So you could figure for every 10 terabytes I bring into the platform after compression, I'd be paying about 23 dollars a month for that data. So very cost effective to bring in a lot of data and just store it inside of here. Matter of fact, this is even more cost effective than I've been able to do it with my own S3. Like I have a guy I don't want to put it in maybe to another vendor's platform. I can do it cheaper. I haven't found that to be to be true is that I can that you know, since we charge post compression on the storage. It makes this very, very cost effective and very appealing to be able to store the data inside of here and on a hot ready to search state all the time. There's no tiered storage. You don't have to think about where it's at or anything like that makes it very, very, very compelling. So with that storage, you know, some of the some of the different things that I've seen in the past, obviously it's like you're going to have your hot storage and maybe you'd say what's called seven days. We keep data in there seven days and now it goes into warm storage stays in there for 30 days and then everything after that maybe goes into cold. And this is kind of what I've been used to doing. But once again that complexity comes in how do I query that data? You know where where does it live to even can I even dashboard off of that? You know, or like what capabilities do I have when it's at these states. So a lot of that complexity came there, but it from us it's just cloud storage. It's all hot. It's all really tribable. So if you have 13 months, if you go interact with that data 13 months from now and you interact with that very first data that you brought into the platform. It's going to behave and it's going to respond the exact same way if the data that it was ingested this morning. And that's one of the big foundational concepts is that we want to make it really easy. We want to make a very performant and also cost effective. And these are some of the ways that we've been able to achieve that. So what I would like to do now, just considering kind of logging and like just compliance and regulatory. It can be a little bit difficult I would say to demo that. But there's a couple key foundation things that I brought up inside of here that I would like to go interact with and do inside of the platform for you. Just to kind of show you like, hey, this is what I mean by like there's no tears and everything works exactly the same with each other. So what I do is I'm going to back out of here. And we're going to go inside of one of our observed tenants inside of here. And I think this is a little bit small. So I'm going to open this up and what I want to what I want to start with is over here on our applications. So one of the things I mentioned is just hitting the easy button. It's like, hey, you know what, I have this regulatory compliance. And you know, maybe for whatever reason, like me or me data that lives inside of your containers, work loads that are running in Kubernetes. I need to make sure that I'm retaining that data for a long amount of time. So when you go deploy our apps, not only is it going to have dashboards and the different modules to go create data sets from the environment. And you know, go create all the specific things that you want to interact with. But all of the collection mechanism as well or built inside of there. So, you know, I mentioned that we aligned completely with open source agents. You know, at the end of the day, you might not be open source collector experts. And that's okay. You know, because we are. And so what we've done is that we've baked open source agents inside of our modules. And, as a matter of fact, every module in here has the agent baked inside of it. So when you go deploy the AWS app, for example, all of the things to get that data in are baked inside of the app. And that's true for all of our apps inside of here. So that's a really nice way just to really get up and running really fast. And you just don't have to think about how to get that data. So, you know, what mechanisms and the configs and things like that. It's all done for you. And now as you deploy these apps and you start getting the data and this is that data lake. So all of your raw data is coming inside of the environment right here. And you can see like, for example, I had the AWS app deployed. I had the Azure app deployed. You know, some of these other ones inside of here. So at any point, I could just come in here and go take a look at AWS, for example. And I could go open this data set and look at the raw data. So if you, you know, a lot of compliance is going to say, hey, I want to be able to, you need to be able to prove that you have the raw on a modified data for X amount of time. And as I go open up these raw logs and I start kind of just listening through these. This is the exact feed of data coming from our AWS account and it's raw format. So you're very easily going to be able to access that. And I'm going to come back to this in here just a second. But I want to show that you're always going to have the raw data available to you inside of here. The other thing that we're doing is that we're building a navigational guide of your environment as well. So if I go open up the data set graph, for example, and I say, okay, well, you know, all of that Kubernetes data that I was bringing in, we create this GPS guide essentially inside of here. And you can kind of see how everything gets related and it's linked together. And for compliance purposes, there might not be a ton of use cases around this for you. However, it is a really nice thing to add because the other thing is, you know, if you wanted to build just strictly a compliance pipeline, I want to dump data into some platform and keep it very cheap. I'm never really going to interact with it. Observe can be fantastic for that. However, there also is a snare. It's like, hey, you know what, my team, we also want to kind of really, we want to democratize our data. We want to, you know, do all of the regulatory compliance things and we want to put that somewhere, but it needs to be in the exact same spot that we're already collecting all of our other data. You know, so for like our developers and maybe the security team and we want to own one platform. And then that's where when you start looking at everything live together inside of here, not only is it super cost effective. You can also do some really cool things around that data once it's inside of here for your developers and your engineering teams. And we have some other awesome webinars that really go into depth about all the cool things that you can do. So I definitely recommend taking a look at some of those. But going back to compliance. I just wanted to show like how, you know, not only is the data going to come in here and it's not just doing nothing is that we also kind of create this navigational guide of your environment as well. So a couple things that I like to talk about, especially just, you know, just from a, a compliance standpoint is that there's going to be some really nifty things that you can do. So if I come into our log explorer right here, one of the options. So just kind of think of like like payment logs. Maybe I'm in an industry where we have payment logs. I can see I have clear text, credit card numbers listed right here. The C, the C, C, B listed here as well. Now for all intents and purposes, more often than not, I don't even want to bring in this data to begin with. So a lot of things that we would do at the collection level is like, hey, what's going to identify these patterns? Let's just drop that from coming in. And that's perfectly viable. There's some scenarios that's like, well, I didn't even really know that data existed inside of there and observe can help you with trying to remove this data from the platform as well. But there's also a scenario where it's like, well, you know what, I need that data. I need those logs because I start to scroll over here. I also have like this nested payload inside of here that talks a lot about like, hey, what's going on with that transaction? And my team might need that to troubleshoot. But what they don't need is they don't need these credit card numbers. And this is where I brought up that flexibility that I mentioned earlier. You have the ultimate flexibility to really kind of do anything that you want with the data inside of here. And I just don't really want to think too much about the schema. So one really nice thing that you can do inside of here and you can do at any point you want is that. So for example, but well, I'm seeing this credit card number inside of here. I want to publish this data set in a way that my team can use it so they can troubleshoot what's going on with these transactions inside of here. But they just don't need this. And so one of the things I can do is, for example, I'm going to come up here and go to actions. I'm going to say, let's go open up a worksheet and a worksheet for observe is a bit of a Swiss Army knife. You can create runbooks inside of here, troubleshooting workflows. It's a bit of a playground where I can kind of start doing adding additional data sets and making correlations. So it's just kind of a really nice area to do some of the things. But also I can I can start customizing this a bit more as well. And in the data right now looks exactly like I was looking at inside of log explorer, except for I can come in here and you don't give this worksheet a title if I wanted to. But the other thing that I could do is, for example, if I come over here and go to opal and this is the observed processing and analytics language. And it's going to bring up our query window here is now I could come in here and do something like drop column and I'll start typing credit card. And so say, okay, you want to drop the credit card number, but yes, but I also want to drop the CBV number. And now when I go run this, what it's going to do is it's going to go grab all that data. But now I don't have a credit card number. I don't have the CBV number, but I have all of the data that my engineering team or developers need to troubleshoot the issues inside of here. And all of the raw telemetry is listed here for me. And one really cool thing I can do is I can now come over here. I could say, go publish this as a brand new data set. And we'll just say logs for developers or whatever we can create this naming doesn't really matter too much. You can name this whatever you want. And now when I hit publish, what's going to happen is that I can now go lock down my original data set payment logs. And we could say that maybe there is a compliance team and maybe they need this information and they can come in here and access it. But the other team comes in here and accesses the developer logs so they can do their job. But it creates just a super flexible way that I can drop columns and I can manipulate the data and I can do things like that at any point I want. And I don't have to think about re-indexing. I don't have to think about schema. I can apply schema on demand whenever I want inside the platform and use roll based access control to go lock down those data sets. So that's one thing that I thought was really nice when I was interacting with this. And then we also have the flexibility. For example, I could come inside of here instead of doing this. I'm like, well, I really don't want to drop the column. Maybe we just want to redact the payment logs inside of here. And so one option that we chose in this avenue is instead of dropping the column is doing more of a hash. So just go hash that credit card. And so now inside of here, I can see it's just kind of a random number that I'm taking a look at. And so that's a nice option here for as well. It's like, okay, I know that there's a credit card number inside of here. I can't see the credit card inside of here. And also maybe you want to do something more like this where we're just more doing like a pattern replace. Like, okay, that CVB number just go put stars over top it. I know it's a three digit character. I know it's going to be some combination of numbers. Just go find that and put stars over top of it and then go publish that as you brand new data set. So just really flexible and being able to do those things inside of here. So, you know, for compliance use cases, those are just some common ones that come up. It could be, you know, HIPAA related data. It could be credit card related data. You can kind of really think about all the different scenarios. But it's like, okay, I really want to drop this column. And here you guys can go use this or I want to redact something. All of those capabilities are inside of here. They're all built in very easy to use and be able to publish these new data sets for whatever use cases might be. The last thing that I wanted to show for you today was the fact that I mentioned, okay, we don't have tiered storage. It's all hot. It's all ready to go for you. And it all interacts the exact same way. So what I want to do is go back into the data streams that I showed just a little bit ago. And if we come inside of here, I'm looking at so the first time I received data from some of these four months ago, three months ago, I want to find so six months ago. So my Kubernetes data set, there's data stream that's coming inside of here. We started ingesting that data about six months ago. And if I open this up, it looks like that I get about 5.2 million data points per hour inside of my Kubernetes data. So a pretty decent amount of data. And what I want to do is I'm going to say, hey, I want to go bring back all of it all six months of that data. And I want to go interact with it because maybe something came up or an auditor is asking me a question. And it's like, hey, just show me that you're collecting this data. I want to go maybe do some certain things with it. So what I'm going to do is I'm going to just say, let's go open up that data set. And it's going to open it up into something that's a very resembles the worksheets that we're just interacting with a second ago. And by default, you can see it's like, okay, well, here's your past 15 minutes of data. And if I look down here at the bottom, that brought back 1.2 million logs and one and a half seconds. So what I'm going to do is I'm going to say, well, you told me that the data goes back, you know, it's six months ago. So what I'm going to do is I'm going to come inside of here and date range. And today is actually it's going to so with today's the 14th. So what I'm going to do is I'm going to say, well, okay, let's go back out. Let's go back this out and let's go to 23 and let's go to December 1st. And let's go see what data is available all the way back there. And if it's not available, there's just no data in it. It'll still return. But I'm going to say bring back everything. Let's go do that. And so now what it's going to go do, okay, so here's our results. So we're already done. 12.2 billion logs and 2.6 seconds. And I can see down here, you know, minus 21 weeks ago, minus 17 weeks ago, I can kind of see a little bit of a time series chart inside of here. But what I'm going to do is I'm going to say, I don't believe you. I don't believe that I have that data and you brought it back that quick. And what I'm going to do is I'm going to come over here and go sort earlier to later. And once I come inside of here, so December 1st at midnight is that we can see if I go open up my raw data right here, just like I showed before. And I start tabbing my way through here. This is the full fidelity raw data as it was sent inside of here. So it's not just metadata. We're able to interact with very, very old data just as effective as it came in this morning. And the other thing too is I have all of the same capabilities as I did with any of the other data. If I wanted to say extract from Json, for example, I want to go grab some fields from here. It's going to drop me into that playground again. That Swiss Army knife, the worksheet. And it's going to say, well, what do you want? Maybe let's go grab that container ID, the container name. Let's go extract some fields from the object, maybe like the API version, whatever. Which is go pick some random ones here and hit apply. And now it went and built that queer language I was talking about. And all of those fields that I extracted are now listed right here. So container names. I can come in here, start doing filters off of this. I only want to look things that related to Kafka, maybe. And this is all interacting off of data that has been in here for five plus months. And it's in its raw format and it's doing it the exact same way as data that showed up this morning. So I hope you found this informative and maybe shed some light on some of the ways that we can, you know, maybe help you improve your regulatory compliance logging. Make it easy for you. Make it cost effective as well. But then also be able to interact with it in a much easier fashion than, you know, just dumping it into something like an S3 bucket. So, hey, appreciate it Grant. And back to you. Thank you Keith for the excellent presentation and demo. We encourage everyone on the webinar to go to observing.com and request a demo or sign up for free trial. You're also welcome to join us at any of these upcoming events as we will be at the DevOps con in San Diego. We also have a technical hands on observability workshop on 530. And don't miss us at the snowflake summit for our breakout session on building a modern observability platform on snowflake. This concludes our session and thank you for your attention.