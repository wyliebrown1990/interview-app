 Thank you so much for the lovely introduction and also thank you for the interesting talk before especially the end. I mean I could see myself as well like Andreas was saying hopefully in the next 12 months we're going to be less cautious as the German corporation also as a German of course and be more open to change and be more open to actually implement things. My name was already introduced my name is Chris Kruger senior data scientist at the ZBank maybe not all of you know what the ZBank is sir I'm just going to introduce you shortly basically the ZBank it's I also have a slide so perfect. This financial institution the lead financial institution for folks bank life is in bank in Germany it's also part of the design group so having in their portfolio insurance investments so I think some of you probably know already these logos and have seen them before. Yeah, these things as such as I mentioned is a lead institution of 730 banks folks and high fives and banks in Germany but we also are working with corporate clients and institutions on a local on a region but also on a very international level together of course hoping for shared success altogether and each of us benefiting from each other. Well that's enough about my employer so it's back to me so the story is about me today so I'm also going to use the time so basically I joined a ZBank three and a half years ago at the beginning of 2020 so right at the start of the pandemic and as part of the data science team I was also part of the innovation team so we have these things called innovation labs or data labs basically they are they are think tanks in a design bank where we help business units going from an idea to developing a use case and then developing this use case to a proof of concept. So this whole process got developed over time it started to get more and more successful so we did start to get traction and more and more people got interested in this whole thing but at the same time of course now suddenly we had proof of concepts that also wanted to be operationalized but we didn't have like a straight way of operationalization so the data science team kind of got split in the way what topics they're handling on the one side people were operationalizing on the other side we were continuing to drive innovation but then of course a lot of other people a lot of other business department came to us wanted to do something with us but we didn't have all the time so solution to that or something we were thinking of was of course we would have to enable these people to to be more self-sufficient to give them self-service tools in order to do things themselves if it's an easy use case so basically my time now is kind of split or the data science team in general at this time is kind of split between actually introducing self-service solutions which also includes data EQ as one self-service AI solution and at the same time also still developing in-house application the latest one being version of JGBT that can be used within bank within bank sensitive data so basically this point where where we did the split we had a shift in our strategy inside the bank going from AI as a very special tool for selected single use cases and also AI driven by innovation having to go through the whole process which don't get me wrong it's not about approach or anything because it of course lean its risk making it less risky than just starting a project especially if the I tools we know you never know if an AI application if in the end it might work or not if you just start a project but at the same time like I was saying we let out a lot of use cases untouched because we just couldn't cater to them so we decided we need to to make a shift to next to using AI as a special tool for select use cases also using AI as a way to augment our co-workers daily lives so basically providing a self-service tool for them that they themselves are able to drive their business innovation because I think as most of you know I mean even though we have business units and IT units a lot of the business units are actually very capable especially in our bank and they more or less are able to develop their own applications if they're given a set tool and also set guidelines to do it within so what happened or what we came up with was basically we first put our data into a central data lake in this case hosted on the Google Cloud so every department has their little space on there where the data is hosted and then we use several self-service tools in order to to make this data accessible and actually drive insights out of them so first one being a colibra I mean I guess some of you know or most of you know it's a meta data search engine in that thing all the data sets in our data lake then we use BI tools one of them for example is tableau so the departments can just use the data lake use the data in the data lake to visualize things and then drive insights out of it making decision based on that and last one of course data IQ for more advanced analysis so if you want to combine data in our data lake into new data and if you want to do machine learning and all of the more fancy things then this would be the tool for you so in general like the three things we wanted to achieve with these self-service tools was first of course we wanted to accelerate the value creation like I was saying we had like this bottleneck being our department to be honest now opening up within guidelines to the whole bank and then the next one would be to basically be more efficient in our operationalization and also making sure that we at the same time upscale our co-workers with the tools we provide okay this is the nice animation yeah data queue itself so basically you could have seen it before in this in this triangle but basically we connect via data queue to our central data lake or like the consumer the co-worker in the end a business unit is going to do it taking data transforming data and then also being able to put data back into the data lake for others to enjoy others to use of course this is all within like strict governance rules so it's not just you cannot just like access everything you know it's there's a rare stringent set of rules as with any financial institution but at the same time like I said so you can get data from the data lake give it back but you can also transform data to internal applications so for example you have certain data sets within your lake but also there's another group that has something interesting and you want to test it out so what do you do via colibra you shop the rights to this other group then you import everything in data queue you connect it and then you could use tablo to visualize the new information you have if that's something you would want to do so basically giving you like certain set of tools and to be able to within guidelines drive insights out of them so I think now the most interesting part is coming up because I think we are very different to to many other data queue customers at least that's what we always hear from our yeah a queue colleagues and also it does take some time for us I mean as a bank and also as a very cautious bank to actually implement things so maybe you guys remember the timeline it's been like two years already I mean we do have use cases on the platform so please know very about that it did take us some time to actually like talk everything through make sure that all the stakeholders are involved and also accept what we're doing but I think we came up with something really cool because instead of just having an instance for certain teams we actually have the whole thing automated so basically if I as a business analyst I want to use data queue because for example Excel is just not fast enough anymore I mean we all know it right these huge Excel files a lot of macOS while we ate calculations in there I mean at one point they they're just not going to cut it anymore you know but I mean with Excel you just open it and you do something right with data queue like you install an instance and then you do something but not within a bank you know like all the data you have to transport it somehow there so we built data queue in a way that within one day we can supply you with a project within a certain scope that also has access to all of the productive data that you are allowed to access so how does this work so basically as a data analyst you would start or as a business owner in general you would start in our internal software shop so the first thing is like you have to decide what do I want to do what I want to achieve do I want to just experiment then you would choose a sandbox so basically you have get a data queue project where you can test things out you can connect to your productive data but you cannot operational less anything and it's also time limited so it's just for you to try you use case at the other option or the other type we currently offer is a productive environment basically I mean if you know data queue you have design notes automation notes so that's basically what it is you have a design note you also have an automation note next to your other so you can actually set things live you can build scenarios you can run things and of course you can also transform from a sandbox to a productive life environment these are two types of projects we have currently we have more underworks for more advanced use cases but it's just like a simplification for you to show how it works so like we're saying what do you do you basically start by going to the software shop you enter a few details in the form your cost center number what do you want your project name to be and also who do you want to have access to your project so what happens once you audit it goes directly to your cost center so like someone approves it and once that happens business roles are automatically created so every data queue project has a set of certain business roles assigned to it that are automatically built that give you different access levels within data queue so some give you full access some give you view access some give you only access to the underlying data basically making it secure yeah once this is done we have provisioning scripts who are creating the data queue project creating all the background connections to gcp creating all the file systems and everything and once that's done like the three users you put in your auto form get assigned to business role and then you can start working so ideally of course depending on how fast your your boss is gonna yeah I mean my colleagues are laughing but basically ideally depending on how fast your boss is gonna say okay the cost is fine I mean I'm just gonna do that you can scale this up within 24 hours so within 24 hours you have a data queue instance where you can work with your own data in a secure manner which is pretty cool at least from from my understanding after having done the two years yeah anyhow of course in the online shop you can do much more you can also order certain Python environments with a certain version or whatever version you want you can order our environments you can order integrations so making sure that your data is not only delivered back to the data lake but also delivered to other applications so it's all basically made very simple for our analysts to be able to to use data queue most efficiently and also very quickly yeah we've been raking on this for I think the better half of 2021 we just tested everything out for the better half of 2022 we basically implemented this whole process talk to all the stakeholders and at the end of 2022 we first open it up and now we already have quite a lot of traffic on it very different departments of course yeah some ESG reporting some regulatory law capital markets so it's not like we we targeted anyone specific basically these are early adopters these are people who are a very tech savvy who already pushed years ago they want some tools to to do things that go well above like what excel and anything else we had to offer was capable of and they started basically with the first use cases we also using it in our innovation lab so of course we're not just stopping there I mean we love that everything is running right now but we're not going to take a breather gonna work hard and we do have some things we want to achieve within the foreseeable future I would say so the first thing is of course I was just mentioning most of the use cases we have there by rather technologically advanced business analysts so we want to build up a community there but also these use cases and other other colleagues basically seeing the impact they can have less technologically advanced business analysts also asking so how am I able to use this so how am I able to to also get this working so basically what we're going to do soon is start internal training sessions build that up another thing would be that we are trying to implement data queue into more established processes so making it a strategic tool so also kind of I guess forcing people to use it more and then also making a lot of like a gen i use cases so advanced analytic use cases available to through a data queue so basically when you remember like at the moment we were like diverging so we have like two streams going on this is a yeah this is us trying basically getting these streams back together because in the end the tools we provide they're not supposed to only be for a certain kind of developer or employee within the bank they're actually supposed to be for everyone so even coders are encouraged of course to use the platform in the future and last but not least we also want to add new projects so we were saying like we only have a sandbox and a productive environment we want to scale this up to be able to use data queue to cater towards high availability applications so send the data there and also find a way how IT can take over data queue project that might be too complex for business unit or might be just too important for them to just be running by them alone solely I think that's pretty much it I think we have a few more questions so or we have a few more minutes for questions so if anyone of you would like to ask anything then