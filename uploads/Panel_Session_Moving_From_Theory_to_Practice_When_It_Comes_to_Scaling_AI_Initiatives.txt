 All right. As folks are moseying back over, we're going to move into a couple more conversations. Now the next piece here I'm actually really, really excited about because this is where we're going to start talking about and hopefully answering some of those questions that may have cropped up during Kyle's last presentation about what they've accomplished because data science is science, right? It's R&D, it's statistics, but science doesn't tend to sort of like make business stuff happen. And so really what we want to talk about here is really moving from the theory side of things, come up with a cool idea, maybe build a model, how do you actually push that into practice? How do you make real business value come out of the things that you're building? And so let me welcome on stage here my panelists, first harsh for from Crowley, lead data scientist there. Yeah. Aaron McClendon, the lead data science or the head of data science at Aimpoint Digital, one of our partners. And finally, Ritual Bantia, who is the head of risk and innovation at Marco Financial. Coming up and have a seat guys. All right. So I'm really excited for this conversation. We've had a lot of fun sort of prep condos as we were getting ready for this and a lot of a lot of different sort of perspective on some of the challenges that we're trying to solve and how do you actually go about really solving them? So the first part and sometimes almost the hardest part is actually figuring out what to work on. Like what are the real business problems to solve and like where do you want to put your time and energy and your focus? So to kick that off, Ritual, how do you guys at Marco think about identifying the business problems and deciding what are you going to work on? Yeah. Sure. Hello, Miami. Great to be here. And what I'll start off with is not just from Marco, but I've been in the FinTech space for almost seven, eight years now trying to solve problems where it comes to risks when it comes to product growth. So I think the first thing is what problems do you want to solve? And this could be about in a Marco context because it's a very, very early stage start up. It's more about what products you want to launch, what products, your customers, the potential customers care about. So it's very much on that growth journey versus an into it where you already have big products taking on a lot of many movement risks such as fraud, credit, because it's into payments, it's into lending. So there the business problem is how do you manage and mitigate your risk in a way so that you can manage and contain your portfolio? And how do you keep optimizing your models? So it's you know, I've seen both the spectrums of problems. The interesting thing is it's after the problems are identified, it's about the mindset, whether you want old known solutions that have worked in the past, or are you open to something new? Are you open to be challenged by what you already know? Or are you open to looking at data and then coming up with solutions which you probably haven't thought about but are open to implementing because it's going to have better business outcomes for you. So it's a lot of back in for there's no clear answer but again, it starts with very clear focus on the problem because ultimately the time, the cost of your data scientists of platforms, of technologies, you need to keep in mind the ROI of how this is going to be useful because it's a finite resource and you want to make that resource, you know, the best use of that resource. So it starts with that having an idea or a framework to calculate the ROI and then being open to, you know, coming up with solutions which are more and more data driven, AI driven. So with how you guys do it at Crowley harsh? Yeah, absolutely. So yeah, definitely I think first thing when we're evaluating a machine learning project, we have to look at the ROI. First is, is it going to generate value? Is it going to generate enough value to put a bunch of data scientists at least one on it? And feasibility comes next, I think, of course, whether the data is there, whether we have the tools to solve this problem, all of that is secondary but the first thing is will we realize enough value from this project if we execute it correctly? So I mean, not to make it sound overly simple right because I think calculating ROI and feasibility definitely takes some time to get into. But one of the key things that we sometimes get wrong when we try to ask these ROI questions or feasibility is about the data, right? So how do we know what data we have? How do we have the knowledge around the data to be able to use it? Do we know? Do we think we have the data to solve that problem? So what does that look like as you guys are sort of you said, okay, we think this is the ROI, we think this is the feasibility, how does that data, the knowledge around the data, the data that you have available, how does that play into this actually being able to identify and solve those problems? I think the way we kind of started isn't from looking across holistically at everything that's there initially but usually we'll start by kind of talking to the key stakeholders. So the people that are going to be actually using the solution and trying to understand from their perspective which things are the biggest drivers of value from the KPIs as we define them. Because I think AI in general, it's not always going to be able to beat someone who's been in, for example, a manufacturing floor for 20 years. And so just sitting down and kind of talking with them and understanding which things are the biggest levers, what are the problems they've identified and then taking an inventory from there, kind of working backwards from that and seeing for these key factors which data sources currently exist and which ones are accessible. I actually really curious about the microrate so you're at a startup right now and some of the times the biggest challenge in a startup is not having much data versus when you worked it into it where I guess maybe the other end of the spectrum of where you have more data than you necessarily know what to do with and not all data is created equal, right? So tell me a little bit about that sort of like especially in the data poor side of things so to speak. What does that look like in terms of where do you find the data, how do you assess its value, is it going to be good, what does that look like? So I think in case of Marco where you don't have done of data, so how do you go about taking decisions about launching products or what will get customer traction? You think about ultimately getting customer feedback and whether it's through conversations with potential customers, whether you want to do like landing pages where somebody puts in shares interest that you want to do a wallet or you want to do a credit card or you want to do like any other product or FX conversion because art, target segment is exporters. So you always have to understand like what problems do they have and that starts with deep conversations. It's about falling in love with their problems because it's not about imposing solutions so much. It's about understanding the mind space that these counterparts go through and then coming up and coming back to the drawing board and then figuring out okay this may be as a digital wallet for example as a product makes a lot of sense for these businesses. Then now how do I go about building it, what data I need, what partnerships I need, what kind of intelligence I would need to be able to launch and scale this. So that's a different data conversation versus like an into it where you know that you have like your accounting data, you probably have a lot of bank feed data, you have you know your login data, you have your performance data. There it's more about making sense of out of the thousand features available which are the top 20 or 30 that would matter to you and that's where a lot more deeper modeling is needed but then modeling is so much dependent on data preparation that you need unless you have like data IQ it becomes a huge challenge to be able to get that but then I also see the use case of data IQ being used for problems such as which are not so much about just data ingestion but more about like now you've started to you if you've adopted this mentality that you want to be data driven. Now it's about like let's optimize and let's start off. So I could see a use case at both the spectrums. What's you guys right like how much of your data is it a problem trying to find data you know are you do you have too much data in trying to figure out what is that what does that look like? Yeah so though I would say the whole spectrum essentially we have projects that are very data rich we get data getting data every 15 seconds and it's everything's good but we also have projects where we it's data poor as you said right like we don't have enough data to make the decision we want to but I think I think what we're trying to do is also we can start the journey of making it a data rich project right we can start the process of building that data mark that we eventually will need for a machine learning model. It takes some time to get there it takes some a few months but we as data scientists can also work with the data engineers with the business to start building that mark shaping it the way we would want so that we can make that machine learning model at the analytical model at the end. And so I might to dig in a little more on that so you just talked about so you're you've got the data scientists you've got your data engineers talking to the business Marco mentioned or sorry for a tool mentioned a little bit earlier talking to customers there. So what does that look like from an organization perspective right so you're talking to customers you're talking to internal business stakeholders you got data engineers said data science how do you guys organize yourselves to work on this work on these problems. Yeah great question so at Crowley we have a center of excellence for data science which is an I'm part of that center of excellence we and we we are we work on data science problems along with governance and democratization of some of the analytic tools but at the same time like the data and and the processes are owned by the business so they have much better understanding of the data itself. So if we want to develop any data science model if we want to develop any statistical model if we want to get it right which is the only thing that drives value right if you want to get it right we have to work with them we have to understand what those what that data means. So no model can be made successful in isolation without the business without working with the business. Yeah and Aaron obviously as a consultant sitting off the side you work with a lot of different companies organized in different ways. What is that as you're coming in to work on a project to support one of your customers or what is that what do you look like for how do you organize and what do you see being successful especially for the customers that you're working with in terms of how they're organized and the people working on these projects. Yes it's interesting I like you said you know going across a bunch of companies we see quite a spectrum of AI maturity you know going back to the data question you know we see data poor organizations with a couple of data scientists and we see I was telling them about a use case backstage where we had data points every you know a couple hundred milliseconds coming through so so like very data rich organizations with very mature IT teams and you know what works best for the organization really just kind of depends on what their long-term goals are for data right I mean there's a variety of different ways strategically you can set up an analytics department you know the COE type of approach the kind of hub and spoke approach more of sort of a mixture between the two and there's a lot of different models that can be implemented and I think it just sort of depends on one going back to that hub and spoke models is sort of the maturity of the business lines you know so are they mature enough to manage a data scientist internally as is the is the organization at that level of maturity or is it more of the case of you need a more experienced data scientist managing a central sort of team of people that can service other business lines like going back to the the COE model so you know it just really is a case specific when you're coming in do you change the approach of your team that you're bringing to a customer based on how they're organized yeah definitely um I think one one thing we see especially in more data poor organizations um is a high prioritization on enablement and training um and oftentimes when we come in in those situations like it's not useful if we build some complex model and then sort of throw it over the fence and you know say see you later right because then if they don't have the maturity to manage that internally uh it's just going to kind of die on the vine with all the other science experiments um and so you know it's it's it's it's it's interesting kind of kind of seeing that seeing that difference you know as you as you go from from place to place that makes sense and especially from that like the enablement perspective right like parachuting in and just dropping a model and leaving typically very expensive and not very fruitful experience for your customers right um but you know so so thinking about where you guys are at where you're trying to build most of this stuff yourselves um like how how so you so you're small your startup so the complexities of a lot of people aren't necessarily the the challenges you guys are at so it takes a lot more than people to do it so how are you guys thinking about then like the technology side and making decisions around like what technology especially again big companies small companies are you making different technology decisions based on the size of the company the size of the team what does that look like yeah for us right now uh the biggest piece is intelligence about the business because when you are financing a particular trade finance like transaction you need to be comfortable with the with the seller because you know the identity the past track record of the seller and there are ways for you to get that data like whether it's government databases whether it's uh uh any tax information whether it's past transactional activity right those kind of data pieces you need and then it's about the buyer because the our creditworthiness basically falls on the buyer because the other ones who are repaying you which is Marco right so for us there we go a lot more deeper into uh like creditworthiness what is the existing credit lines what is uh would they be able to repay the obligation right and the third piece is the relationship between the buyer and seller because that's the third piece of intelligence now from a data point of view if you were to do all of this which has been happening uh you know since we started because again we are experimenting we are building but you could be tackling all these decisions or concluding all of these risks in a very very manual way or you could be building a model that takes into account all of these different parameters of risk gives you a score of a deal that helps you evaluate okay whether it's a go-no-go or whether you want to because that's a goal you don't want to be spending time manually reviewing these credit evaluations because you can't be able you won't be able to scale as an organization so if you want to do more and more deals you have to be data driven you have to adopt you have to build trust with these models so that's just one example but that's a starting point similarly other decisions you take there are ways to do it in in ways uh which are probably very uh I would say inefficient but then you have to figure out where are the possibilities of efficiencies and start your journey there and at at the end of the day even if you are it and into it every model has its journey you have to start somewhere so the philosophy remains the same and so you talked about that sort of like that how do you start to optimize and scale right so how do you think about then the tools that you're choosing and using and developing in terms of not making a decision once not making a decision twice and more the point not becoming the bottleneck for those decisions so that your business stakeholders are able to do it so like how are you thinking about like are you building applications for them are you integrating into the stuff that they're doing because I think it's a really key part of scaling is you can create a thousand models but if you can't if the business folks can't interact with them it doesn't really matter how you scaling like how do you think about that that like last mile in the end user piece yeah I think the biggest piece is building trust in your model and that happens by performance when it comes to things that we are doing because it's such high stakes volume of business you're putting in you know tens of million dollars into a business you can't you can't just adopt a model one day and just use it right you have to build trust in the model or any other path or any other parallel process that you've to build you've almost have this parallel process okay this is how you do it today this is how it could be done let's try test out with 10 15 20 transactions and then see the performance of you know how much time you spent in doing it in your current path versus this new and how much more you could have done if if obviously it's not just about you're going to have a perfect model on day one right you are going to learn as well because you there's a lot of human intelligence that goes around into making a decision the idea is how are you able to understand that capture that and improve and iterate your models and I think that's where the choice of the platform comes into the picture because you want to be able to quickly iterate and you want to see the performance you want to compare you want to move on and and that's the way to go about it Harsh I say you're not a good bunch of that so I want to get your thoughts that seem to resonate about the data yeah so I mean it just brought me back to like a few years ago as trying to build a model I spent eight months trying to just improve the accuracy of a model right and that was my objective at that time and think about it now that that was in an academic setting now I think about that feels very different in real life like right now when we talk about data like when you talk about a machine learning model we got to start with like where is your data like is it if you may want to make a decision every second or so like but your data is coming in like monthly that's probably not good right like so where is your data how fast are you getting this data then what is your data like you got to understand that right like you can have for the same use case you could have like commercial data you could have operational data financial data and that kind of drives what features you build and features to drive how good your model is going to be so and I think like that understanding that data is like 80% of the work now like and now go comparing it against spending eight months on the model that's kind of like 20% now yes yeah as you're thinking about again that sort of like that business user who then needs to make those decisions daily monthly weekly whatever that looks like how often are you sort of bringing new technology or new tools into what their their sort of business process looks like or like are you designing and changing those or you're trying to integrate into what they're already doing how do you how do you think about that sort of like you know someone needs I'm built a model somebody needs to make a decision with this model how do I give them the model in a way that helps them do that yeah I was thinking like earlier on the to your previous question around you know recommending a technology and I think that that was something it plays into the one you just asked is around that productionization paradigm and I think you know going from place to place we kind of see a range of ideas of what analytics is and I think one of the things that we always try to get people to do is not just to think the next couple months right but to think the next few years because a lot of times when we come in people aren't thinking about ML ops or productionization paradigms which is that serving insights to a user right they want to know like I just need a dashboard so I can see this chart but you can build dashboard but it's you know we we try to avoid like getting people into a situation where they have this sort of spider web of technologies sort of all you know interacting and conflicting with each other and I think that's you know understanding how to frame productionization or ML ops in terms of you know something that a business user finds value from then can kind of help you know see the light at the end of the tunnel for that you know two three four year horizon or you know what technologies do we really need to bring in to play and how far out do we really need to plan so that you know our organization are in our data organization scales with our actual organization and I like the way that you put that around the ML ops side of things I feel sometimes when people talk about ML ops it ends up being a very sort of narrow perspective of like oh how do I manage this like giant you know trillion parameter model in production so that it moves quickly or that sort of stuff and it's like they're very much thinking about they built a model and they need to make it like run in in real time or you know with high availability or whatever and it's like very technically focused but I think ML ops is a lot broader than that right it's very much how do I push a model out how do I make sure that it's living it's serving the purpose that it needs to do so how do you how do you think about that you know in terms of hey I've built this model it I need to be able to do it over and over again many many times is that something that you're trying to think about especially in early days for you guys are you trying to build the models today with that four-year horizon or I think is sometimes the startup world or just I mean startup world in general but just like data science in general solve this problem for me today solve the next problem for me tomorrow are you how do you how do you take that like four-year perspective on how am I going to build this model today to support not only this model four years from now but the other 100 models I'm going to build it yeah I think the answer is that today what we are building is for our current suite of product or which is a very very narrow suite of products that we offer right so the more at this stage for us it's about finding the next problem the next wave of products the modeling for those products will be very different because the data that you'll have will be very different and and you know so you can't like build the current model for that four-year path because the four-year path maybe the organization will be completely transformed because ultimately what we are solving for is the liquidity of small businesses right liquidity financing is a is a banded way of providing that liquidity but there is a lot more ways like faster efficient payments there's a lot to be done so for us I think it's more about that trajectory and then we'll continue to build models that support those kind of products but but onto your previous question I just want to highlight one thing is about you know how do you get and this is again going back to my intuitive experience of how do you how do you get to continue to build models which are effective and I think that is the most important thing that sometimes we forget is building that understanding of the problem that we are trying to solve because when it comes to managing risks right a data scientist in isolation can build an amazing you know decision model which could take path A path B right we could take certain decisions but then how does it impact the actual user who's gonna be agitated using that model and for example if you're using a model to decline transactions real time if that model goes even slightly wrong you are impacting you know your real customers because you are declining that transaction that's a huge cost of a wrong model so the idea is the end use of how that model is gonna be used I think you need to build that cross functional understanding we always had this challenge of like you know policy analysts on one side who for example came from PayPal they they knew that this is the only solution that could work and then the data scientists in this case were the challenges where I had a simple rule like let's get all together in one place let's understand this particular loss event and figure out what really happened here because losses can be can look like fraudulent but it could be something else sometimes it's first party sometimes it's third party sometimes it's even if it's a credit loss it could be sometimes because of an ability to repay sometimes it could be about the willingness so how detailed can you go to really find that root cause because if you don't understand the problem you will not have the right solution and and and and building that across you know the cross functional teams of not just data and scientists but policy analysts of operations of business people and building one common understanding goes a long way and then the data scientists can know that exactly okay this is what we need to solve for and this is the impact and I think that's very sort of empowering in my experience yeah something I wanted to highlight to your previous question yeah no that it makes it's funny because it comes in some ways brings us back to the beginning right I was like how do we decide the problems to work on it beyond just deciding what problem to work on it's the context around the problem that even if we can solve the analytical problem at the center here understanding the process understanding the context understanding how people will change and make those decisions right and I think that very much highlights a lot of this sort of like how do you scale requires thinking about all these pieces together it's not a technical decision it's not a people decision it's not a problem it's thinking about all these things sort of like in parallel to decide how you're going to work on it so a couple minutes left pivot a little bit to one of my favorite questions to always ask people which is you know you look back over the last couple years one of the biggest failure points that you've had that you've learned the most from or and maybe even better the thing that you screwed up that you're still not quite sure how you screwed it up but you know it went wrong so I'll throw it to you Aaron first I'll give these guys a little bit more time to think about it but what tell me about like a good failure and then I've never failed at anything no okay well we can just move on so harsh now I think you know especially getting started in data science it's going back to kind of harsh as earlier point is like understanding the ROI in am I going to continue focusing on this one single thing to squeak out a few extra percentage points of performance how long am I going to spend doing that for versus kind of thinking more broadly and understanding you know I need to make this something that's usable to the stakeholders I need to make the results understandable right and I think it's sort of I guess not like a singular mistake but maybe like a like just a mindset shift of you know coming from sort of a graduate school education where you're solely focused on a single problem right like that's your whole thesis you're just investigating it you want to solve it to the you know up most of your ability versus okay what's the trade-off between spending too much time doing that versus not enough time coming up with a solid production plan or not enough time understanding the broader context of how it's going to integrate into the organization that makes sense harsh about you yeah so similar I'd say being model centric from my experience is probably not a good thing being data centric is probably a good thing the other thing is maybe being flexible about like technology and method methodologies like I would say that's a learning that I've had in my professional experience but but that being said like I know that some projects sometimes they don't bring expected ROI and things like that but a lot of times it's also there's a lot of intangible learning that happens like you get domain expertise and things like that and there's a lot of transfer ROI generated to other data science projects as it was a lot of that so what's what's saying right it's only a failure if you don't learn from it so but I think that's a good point right like it's data science not everything is going to work out but so knowing when to pull the cord and say nope this project's not going to work or we need to change stack or whatever then how do you learn from it and so then finally we're told I'll leave it to you to bring us home around sort of a failure yeah I think I think I have too many failures right everything I am because of because of Aaron between the two of you you've got to get balanced because because although you know I'm responsible for managing risks but personally I'm a risk taker right so I don't think we grow without our without taking risks and without failing personally I think my failure is a personal one that I didn't start a business of my own and I think hopefully I connect that in the future but but really my passion for for data science grew after into it because of the impacts we could we could build and I think there's so much to be done in this space and I love what you guys are building at the data IQ because I did it solves a lot of the fundamental problems so that we can actually focus on the decisioning and I think that's really the goal and it and I would like to see more and more real life applications of of of of decisioning that is that are truly data driven and that are that are very organically you know you could you could see and see the impact of and continuously evolve so yep all right well really lively conversation thank you very much for for all the pieces you guys brought a lot of different perspectives I think some good nuggets in there so a quick round of applause for my panelists thank you very much guys all right