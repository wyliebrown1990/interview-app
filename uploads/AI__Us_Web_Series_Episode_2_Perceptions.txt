 Terminator Blade Runner 2001er Space Odyssey Three huge movies among many others that have done their best to make AI seem scary as hell. Machines become self-aware and decide to destroy and or enslave all humanity. So you have to wonder, is that how the majority of people perceive this technology? Perhaps it is. But are these extinction level risks what we should really be worrying about? Shouldn't we look more closely at the way AI is currently being used? Ways that are perhaps out of step with our values and expectations? Are the main problems with AI human problems at heart? Before we look into it, here are some facts for you. AI has been with us for decades with the earliest successful AI program being written in 1951. And the concepts for millennia, Homer's Iliad references out of officially intelligent machines all the way back in the 8th century BC. But people are still split about it. When asked, 47% of Americans said they were worried about AI. The world of business certainly isn't though. Its estimated AI could contribute up to 15.7 trillion dollars to the global economy in 2030, which is more than China and India combined. But it already has equality issues. Only 10% of AI researchers at Google are women. We build machines to do the things we can't. We've done it for centuries. AI is no different. But as creators of AI, how do we avoid filling it with all our own faults and failings? How do we ensure what we make is fear, legal and safe? And in doing so, change the perception of AI to the rest of the world. We need to worry about the social justice issues that are hidden behind the AI. A lot of people think that AI with the current technology is able to take tasks independently, completely. Today, AI is good at three things, fundamentally. Prediction, imitation and optimization. The machine learning is more like a way of computers basically trying to understand or learn how to become humans. And I've always been at computers I've seen. There are a lot of researchers who do not like the term AI or artificial intelligence for that very reason. They prefer to say machine learning or they prefer to just say it's just advanced data analytics. I actually think that the majority of the public don't really think about AI that much. If they are thinking about AI, they may associate them with computers or perhaps associate with them with something that's far in the future. People have all these connotations wrapped up around what it is to be intelligent and it often means something that is smarter than you are. The biggest misconception that people have with AI. I was looking at perceptions and what you would have with artificial intelligence is like media articles where they would show like terminate of robots and things like that. There's no majority public opinion. I think there are three key factors that would influence whether it's good or bad. What geography and what country you're coming from. It can be what age demographic you fall into and what socio-economic status you fall in. The issues that are emerging from AI are an amplification of the issues that we're already there and those are the issues that we need to get to grips with. So social justice that's already there. Discrimination that's already there. Big problem with AI is that a lot of AI systems is not all but a lot of AI systems are based on just learning from historical data. You feed it some data. It spits out a prediction. There is the fact that humans are themselves very biased in the way they make decisions and if these systems are learning from past data about human decisions, they tend to just incorporate that bias. Visual Commission technology they found that they actually are not very accurate on when they're tested on a black woman for example. I think we stop the prejudices and the biases from filling in by bringing more people to the table. You know diversity of folks to come in, play a role in the control and in the design of the algorithms of the products of the services that will be introduced to others. And it's something that the public really should be aware of and should be asking hard questions about you know what data did you train the system on. You know most data sets are from the internet, they're generated from internet data and I'd love to say that the internet is a repository of evil. So if you're training the data set on that particularly, then what you are going to generate are necessarily evil results. I think increasingly we're going to see systems that learn in simulation. We might see systems that increasingly make decisions that seem very strange to humans and yet produce very good results. And I think because they're so removed from human decision making, we're going to have an even harder time like understanding why is the AI making this decision. So how can we get an algorithm to explain to a human being how it's come to a final decision on something like that. Interactions where the AI makes it clear as an AI. The next step for AI I really think should be some type of regulation. It's inevitable that machines will have to make moral decisions. The risk is whose values or who is actually controlling what kind of values or what kind of moral decisions they are about to make. And it should be about data and people's rights over their own data. If this is going to have life altering consequences, you have a right to know that yes, an algorithm is going to be used to make this decision. I think the next step for AI is the implementation of the capability of common sense. I think we'll have more driverless cars, right, so more autonomous vehicles on the road. But up for now, computers are stupid. Rather than asking where do you think AI is going to be in 30 years, it's better to say what society do we want in 35 years? And then how can we use AI to get us there? We've talked about the implications of AI for centuries, millennia even, but we're living with it now every day. One of the fears and concerns that people and businesses might have, are they founded in truth? I hosted a roundtable discussion to talk it all through. So there's a lot of discussion hype, perception of the value of AI. Why are businesses hesitant about adopting AI? I mean, I interview a lot of companies about their use of AI, and I have talked to some companies that have been reluctant to use it. And I think part of it is that that issue about where is the value? And I guess they feel like you need this highly specialized skill set, and they are not in a position to hire those people. They're either too expensive or they just feel like I'm not the type of business that's going to be attractive to that. I had a discussion the other day with a potential customer, and they say what they would love, is that once an algorithm is designed, it just looks like any other IT product. It's completely opposite to the hype cycle and the sexiest job and all of the money and interest that actually when it's valuable, it'll be so ordinary that people are just using it to solve problems when it's the right thing to solve the problem. I think that's the great lesson is give people who love solving problems and finding new things to do, new tools that allow them to do that. Exactly. And I think that the people who are not hesitant to do that, correct as an example to the rest. Like one thing we've been observing all the time is that the big brands are very hesitant to adopt it. However, we work a lot with emerging designers, recent graduates. We create the proof of concept. They show to them that it's actually can be implemented and then the big players come and they adopt the technology. There also is like a dichotomy between those companies. I think that our Cersei is primarily a tool that enables them to do what they're already doing at less cost or faster. And those that see it as something that's really strategic, it's going to let them do something that nobody else in the industry does or that they at least their business has never been able to do before. I see that time and time again when businesses are looking where should we apply, where should we first apply AI? Something I also see and sort of personally as a data scientist was when you look out at the whole business, sometimes the parts of the business that you think are a massive opportunity for AI are actually already highly optimized and there's no more gain. And that's a very awkward thing to realize maybe I'm too expensive for the value I'm generating and I think those companies need to think about applying AI creatively to create something completely new. Do you think businesses always need to adopt AI? Is there an imperative to do so or when would a business not? Seven out of 10 businesses responded to eight out of 10 business surveys have said that in more than half of cases they heard from their friend that they should be doing this. So it's something that investors, commentators will be asking them about on earnings calls. Maybe something comes along or replaces it as the thing that people get asked to do. So there is a drum beat out there. If we're moving from extraordinary to ordinary when we're thinking about AI what kind of new normal are we going to be looking at when everybody is using AI? I mean I think it's very context specific and I didn't think you know we should have a blanket regulations I think each sector needs their own but I think there's a lot of work to be done when we think about what will be the unintended impacts of using these technologies for for whatever reason in different businesses. Definitely there needs to be regulations but I think it's very challenging to be able to generalise for all industries and all of use cases. I don't think you should I think you should be it should be sector specific. The health sector are generating their own regulation, fashion industry can generate their own regulations. I think regulations can definitely be contact and should be. Not even as to contests specifically. Definitely. Definitely. I agree. Businesses that I speak to say that they can't get the talent that they need to reap the full value from AI. Meanwhile myself as a practitioner and lots of others that I know know people with skills who can't seem to get jobs and so all this program of work that we've set out, all these problems that we'd love everyone else to solve. Who's going to do that work? What skills do they need and where are we going to find them and develop them from? I think the question lies in trying to educate a younger generation of people who are going to be able to have transferable skills which doesn't mean that they're not going to be specified in a particular industry however when I was a student you couldn't create a website if you didn't know how to code like look at how it is now it's been democratised then anyone can create it. It doesn't mean that everyone can produce like impeccable website but still you have the power to it because I think that these people are going to be extremely valuable assets for brands and companies. It doesn't mean that they will be the ones who are going to develop and develop the tech but they can be the best consultants actually to know how to make the right decisions so I think it's really important to try and educate younger generations with multidisciplinary skills. The students that are coming through now kind of want that there's a desire to learn all these different skills and it's not just so much from we need to give say fashion students digital skills but we need to give data scientists ethic skills social science skills and skills to interact with fashion designers or whoever so I think this multidisciplinary perspective and teaching students not particularly particular one discipline but how to think and engage with a whole variety of disciplines and the skills to do that is I think we'll be invaluable when we think about AI. So it seems the problems with AI will always be human problems our biases our prejudices can we find ways of developing AI without infusing it with all our flaws and as for the barriers to entry AI technology is becoming easier to understand and much simpler to use it's undeniable so do we have anything to be afraid of? I would say only what we fear in ourselves