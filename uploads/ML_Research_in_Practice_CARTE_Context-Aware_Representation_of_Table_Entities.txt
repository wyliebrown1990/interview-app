 So hi everybody, thanks for joining us for a new edition of the ML Research and Practice seminar. Today we're very happy to have you from Inner Hia, who's being a postdoc, just like our very own Ricardo, who's going to talk about, as you can see, context where a representation of table entities, and I don't want to tease it too much, but let's say it's about trying to apply the fancy recipe of transformer that has much success in MLP to something that we've very much know and walk a lot with here at DaDekko, which are table-a-representation and table-a-data and see how can we steal some other idea from MLP and leveraging the context of tables to get interesting and detail-inviting. I hope that's a decent introduction to the topic and with that, for Verju, I'll take it away. Thank you so much for the introduction. That's pretty much explains a lot about the basic outline and I'm very happy to be here to be presenting. I've been so into DaDekko for a scrub sprint and it was a really nice experience. Today I'm going to talk about the research that I've been doing as a postdoc. It's been about 10 months now and I guess the results or the works are still in somewhat printed winter stage, so we're still developing more and I guess there's no completeness in the research and we're going to keep developing it and proving it until whenever. Okay, so my name is Jun. I'm from the Soda team in Imea and I work with and today I'll be presenting on the context of representation of the table entities. So yeah, we'll be outlined in a piece of format, introduction, proposed method, experiments, and conclusion. To start, well let's say that I know that I'm not in the industry, I'm mostly in the research departments, but I know that a lot of industries, a lot of companies are trying to use data for automation, diverse applications, better forecasting or efficiencies. And I guess from a lot of data, a major part would be the table tabular data. While I have knowledge graphs here, just to get people to get people more familiar with it, but usually a lot of a lot of industries actually deal with like tabular data. I mean, other places deal with like images or like languages, but I know that tables are used in a lot of places. And so we have a lot of tables and usually with the tables, the comments, machine learning method that we apply are either grading boosting on the left. So it's like a grading boosting trees, where it's a ensemble method, where you have a lot of trees that is well known to perform very well with the tabular data. On the other hand, with the success with like the CNNs or I guess the transformers, people are applying deep neural networks to tabular data. So there's actually a PhD students at NMEA who wrote a good paper, a benchmark paper comparing I guess why grade boosting trees perform better, and it's giving a lot of citations. It's very good paper. And I guess still the internet verse is in somewhat. It's not in that top range, I guess, when we're dealing with the tables. So we have grading boosting as like what we want to really want to beat in the end. But another perspective, I guess we have the importance of the pre-chain model. So we have a lot of data and what we can use with it is to actually have a pre-chain model that's been known to be successful for language models like birds starting from birds. There's like a lot of things. And also for the images, there's a convolution in your your your the earth. I know that there's like a unit that's very used for fine tuning from the pre-chain model. So like for language models or images, the pre-chain model has had been very successful. And I guess we have a lot of tables out there in the world. But to make the full use of it, we might be better by having a pre-chain model to improve some performance for the downstream tasks. So I guess the major goal for the pre-chain model would be to have like the downstream improvements in downstream tasks. And possibly extensions to transfer learning or much more. Yeah, so I guess that's the end goal with it. Now, and I guess there's been some works that tries to do some pre-chain models. The one at the top is called Charlotte. It's what the title says, a deep learning approach to approach to making to semantic data type detection. The one below is the turtle. I think record also for the people. Yeah, they have been trained on a large set of the tables. But usually they're they're focused on the evaluations of performance is actually finding some characteristics of the table on my column annotations or entity link predictions. So it's not really on the downstream tasks where we see like a new set of table. So we see their experiments. It's usually on like the tables they left out to test that's like column annotations or entity linkings, things like that. But it's not really on like the new set of tables where we want predictions through the machine-related methods. So I guess the focus for this work would be on improving the downstream performance of like what we could see on the benchmark data sets. It's making it easier. Another way of kind of embedding knowledge is or having a pre-chained and vectors or embeddings of the entities is from the knowledge graphs. So the knowledge graph, I'll explain in later in details a bit more, but collection of knowledgees and they're represented as a graph. And what it does is they embed the entities and the relations. And since we know the embeddings of entities, one since we could have the embeddings of pairs and embeddings of soil like capital Korea extract it from the big knowledge graph. So we can kind of put that into the tables. It's kind of like have a concatenation or some form of integration. And it's kind of like a supplementary information. So you do like joins by matching the entity names and things like that. So that's the other way. And one of the PhD graduate alumni here actually did a good work with embedding the knowledge, which also extends towards using the numerical values. So so far there has been works that describes the tables with the pre-chained model. In the other hand, there's a knowledge embeddings which can be used for the downstream text. But it's well there are a few problems actually when we think about making a pre-chained model for the tables. So I guess the biggest problem, there are four things that I mentioned, but there's the out of vocabulary. So if you see a table that does not contain the name entity name within the knowledge graph. So for instance, we have New York Times, the NY Times and the downstream tasks. But in the knowledge graph, it has the D New York Times. Then we have to manually match that. So like if we work with the out of the capillary, it's kind of hard to extract the embeddings from the knowledge graph and then directly using on the downstream. So it's one of the problems. And in some places we have different tables specification. Here it's easy said. We have different columns for every tables. And so we have a lot of data with knowledge in tables. But how do we actually combine them? That's another problem. And I guess that brings to like the full use of the tables where so we have different column names, different entities. So how do we actually kind of map them into the same space where we can kind of do some magical stuff? So I guess those kind of are like the easy problems that we can kind of look at. And well, one of the solutions that go to details in later is that we can actually use graphs. So through our PhD, I was focused on making graph-based models, not in the context of tables, but just like labor qualifications or graph human networks. But here we're going to use graph in a different sense. So we're going to map. So knowledge graph, it's essentially a graph. It's inherently graph, so it can strictly use it. But for tables, we're going to have some tricks that we can transform the tables into the graphs. So we map the entities into the graph space, that we could say. So we kind of have like a same feature space. And additionally, we're going to use the language models. So it handles both the out of vocabulary and mapping into the same space. What I mean by the same space is that since, let's say we use BERT to extract the features for the entities that we just injected in BERT. And then we'll kind of have like a stimulus space of vectors where we can kind of have the same level of the same level of the vector space for the entities. And I guess for graph human networks, we're going to use later. We need some form of features. Well, it depends, but it depends on the problems, but for us, we're going to use it. And it's not outlined here, but we're going to use vast text for the simplicity. Okay, so these two are the solutions. And one other thing that I'm going to use is graph human networks to briefly explain the basic ideas to, when we have a graph, we have, we use your networks to propagate information. So we propagate information from, let's say in the picture, we have node i and we aggregate information from the neighboring ones and then we send it to the neural network to other states. And we have the four pass backward pass. So it's a neural network guided by the graphs. So the summary of the proposed method is that we have tables. We're going to make some form of graphs and then we're going to use graph human networks. And we're also going to use transformers. We'll see it later and we'll have some form of contrastive learning framework to make, make a protein model where we can actually use it as the downstream tasks. So let's go into more details. Graphs people might know pretty well. Graphs is just representation of a data set. A data set traditionally, it's a representation of the data sets where each of the nodes represent the data points and the relations are defined by the similarity of the data points. This notion has been used quite a lot like in the graph-based label propagations, they transform the tables into graphs by using some form of similarity checks. So that's like the conventional way of looking at it. So each of the nodes do know the data points. So I guess if you translate the tables, each of the rows or each of the instance is the nodes and their relations are defined by kind of having like a cosine similarity of the features between the data points and things like that. But that's the traditional or the conventional way of seeing graphs on the tables. But what we propose here is kind of a different view. It's yeah, it's very different. When I first came here and looked at it, I was kind of confused, but I got hang of it. So let's see. So what we have on the left is a table and what we want to extract is a graph flip of each of the rows. So let's say we're interested in the entity one. Then we kind of form a graph flip to the right. So we have in the middle a blank one, a blank kind of a blank node. It's kind of considered as the CLS token in the birth model or the language model. So it's kind of like a summary. They can see when we aggregate information from the neighboring ones. So we have it as a blank so that it aggregates information from nothing. So first we have like the entity name. So we have int1. So and we have the columns as the edges. So column one could be one of the edge, column two can be another edge. And it's corresponding values in the table. So in the cell, the v1 sub 1 would be like the value of the leaf node that corresponds to the edge column one. So this is like a new way of like the proposed way of viewing each of the rows as a graph flip. So it's kind of different from the formation of the graphs. Well, they have done before. So we have here if we have a K number of entities or K number of rows and we'll have, we'll end up with K, different graphlets. So yeah, so on the left we have tables. We translate the two graphlets for each of the rows. Now and then to initiate the features, we use language models. And I said before we simply use the fast text. So for instance, if you have the name pairs as a name, then we're just going to use pairs, which could put pairs in the fast text and use the embedding for that. So we'll have some dimension defined by the fast text model. And so we have features for each of the nodes and features for each of the edges and for the entity in the middle it should be blank at first. But in the end, it will have some features later. So let's see basic idea of graphless construction or the graphless that we'll use. I guess for the pre-trained and the dot streams. Another thing that we use is the pre-trained model with the knowledge graphs. So we're in the cylinder-preliminage state. So we restrict ourselves of the pre-trained data with the knowledge graphs. We do have plans to extend it for the tables. But now we stick to the knowledge graphs for the pre-trained model. So now what we do is that so a knowledge graph is just a collection of facts defined by a triplet. So we have subject, predicate, objective, predicate is like the relations, subject is the heads, for instance like molyleza or actually the Finchi painted molyleza. So it's subject will be the Finchi and the object will be molyleza. There could be like bi-directional relations or like just one direction relations, things like that. So basically a knowledge graph is huge or depending on the size. It's a collection of the triplets. So triples with a head relation tail. We use, it's called Yahoo, yet another great ontology. It's a large, a general purpose knowledge base knowledge graph. It has been created from the Wikipedia's with like the word that a geo name. It contains, well we used a Yahoo 3. There's a more recent version of Yahoo 4. But what we use is that it contains over over 2.8 million entities described by a some point two million triplets. So some point two million relations. But for extent of to Yahoo 4, we'll have I think 10 type of entities and 10 type more, even bigger relations. So that's the next challenge that we'll get a face. But for now we focus on Yahoo 3. How do we extract the graph that, well first we select an entity. For instance, if we have like a graph, a knowledge graph on the left, and we are interested in molyleza. Then also we define the number of hops that we are interested in. So here if let's say I'm interested in the number of hops one, then I'm just going to extract molyleza that been chit and the loop as a sub graph from the big knowledge graph. I'm just going to use that as the input for the pre-trained model. So I guess to extract a sub graph, I first select the central entity, extract its sub graph by a user specified number of hops, and then extract the initial features from the past text as I explained for. So that I organize everything that I can use as an input for the channels. So the graph, human earth, I'll briefly explain. As I said before, it's the aggregation of the information on the nodes of the neighbors through the nodes. And then it has been sent to another space by the neural networks and the way it has been shown to perform well for like citation networks and things like that. And I guess there, well there's a lot more variants now, but the two basic ones were the graph convolution networks, and by kit I think from Amsterdam. Yeah, it's just, it does the aggregation over the one hops and it's the, so they don't actually have the waiting vector on its, the waiting vector. So the amount they aggregate from the neighbors is defined by the number of neighbors for the graph convolution networks, but for graph attention networks, they kind of have a parameter, learnable parameter, where they implicitly have like a adjacency matrix, well they form the adjacency matrix, and then they learn through that propagation. So we have those two are the major ones, and I'm going to be using, well, like I kind of use the attentional mechanism of the second one. So this is the transformers that have the basic form at all. So from like 2017, it's, it became a hot topic for the language models, even for the images now, and I guess a lot is used in the tables as well. I could, I've read a lot of papers that trying to use transformers. For me, it's basically to have the pre-trained model. So how do we do it? So in the pre-trained model, I guess one of the important part is how we define the attention, attentional heads. So this on the right, on the right of the figure, we have the attentional head. So what they use is the scale.product attention. So there are two major types. One is the additive, and the other is the scale. I know that there are a lot of periods, but two are mostly used, and the work that I mentioned before, graph attention network actually uses the additive, additive, attentional model, while I'll be using the scale.product, same to the transformers. So inside of the attentional block, we have the same attentional block, but the thing is that there's the optional mask. So the optional mask can, so for instance, if you have like the sequence of words, I am eating an apple. So the mask is optional where I could be focusing more on something else, instead of just like M or something. So it kind of masks which, so without the attention, I should be focusing attention to all the words in the sequence. Well, if you have an optional mask, then it's kind of having I focusing on some of the others and excluding some of the others. For the graphs, so their masks are, I guess, inherently defined. So we have graphlets, and since the graph, the attention are defined by the edges. So we have the so we have the optional mask. It's not the N by N matrix, but it's more of a sparse N by N matrix defined by the graph that we construct. So we're going to just use the structure of the constructed graphlets inside of the transformer. So the basic architecture is that we have the transformers, and we also have the structure of the masks that we're going to use for the grass, when we inject the grass. So that already has like a defined edges that we want to pay focus attention to. But then we're also going to use the edge information. So here, for this top, I'm not going to put too much details into mathematical formulas. I'm just going to briefly give like a high level outline of what we used. So here we do aggregate information of the node and edges. So we have both features for the edges and the features for the nodes. And I guess if you're kind of aggregating information from the neighbors, I guess, well, the first graph you want to or stay only focused on the information from the nodes. So they didn't use the edge information, but more recently, they've been trying to use the edge information. And for us as well, we're going to also incorporate the edge information. Here, I denoted this little dot. So that instead of doing other things, I'm just going to, when we aggregate information, I'm just going to use the dot product of the node information with the edge information. Since they've been all been met to the first X of settings, they'll have all the same dimension. So when we aggregate information, when we're calculating the tensions and things like that, we're just going to use the the combined information of the nodes and edges by the dot product or the L on Y's product, sorry, L on Y's product of the features. So it kind of simplifies the computations or computational complexities. And the other part is that we are going to use the contrastive learning framework for the lost terms or the lost part and the bad propagation. The contrastive learning is to actually when we're given a set of data, we want to separate the positives and the negatives. So we, well, there's a lot of different contrastive learning frameworks, but for us, we're just going to simply have the actual positive and the negatives and then we're going to separate the positives. So we're going to have positives to be closer to the actual positive and the negatives to the closer to the negatives through the bad propagation. And how do we actually do it? Well, we have first have to define what are the positives and what are the negatives. So given a sample graph, so let's say we did an extraction of Mona Lisa and we have like the looves and things like that on the the leaf notes, like the the tails, you could say, and the positive is simply just the truncation of some of the notes. So for instance, in this figure, we kind of truncate the column 2, column 4, and that is just simply another positive. And for the negative, it's going to be a little difference. There's we consider many stuffs, but I guess we also consider like replacing some of the notes with like other values, but we stop to a simple framework where we consider other graphics that we extract from the batch. So when we put put the batch inside of the training, we consider all the other entities as the negatives. So I guess we calculate the cost function for all the entities and for each of the entity, we consider all the other entity as the negative. So that's how we calculate the loss terms. And so there are a lot of loss terms. I think there's like all the twins and things like that for the loss terms for contrast of learning, but we stop to the ones that use the positive and the negatives with the influenza. We said that let's just stick to the simple things. I don't know, maybe later we could try out different loss terms, but for now, so that's stay simple. So the basic architecture is that well, first we have the MLP. So we have our graphlets and that we send the features of the nodes and edges to these shape things. Now we have the attentional block. So the attentional block, I said the nodes and edges here, but we usually update the features for the nodes using both nodes and edges with the attentional mechanism. Now for the edges, since I don't employ the attentional mechanism for the edges, so for now, we only update the nodes. I know that there are other words that also updates the edges, but it's pretty complicated and there's also computational complexities bringing into it. So we're just going to stick to the framework where we use the attentional block to update the nodes. And we update the edges over the layers through just the simple MLP. It's the rest that structure, so it has residuals in it as well. So I didn't specify the architecture here, but we use the same number of layers and number of attentions as births. So we have 12 number of layers, and we also have 12 number of multi-attentions. So I guess the thing from the transformers is that we use the form of attentional block that is defined by the graphs, and it's a little bit different in the attentional heads. Well, I guess the formation of the attentions. And lastly, we have the aggregation. So in the aggregation layer, it's just same as the attentional block, except that I don't update the edges, since I'm not going to use the features for the edges where we're going to be contrasted so what we're interested in the entities, not the relations. So what we're contrasting is the features that we extract through the aggregate information that has been updated by the layers of the attentions. We aggregate the last layer on the center node where the aggregate information and the center node defines the feature for that entity. And then from the batch, we use the features and use it on the contrast of the Ernie framework, and we just back propagate. So, it's a bit... I'm sorry. Yep, could you explain one more time? I'm not sure I'd get the labeling of positive and negative. Okay, so maybe I'm missing something. So for the positive and the negatives, so we have... So for instance, we have the positive one of the central entity, let's say, PERS. And then its leaf nodes could be like, it's the capital of France, it has population something. There could be many information within the knowledge graph, well within the extracted subgraph. But then for the positives, we truncate some of the leaf information. So we kind of delete the information about the population. So that one of the positives would be having PERS is capital of France, but does not have the information, like, has population something like that. So it's just the truncation of the extracted subgraph from the big knowledge graph. And I guess within the batch, we define, let's say we have like 64, size of 64 batches, then we extract 64 subgraphs for each of the entity. And then we make a positive for each of the 64 by doing some truncation. So within the batch, we have the one original, one truncated, and I guess we have 128 as a total of the batch size. So for each of the entity, it contains one positive and 126 negatives. If we consider all the other entities and its other positives as the negatives. So does that make sense? Yeah, a little bit. Thank you. So we define the positives and the negatives like that. And we just put it into the contrast of learning loss. We backpropagate. And I guess the specifications are similar to the BERT model. The original BERT where it uses the cosine warmobs. It does 100,000 steps, I think, for the backpropagation. So it doesn't actually use the concept of epoch, but just does the 100,000 steps. So we also do that. And I think it took about, there has been some interruptions, but it took about three to four days to train. And so we have the pre-chain weights from the pre-chain model. We could actually use for the later use. So one thing that I want to note is that's what we're actually learning from the pre-chain model. So here we said that we learned the attentional parts. And I guess if you translate that to the tables, it actually learns how to aggregate information. And so it's like we have, it learns how to aggregate information by the context. So if we give a graph, it calculates the attention part by itself within the model. So for each of the entity will calculate different attentional values for every different graph. So the attentional values are actually the amount of values that you actually want to focus on aggregating information on. So when we're given the table, so let's say we extract entity one, a graph of entity one, and we put it into the pre-chain model, then in the end the attentional part is, well, the pre-chain part is how much it wants to separate from the others. But the concept is that it wants to, it learns how to utilize the context within the table of like how much of the column value, how much of the values on the corresponding column we want to aggregate. So for each of the entities, it learns how much to aggregate from the column from the values. So I guess in linear regression, the better coefficients, the beta coefficients that we learn, it's generalized for all the data to minimize some of the squared error. For this, we've brought each of the data points, it calculates the different beta, let's say to make it easier. So it kind of learns in context what it wants to aggregate information on. Okay, so now if we want to apply that to the downstream, this is kind of like the basic outline. So we've kind of a fine-tuning model, so this is still in the preliminary stage. We're testing out a lot of things, but the format is that we have a table of data, we have a graph-lit transformation. So for each of the rows, we extract the graph-lit out of it. So for a key number of data, we have a key number of graph-lit, we put it into a G&N with kind of a transformer-based G&N, and then we see how it performs with the downstreams. Okay, so this is the interesting part that we're still kind of experimenting on. So we had the whole big 12 layer, 12 multi-attentions, things like that. But in the end, computationally and performance-wise, we found that we take out the aggregation layer. So the aggregation layer is like the final layer before putting into the contrast of learning layers, contrast of learning loss. It's the aggregation layer where from the center node, it aggregates information on the neighboring ones. So we take that layer out. We also take the first initial MLP, so it just maps from fast-tech servant banks. We take those out. We have it as the pre-train box, so we only have that from the pre-train model. So I know that this can be said to be like, oh, why don't you take the whole model? Take the whole thing. I found that it's computationally slow. We're training, and I guess performed pretty well. And for tables, I guess one thing that I didn't mention is that for tables, when we extract the graphlets, it will be like a star like graph. So the star like graph is that when we have the center nodes, and we don't have any edges between like the other outer nodes. So it looks like a star. So if you have that kind of information, usually the information with multiple layers is not so much, it does not flow through like the edges. So for the graphene network, the information propagates by the defined edges. And for star likes, if you have multiple layers, it doesn't really propagate through multiple edges. So they kind of have like an oversmoaning problem. We have a lot of edges. So having just single aggregation layer seemed to perform pretty well. And we kind of have the initial block for the numerical values. The numerical values here. So I didn't mention before, but on the knowledge graph, there are numerical values so that population and things like that. But let's kind of, well, we're going to have the first we said that we let's just focus on the strings. So we just transformed the numericals to the strings and then used it as if it was a numerical. So for the benchmark data sets, what are things like that? There are other numericals, but treating them as a string didn't really perform well. So we kind of have a different initial block. It's just a simple one there, neural network, that sends each of the features to the embedding size of fastx, so 300. So we just simply have an initial block for the numerical values variables as well. And we just combined that here. So it's it's trainable from the downstream. And then finally, we had the three layer and LP as the classifier layer. So the model itself is not heavy. It's quite simple. And it's also usable by CPU. I mean, not for the pre trained ones, but for the ones what we trained the benchmark, trained on the benchmark data sets, it's trainable also on the CPU. All depending on the train size, but it's for most of the cases, it was trainable on the CPUs. And I guess one other thing is that we do use the in sampling method. So we have we used a banking strategy. So for given one of the model, we have K different train validation splits. So we make let's say 10 different models through 10 different train validation splits. So the validation here is used for the early stopping of the neural networks. So we have 10 different train validation splits. We have 10 different models. And then we kind of combine the outputs, we average the outputs of the 10 different models. So we put a graph that's to the 10 different models that will create 10 different outputs. And then we just average the outputs. We could get more into it, but the capacity gets larger. And so like we could have different weight initialization frameworks with the bagging and things like that, but that gets bit more complicated. It meets more, more number of different models. So we just stuck with the different K difference train balls, you just putts. And the downstreams, we usually use the 30 as the K, the number of different models, just to have the normality, the exception kind of issues thing. But it also choose to be like 10, it's like a user's despite the length. So now for the experiments, so we have, so I for here I have 10 different benchmark data sets. We're testing on a bit more, but we're going to edit later, but I guess there's things to be done with some matching and things like that. So 10 different tabular data sets that usually contains the that always contains the entity names. So for instance, we have like the table for the journal score. We have the journal names as the entity name. And I guess like impact factor, or I guess in some case of H index as the targets. And there are like other metrics that's used as the features. So journal scores, for instance, we also have like revenues for movies with like other information or museums, revenues on American museums. So they have the entity names. So kind of corresponds to the entities within the EBO, within the knowledge graph that we preaching on. We also, well, we're still in the process of using more data sets that does not have the entity names, but those are still ongoing. We're still testing. Yeah, so those are the data sets with these specific entity names. And for the label setting, I guess when we have we use like a five or like let's say 80% train, 20% test. Rating boosting is like it works really well. And I guess since it's learning over the residuals for the grade and boosting models, it learns how to just, it learns pretty well on the tables. So we wanted to focus more on the settings where it's been hard for the grade and boosting models to outperform. So I guess it's limited in the number of labels. So we can combine it to 60 or to 512 to the power of eight, something something like that. So that's I guess within. So the knowledge graph that we trained, we want to extract the background information. I want to see that we actually use the background information from the knowledge graph and then what's to see whether it kind of translates to the benchmark data sets. So it's kind of limited in terms of the end train, number of train sets. We use the 5cv for parameter optimization. So I guess if we have like a thousand data sets, then let's say we use 128 for the train, then we do like a five miles validation over the 128 to get the best parameters. So it's, I know that using cross-validation only on that to get the parameters, it's not journalizable to test it on like a big test set, but it was hard to use other auto-saysian techniques. So you just stop doing, sorry. Yeah. So we kind of stop to the 5cv on the train sets, and that's hard to generalize. And for the performance measure, we use the R2 score for the regression and the AUC for classification. So on the 10 data sets, we have seven data sets on regression and three data sets on classification. And we evaluate it all in 10 different train test bits. So first, divide train test. We used a train for parameter optimization as blah blah blah and tested on the test list test data, which is quite large depending on the data sets. So yeah. This is the, it's kind of like a preliminary result. We're testing with more, but on the left, we have the regression and on the right, we have the classification, and we normalize the score, so that the lowest has zero and the highest has one. So here we have the cart gnn, which is the proposed model, or tended to perform pretty well on those settings, like 64, 128. And the gap between, I guess, the cat boost, or so we have table vectorizer, sorry, for not mentioning the comparing methods, but we use a table vectorizer from the scrubs and the, and soda, which transforms tables into vectors by using some encoding methods. So the table vectorizer is using that with the gradient boosting classifier. So we first transform the original table into vectors and just apply the gradient boosting classifier. Table vectorizer fast text is that we just extract the entities external entities from fast text by putting the entity name into fast text and then kind of just concatenate it. So here we have the proposed model cart gn and performing pretty well. And the gap, yeah, when we do it for like 1000, 2000, and the gap really closes a lot, but you can see that it's the proposed model learns the faster to saturate in the performance compared to the other models. And this is only for three datasets. And here we have the cart fine tuned. What I do is that I train on cart gnn, I just extract the entity embeddings by taking out the classifier and then using gradient boosting on the extracted embedding. So it's just my first string on cart gn and then I take out the classifier, I train a bit more with gradient boosting, but I think for that, there needs to be other schemes. But anyways, in comparison, I guess it performed pretty well. And there are high hopes, I guess, with these results still developing. So hopefully we have a bit more things later. This is just looking at the average rank. So the average rank, of course, it's pretty low for the cart gn and the one below is the critical plots where it does the testings. If there's a difference between the algorithms, statistical difference between the algorithms, and it seems that the proposed model forms pretty well on, let's say, the limited amount of labels. So yeah, those are the big results that we have. Finally, for the conclusion, what we're doing here is like a pre-chain model and the downstream framework that we can use for the tables. So we start from the graph of construction, and then graph of construction of the tables is kind of a new thing that, well, some papers tried, I know that the carto also has that previous worst, that also does it, but a little bit differently. And also, we proposed a transform where we used the graph information inside of it, using both in nodes and edges. One of the things that we use is using the LNY's multiplication. I didn't go into details, but the reason for using that is that one of the knowledge graph embedding methods actually uses the LNY's multiplication. So when they do their things, they move from the translation to enlargement of the vectors and things like that. So there was a motivation for that, but I didn't mention it, so it was good, sorry for that. But I guess using that into the LNY's multiplication and to the transformers was a new thing, and the downstream models is also another new thing that we proposed. And yeah, I also really want to focus the fact that we kind of learned how to utilize the context within the tables. So that's one of the big parts that I guess this work is going second direction that we're going towards using use of the context within the table. For more words, I guess we still need to test on out-of-capacity problems. What I mean by that is that for now, we have so we have some datasets that have entity names within the knowledge graph. So we can kind of compare with the knowledge graph embeddings, but what happens if we kind of make some typos into entity names and things like that so that to see whether the performance is kind of robust to out-of-capillary. Also in the pre-train model, we didn't actually force the out-of-capillary in making depositors or negatives, things like that. So we're kind of trying to make some enrichment data with the out-of-type rows and things like that. And I guess for the second part, we also want to enrich the pre-train model with more data. As I said before, YagoFour, the more recent burden contains a lot more entities with a lot more populations. So that could be of help. And I guess for the pre-train model, we didn't really enforce the tables. So it should be that we should try to put in more tables so that when we look at the downstream, it kind of sees the resemblance of the tables and things like that. So I guess we actually want to collect more tables like wiki tables or get tables and things like that. So we could put more noise into it or to some other magic into it. And I guess the last part is usage on transfer learning. What I mean by transfer learning is that well, we have multiple tables. Let's say we have multiple downstream tables. I guess this is a long shot. So it's not going to be happening at a short time. We have multiple tables of let's say housing prices of Boston, Seattle, things like that. So we have multiple tables with with similar characteristics. Now, there are column names. They're all different. So what I actually want to do is that since we are mapping all the entities within the graph, so we now have we can make this total thing, the whole thing as a data set. Although, like for instance, for table one, it contains entities in Seattle on the table. Second table, we have entities in Boston. Let's say housing in Boston. Well, traditionally, it was a bit hard to kind of map both of the tables into one dimension. I know that there's like kernel betting, kernel meeting betting, things like that to I guess map everything into one. That could be used possibly, but I guess one big thing is that we kind of map everything into the graph where we can inject the graphs into the pre-fair model so that we kind of have like a similar embedding for all the graphics for all the entities from different tables. So I guess in this way, we could have instead of just predicting on one table from one, predicting one on one table, we have predicting one table from multiple sources of tables. So I guess that's for later, but I guess using graphs could actually bring benefit into kind of bringing multiple tables into one other. So that's all for my presentation and thank you for, thank you for how long I've been if you have any questions. Yeah, thanks a lot for the presentation. So because we're getting short on time, I have questions, of course, I don't know if people have questions in the audience or whatever I should start. Okay, I thought maybe people will have and because we want to have much time, I just simple a few questions. So we found this incorrectly, the model is for now mainly between onyago and then Iverges, or fine tune on the task. I've also tried to learn onyago and then just on the task strictly. Yes, so that's not using the pre-sing weights. So that's one of the things that I'm right now, but we're testing on. It seems that there shows a little difference. There is actually a difference between the pre-sing weights and pre-sing weights. So if we randomly select weights, sometimes it reaches that point where it performance pretty good, but it also has the cases where it does not, depending on the weight initialization. So yeah, I have the thing so we'll test later. Yeah, that's a second question. Is it like when you sample the graphlet, which you know like the run and walks, do you only sample them randomly or is very like maybe could you learn them? For instance, you could learn that somehow having small graph, we've given a relationship to push our, not bring a lot of information, so you should not bother having this edge when you learn this. Maybe it's more complex. Just to see the basically my questions like what is the impact of randomly sampling graphlets? Yeah, so I did not have any schematics for sampling things. So I just did a random sampling of the edges. I mean, I'm not completely sure. I think they could. Yeah, that is a good question to answer. Maybe fingers like if you do random sampling and you do it enough, like long enough, and like box it's you got to see everything. So yeah, you don't need to. Yep, so one thing is that I do so in Yago, there's a lot of triples that only contains just one single relation. So those information are not really useful when I'm looking at tables. Since tables each of the column corresponds to the relations, we really want to have kind of like a sample graphlet from the Yago to have at least, I guess, let's say 10 amount of relations. But a lot of entities in Yago do only have one relation which is not quite beneficial, I think. So we kind of have like a framework where we, inside of a batch, we sample I think 80 to 90 percent that has a sample 80 to 90 percent that has entities with more than five or more relations. I don't remember correctly, but I think that's the number. And the other 10 percent is that we sample from the entities with just single relation. So I guess that's one of the schemes that I do, but I think there could be more sophisticated ones to make it better. But we really want to move to that. I think those are really good questions to improve the preaching models as well. I see we have a question who popped up. We'll just read it out loud from Alexy. Speaking of transfer learning, do you think it would be possible to learn a very generic task like ImageNet that can be fine tune afterwards depending on the task? I guess it's similar to what you dream somehow, but you know because you are learning already on the generic task which is Yago and it was like, ImageNet, Yago is your ImageNet in some way, no? Kind of like that, but yeah, it's, I mean, hopefully it's possible. Yeah, yeah, that's a goal, but I'm not completely sure actually. I still have to try out more experiments on that. Yeah. Sorry for not answering the question, but yeah. Yeah, thanks. But it's perfect because we just on time. If there's a final last short question, so that it's not just a description between June and me, people will come to otherwise, we can just thank again June for its time and its presentation. So yeah, so having a list. Thanks again. It was very interesting presentation. It's thanks for taking the time and I think we are looking forward to read the paper, I guess, when we thought that he curious one to get more understanding of the detail. Yeah, thank you so much for everything. Thank you. Yeah, it was interesting. Thanks everybody. Have a have a good day and I will see you all soon. Thank you. Thank you, legend. Bye bye.