 Hello and welcome to the observability trends webinar series. I'm Grant Swanson your host and today we will explore the topic of Generative AI and how to use OLEGPT for advanced observability. We'll kick off with a short presentation followed by a live product demo. Feel free to type any questions into the question window at any time during the webinar. All questions will be answered via email immediately after the session concludes. I also wanted to highlight that we will be conducting a hands-on observability workshop on Tuesday March 12th. Everyone who registered for the webinar will receive a link to the webinar recording and a link to the upcoming workshop. Now I'd like to introduce our guest speaker, Field CTO, Tom Bacheler. Welcome Tom. Thank you, Grant and hello from me, Tom Bacheler, Field CTO here at Observe. In today's session we're going to walk through and observe a Generative AI and how we're leveraging these new technologies to go and help people achieve their observability goals. So let's dive into a couple of things before we get into a quick demonstration of some of these new capabilities. So first of all, what are the goals and what are we going to try to do with some of these new technologies? So let's just look at a couple of things that theories or areas that we've been working on here over the past few months. One of the first things we realized and this happens with any tooling is people quickly want to be able to go and understand how they can achieve a goal. Maybe they want to be able to go and visualize some logs. Maybe they want to go and pull out some metrics, etc. Now, often where people want to do this, especially if they're brand new, logon results using search engines to go see if they can go and kind of find some examples or someone's built some steps or something like that. While we found using GPT, we can go do that for them. We'll go and have a look at how we can just easily get people that step by step information and go and have a look and how they can go and achieve what they're looking to go do. A second thing that we found is people actually often want to help understanding the logs themselves. That could be kind of helping us in unstructured logs. That could be, I want to know what this error message means. So we'll go and take a look at how we can help people go and achieve those goals. And finally, we have a query language. We call it observe processing and analytics language. So we want to be able to go and help people construct those queries and not have to resort to diving through reference documentation to understand exactly how to use different verbs and functions or maybe even switch want to use. So let's take a look at what are some of the specific features that we've added to the product recently. So the first one we have here is Oli help get quick answers to product questions. For example, how do I go and view my container logs? Let's go and get that step by step into people's hands. They don't have to go dive through documentation. Oli explain help people explain logs in plain English. A lot of the motivation for this feature came from what we identify as a common pattern. People will use a login tool. They will go and find maybe a particular error message or something like that. They will take that error message. They will paste that into a search engine. And then they typically end up on kind of some other site where people are discussing what that error message means, how we might go fix that, etc. So as a thought there that we could potentially sure circuit that process and get people some of those answers quickly. What are my personal favorites and you had a lot of excitement for this one when I first saw this work. Oli extract help people build reg X's to pause unstructured logs. This is fantastic both for new and experienced users and as we get into the demo we'll touch a little bit more on why we think kind of some of that is particularly exciting. And then the thought we have here is Oli copilot helping people build queries in the pool. Again, this one is another one that I find super interesting. Just before we dive into the demo, just a little story on this one. So as we'll see as we go through what Oli copilot will do is it will go and suggest actual specific opal queries to go and answer some query wants to run that we've gone and described in plain English. Now, if anyone who's listening has built one of these features it, you know, themselves or use some of these kind of generative AI technologies heavily. We will know that sometimes it can. So it can invent things that that maybe doesn't exist or maybe aren't true. Well, there are a couple of occasions that we're building this where it would go and hallucinate particular functions or verbs that exist in a language. Those things did not exist. So we had some interesting decisions to make when we hit those situations of. Do we now go and refine that model? So if I refine that model, can I make sure we don't kind of it doesn't give that syntax or that function, etc. That doesn't exist. But there were other situations where we look at that and we actually thought to ourselves. We really should go have that in a language. It makes complete sense that the the the the gently AI has hallucinated that and that should exist. So we think to ourselves, OK, that's great. Well, we should maybe go kind of add that in a couple of cases where that's happened, which I thought was kind of pretty interesting. However, we're not all here to go and sit through some slides. I know we want to dive in and see some of these features in action. So I'm going to go ahead and do that. So give me a quick second. So here I have I quickly dive over into observe into our product. And this is really kind of where you'll land if you come. So just for those of you who are not familiar. If we see here, I have. I have some kind of key data sets that we have highlighted here and this is a, you know, in this environment. This is just a really simple demo around. So this is the things that people would typically want to kind of go to first and go and take a look, etc. But let's imagine there's maybe there's some instant maybe maybe I'm a developer and I've released something and someone's letting me know that something wrong or I want to go and have a look at something or something like that. And as such, I may not use this tool and very often, right? It may not be something that I come into today. I might be brand new in the organization. So then I say, OK, I know whatever I released this runs in containers. I want to go have a look at my container logs. How do I go to that? What if I click on the GPT here? I get a chat box and we're all fairly familiar with this and I can put some questions in here. How can I view my container logs? And let's go ask that question. Let's go see what happens. OK, awesome. So it's returned a number of different steps to me. So here we go. So OK, so let's go and have a look at so step one log into observe. Well, we've kind of already covered that. We're here. We're logged in. We're sat in the tool, etc. From the left navigation bar under investigate, click on logs. OK, cool. We can go ahead and do that. OK, number three, in the search log data set field enter container logs and select it from the search results. OK, well, I could do that. Actually, it turns out by default, I have container logs here. So I think we're in a pretty good spot. And this is kind of interesting. So now here I have a whole set of container logs. And I will all I did was just follow some simple steps that the bot generator for. So that was kind of pretty cool. OK, so let's you know, we're in our logs and you know, we'll keep the use case here pretty simple. So today we're we've been to kind of work quite a lot with kind of engine X access. Knockers there, particularly your fancy or advanced actually quite the opposite. Something hopefully many people on this session are familiar with. So we can really kind of go and focus on the features, rather than what goes and sits in the logs and what's happening there, etc. OK, so let's do a couple of things in here. So let's filter on my web server. Unless just filter on standard out. So we just go and have a look at some of these logs here. OK. So this is great. So I have access logs coming from all of my engine X containers. That's awesome. There's a few different things that are less and awesome. So if I kind of double click on one of these logs. So I have this string. You know, it is sort of structured, but it's not really kind of an unstructured log. So you see I have some interesting things in here. I kind of have an IP address. So that kind of stands out. I can see I've got a method. I've got a URL that sits up here. That's great. Then I have OK, so this is probably my response code. I may not know what this number is. And some of the other things, etc. So even in this very simple case. I actually come up with situations where you can kind of glean some information, but other information kind of may not be so hard. Maybe be a little bit harder to go get it. I might actually end up and go off and hit a search engine and kind of ask for the format of these logs or something like that. So I can pull things out, etc. I don't understand a little bit more about what's going on. Well, this is some of the steps that we wanted to short circuit as we build out some of this technology. Right, I don't, you know, that's kind of taken me away from from what I want to achieve is extra steps that sit in there. Now I start to kind of have tap proliferation and more and more things open. I'm trying to synthesize it a lot in my brain. And you know, one of the things that we think about when you know whenever building observability tooling that people using for troubleshooting is, you know, there's going to be quite a lot going on. There's probably a lot to think about. It might be quite a stressful situation. So can we remove kind of some of that load, remove some of the things people need to think about maybe. Just to go and try and make people's lives a little bit easier. Well, in the presentation, I talked about all the explain that that could go and, you know, take some of these messages and help us understand what they mean or maybe help us come out and play English or something like that. Let's go try that with one of these. So I hit this little dot here and I can say explain this message. So this goes off to GPT and goes and ask what this message meant. So here we go. So we can see here. This is, okay, this is in the common log for that. So maybe we did or did not know that as we went and hit this. And now we've got day and time. I can see, okay, response code. Ah, that number where maybe I wasn't quite sure what that was. That's actually my response sites. Okay, I can see the referer user agent, etc. So again, this is breaking this down and. And we can see here that was questions made with Firefox on a Macintosh computer. Okay, I didn't even know how to interpret that. That host that browser strings. So that's pretty interesting that we've gone on got that. So that's got a pretty cool that we can see with a variety of different log messages. How this could be kind of helpful and quickly accelerate just, you know, understanding the meaning of what we're seeing in the data, etc. So that's some problems. So I did this for one message. If I kind of closes window, we can see we kind of have quite a lot of messages. You know, actually I've got 27,000 here in kind of the last 15 minutes. And I might want to go and look at kind of bigger windows of time now. Clearly, I cannot go through 27,000 messages asking it to explain to maybe go and. Get some counts of things go and charts and things, all of that kind of stuff. I need to have a better way of understanding this unstructured law. Now, typical way that we go about this is we would use a regular expression. Now I might sit here and write a regular expression myself. I might go and, you know, go and look one up on the internet and go and hope that that's right and kind of copy paste that in and use that to go pause out this log. Well, here's something that the personally I'm kind of very excited about. So if I could have clicked on this column header and I've got extract them string. And just like traditionally, this is where I could type in a regular expression. Even if you're pretty adept at regular expression, it's going to take some time to go and write that out and. And get that right and put out all the my capture groups and then make sure they're all tight correctly and you know it's not that I couldn't do it. But it's going to take me to you know, amount of time. I spent a lot of my career writing regular expressions and it is. It is not uncommon for me to sit down and start now I don't profess to be amazing at writing them. But it's not uncommon that I would take a lot of my this and I would sit down to go write that regular expression. You know, using some of the regular expression kind of help the tools out there. And again, my head down on my head pops up and 45 minutes have disappeared and now I suddenly have my regular expression. Well, what we can do using using GPT here is I can go and click this extract reg X button. And let's go ahead and click that. So what is going to happen now is this is going to build a reg X for me that I can just go and apply to all of my data. So let's go ahead and fly here and go and see what we get. So we see here we kind of started to build this. And this is the original log line I had. But now I have all of these new columns, right? I have my IP address date and time. Here we go. And now I start to pull out some useful things. I kind of pull down my URL here. I have my response code, my response size, etc. So as we, you know, as I'm sure we're familiar with, you know, that unstructured information. We can kind of eyeball and go through this. If we want to start doing visualizations, representations on this data. Understanding a little bit more around kind of try to pull out, maybe try to pull out the signal from the noise and kind of a sea of logs. We definitely want this data broken out into columns. Because then all of those operations become a little bit easier. And actually, let's do a couple of things in that regard. Let's go and kind of play about with this data now that we have it and see kind of, you know, why we want to use these tools to go and pull this information out. So real quick, let me go and hit visualize here. And I'm trying to change this. I want to go back. I will get the kind of past four hours worth of data rather than a little 50. Okay. So what we built here is. It's just a line chart and it's just showing a count of values right now. And we could have done this with the rule, right? It's just a set of logs and we just go and count them up. But you know, we use the reg X extracts. We had the GPT build the reg X forest. And of course, with observed underlying, you know, schema on demand. So that can go and just we can go add in additional columns into our data on the fly. And then it was us to go and use these things and you know, why is it a little bit more interesting and another little bit better. So in my cut in my expression builder here, over time using can values of all events, but I have no Greek white. Right. So it's just a straight count. Well, now we pulled out some of these things into those columns. For example, status codes. I can go and group these things by status code and then go and run this query and see what's happening. Okay. So first things first, my overwhelming kind of count is is 200s. So that's kind of pretty good. But if I look down here, I have some 404s. I might kind of have some kind of 500s in here, 502s. Oh, that's kind of a 117. Then that's a little bit of spike of a 502s and 503s. So all of these kind of errors go sit in there. Again, kind of schema on demand, doing that extract makes it real easy, not just the visualization, but also to go ahead and apply filters. Now, one of the things that happened is it wasn't just about having the capture groups and pulling out those fields. And that reg X also added in type information. Now, that's important because maybe I just want to care about kind of my 500 and up status codes. I really want to go and just look at those errors. And that's really hard in this because I got some 404s. So maybe someone's accessing something that doesn't exist. A whole bunch of 200s. It's really difficult to kind of go figure out those errors and what's going on. Well, I can have status code in here. We see that has a type of in 64 that was embedded in the reg X that got generated. I can say, great. Then equal to 500. So just filter down to those errors that we have in my data. And again, we're going to run this. Okay. Now I have a very different chart. So I kind of get these sporadic kind of 502s. I've got some 503s. This is really what I'm seeing here. So now it looks like I've got down into data that's showing me some things that aren't going right right. And really is maybe where I want to focus my time and focus on my efforts and. Well, now we go down to that data. We probably have a lot less. So let's go and have a look. They didn't go and see what's going on. So we'll click back into the logs. We keep all of the same filters that we had for the visualizations. But we had thousands of thousands of log lines. And now we have just 1,380. So an awful lot less. So here we go. And I see my kind of smattering of error codes, etc. Now, let's say I want to get into. I want to look at some summary information. I want to know like I've got a whole set of URLs here. Like which endpoints on my application are kind of seeing these errors? Is it across the board? Is it a smattering of a few? You know, even with we reduce the data set a lot. But we're still dealing with over a thousand rows. And just by building that is going to be hard to kind of figure out where that impact is what's going on, et cetera. Now this is where kind of, you know, the final thing of, you know, we went through kind of the colby bot. We sort of had a look at the oh they explain. We looked at the extract. And now let's get into co pilot and see how we might leverage that and to see what's going on. So we focused all the work we've done. And really kind of been with this build us so far where we've leveraged the UI to go and apply some filters. and change between visualization logs, et cetera. But sometimes we just want to step down and get into the query. So that's where I go and hit it open. And we see a couple of things here already. So remember a few minutes ago, I kind of filtered on a web server and standard out. Well, this is the opal for those filters. When I did my all the extract, this is the reggae that it generated. So yeah, imagine trying to go and create this in the middle of an incident. Much better to have something I can just go and have generating that can be going my day. And of course here we have my status code filter. Now our question when we came in here was, which endpoints were affected and how much of those things. So I probably want to kind of get a count where URLs. So let me just add a comment here, count by URLs. So I'm going to hit a button here. We'll see this copie that will flash up. So we'll go ahead and hit that. And it's thinking and now it's generating. Some opal point. And this is the correct opal. This is a stat by and it's going to count the URL column and group by URLs. And I did not have to know how stat by works. Let's go ahead and execute this. Okay, fantastic. So now I have all of these endpoints. And I can see kind of yeah, really filter down on the errors. So I can see a count of each endpoint by errors. But this is still a little bit awkward, right? It's hard for me to tell what's most impacted, what's least impacted, et cetera. Well, let's try this again here. I am going to keep this super simple. I'm just going to say sorts. I'm not going to give it kind of any more description other than that. Just I want these things sorted. Can you please go and figure this out? So we will hit the copie button. And here we go. Okay, so we have the sort verb. That's great. Descending. That's probably what I want. Fantastic. Let's see what happens when we run this. Okay, this is amazing. So now I can see that this manufacturer endpoint, the fall is probably a parameter in here. It looks like that's kind of the most impacted. And I'm starting to answer some of those questions and go and get into this. So that's a lot I wanted to run through today. So thank you very much for your attention. But yeah, we see we hit on using the Ollibod to go and get guidance over how to go and achieve a goal in a product. We looked at explain message to go take a log line and figure out what that means in plain English. We use all the extract to go and extract data from unstructured into structured logs and using schema on demand. And then of course, we've just seen how we've used the copilot. To go and help us go and write open queries. The other thing we found with the copilot is is not just for new users. A lot of experienced users are using this and leveraging this as well. Simply because it just saves time. I know what I want to do. I can go and tell the tool what I want to achieve and it will just go and generate that open for me. I can get my results and I can move on with my day. So thank you very much everyone. This has been a great session. I've been Tom Bachelor. Have a great day.