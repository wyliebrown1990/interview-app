 Hi everyone, welcome to Building AI Solutions with Trust by Design featuring Conor Jensen and Oz Kron. We're super excited that you've all joined us today. Just one quick housekeeping note is that a recording of the session will be available to you after. So I'm going to go ahead and hand it on over to Conor and Oz. Morning, everybody. Good afternoon, depending on where you're joining from. We'll do a couple quick introductions that will not change. So for thank you, everybody for joining. My name is Conor Jensen. I am the Global Field TDO at DataIQ. We'll get into that a little bit. If you're not familiar with DataIQ, we are a Data Science and Analytics Development platform. We'll talk a little bit about that here in the future. Been here for five years. Let our Data Science team, let our field, CDO team, for the last couple of years we work with our customers as advisors on strategic topics and how they're working about building AI products. Excited to be here today. Oz, you'll introduce yourself as well. Yes, thank you, Conor. And good morning. Good afternoon, everyone, wherever you are in the world. My name is Oz Kron. I'm a partner with our risk and financial advisor practice at Deloitte. I lead our trust for the AI services and market offering. So in that capacity, thinking about all the risk, governance, and controls, considerations associated with these emerging technologies like AI and generic AI, and very excited for the session here in the next 30 minutes. Awesome. Looking forward to it. All right. Well, let's dive in. So what we're going to do today, I'm going to talk a little bit first around some of the broader generative AI trends and things that we're seeing, and especially how that's contributing to the conversation around trust. As that is evolving right now, Oz is going to give a little bit of a perspective, especially digging into the trusted pieces like that. And then we're going to do a little bit of a fireside chat going back and forth. So diving right in at the start here, the first thing that we're seeing as a conversation that's really changed going into this year, coming out of last year, is really the conversation around the compute cost volatility and leveraging GPUs and the advantage, but the struggle that's going along with that. So for those of you who are unfamiliar, just in case GPU, originally it's a graphic processor unit. So originally you would have in within your computer, you still have a CPU, which does all of your compute, running things like your OS and your applications. But you're on a GPU that was managing the graphics and what was happening on the screen, and those were developed in a different way to deal with the load of computing was going to happen from a graphical perspective. What's turned out over the last 10 years is that GPUs are also really good for handling machine learning, deep learning AI computations, especially now over the last five or six years as people have moved into neural nets, and one flavor of neural nets that we're hearing a lot about right now, LLMs, is that these GPUs are especially efficient handling the large text type data sets for the text generation. But why is this causing volatility? Well GPUs have been becoming more influential, probably it's a really over the last five to six years, where you've sort of started to see them as mainstream or at least sort of like a clear secondary usage in the AI and ML space in developing these AI applications, and that parallel processing that they're able to do is crucial for doing these large-linguages models in generative AI. Well, of course, there's been a huge embrace of the foundation models and the other types of LLMs in generative AI models over the last year, especially where the demand for the compute processing power these GPUs can provide, especially hosted by the public clouds, is really outstripping the supply side of things. And so, this surging demand, especially driven by the adoption of generative AI, with uncertain supply, right? The Semiconductor supply chain has had a lot of turbulence over the last few years. That's pretty much all supply chains have, but especially with, you know, there are some choke points here in the Semiconductor for the specific types of chips necessary for GPUs. And then of course, you know, we are seeing supply chain impacts on this from global uncertainties. And so, all this is combined to really sort of drive a different conversation around what you can trust and use and leverage from this. And actually, like, now we're really starting to see conversations of people moving back to hybrid architectures thinking about building on prem supplies of their own, trying to control this themselves. Another dynamic of this is that as people are leveraging these generative AI models more and more, most of this is happening currently today via the commercial API driven services like OpenAI, CoHear, Anthropic, and then of course, the cloud-provided ones like bedrock or vertex. And so, as those are really becoming more and more leverage, we're really starting to see even sometimes a lack of availability and an increase in costs of those. And so, people are looking now actually to go back a little bit the direction away from the cloud first option into thinking about building their own on prem architectures a little bit. So, as people who are pursuing generative AI more and more this thought around how do I control for these costs and this uncertainty right now or on GPUs as a really important issue from a business trust perspective to be able to trust that you can leverage these technologies at scale. But secondly, the other piece that's happening right now is it's sort of all the conversation right now feels like we're talking about large language models. But we're really fit in. You know, it kind of today feels a little bit like generative AI and LLMs is everything you should be doing from a AI perspective right. This should be your focus we're having conversations with our customers or prospects around what is your genAI strategy right. And it sort of feels like this is a brand new thing that is completely dominant part of the ecosystem. But really, the generative AI and the LLMs are a piece of the puzzle but they're just one piece of a larger puzzle where we have AI, we have machine learning, we have neural networks, we have NLP which is feeds into this computer vision also part of the generative AI set deep learning the type of architectures neural networks and everything. And then we have generative AI and LLMs. And so all these are fitting together and there's a lot of sort of overlap and distinction between these. But I think it's really important for us to keep in mind that we need to be thinking about these generative AI tools while they are bringing new capabilities to the fore that they are part of our overall AI and data science landscape and then we need to be like thinking of them in this broader overall architecture. And the third piece that I think is really coming and has has hit home in the last, you know, I'd say maybe like three to four months as people have moved out, I guess the early movers have moved out of the prototype and POC phase and to really put against some generative AI models into production is how do we manage these LLMs, right? And so the couple of things that I think are important to note here and it's a lot on the slide, but just sort of thinking about how do we pull apart the LLM itself and the AI service layers and really be able to think about things like auditing, PII detection, content moderation, cost tracking and divorcing that from the LLM itself, right? As we think about being able to be more modular, those sort of service and ancillary layers around that are becoming really important. And then of course, security, securitization, auditing, ensuring safe use, controlling who can access which LLM with which data, how are they doing that? This is a really important and I can say struggling challenge here, especially because the external nature of leveraging a lot of the Gen AI tools, it's different than what we've mostly been doing, which has been managing and maintaining our own AI models locally for the vast majority of the use cases. So it's changing a little bit the paradigm in which we're working in how much we're having to go and think about managing AI products in a different way. And to actually sort of dive straight into that, I'm going to hand it over to Oz here to start to talk about what some of that can look like on the management side. Great, thank you, Connor. So as Connor mentioned, the rate of technology advancement is now increasing faster than perhaps humanity, human ability to adapt it and not only a pace of change has changed, but the volume of data is increasing rapidly as well. So in the conversations that I usually participate in in my role, I experience two different types of organizations. One in the kind of the path of fear, that's dominated by headlines that AI is going to take over the world and it's going to challenge humanity and it's coming for us all and it's one that is paralyzing because of that fear. And then you have the other end of the spectrum, the path of idealism, it's the part where we run headlong into the possibility of a future that's really bright around these emerging technologies. It's going to cure cancer, it's going to solve world problems and make the world a better place and so forth and so on. So under both of these scenarios, regardless of who you talk in an organization, regardless of the role in the organization, one word that constantly comes up in conversations more than any other word is trust. And what does it mean to have trust in the AI that we're going to kind of live with? So we've decided that we don't want to be just talking about it, about the word trust, but we actually measured it. So our trust ID, Generative AI study has actually revealed that if your organization is experimenting with some of these emerging technologies, that lowers the level of trust. In certain cases, the consumers that know that the brand is using AI is 144% less likely to trust that brand. The perception of reliability for that brand that decreases one has about 157%. And if you think about the role organization, the employee trust by the employers in that organization also decreases by 139%. So despite the kind of this widespread skepticism around these emerging technologies, the field is rapidly expanding and growing, and but it also requires the companies to adopt new, find new ways to really grow that trust when AI is utilized by their organizations. If you can go to the next slide, please. So we talked about AI, Generative AI, Machine Learning, Deep Learning, all of this stuff, but how do we kind of think about the erosion of trust with these emerging technologies? I think we all know that there are some existing risks from AI, whether that's bias, that some of these AI algorithms trained on bias data can perpetuate discrimination areas like hiring or loan approvals or even criminal justice. So identifying and mitigating bias is crucial to ensure fairness and equal opportunity when it comes to these emerging technologies. Privacy is another one. Some of these systems that collect and analyze rest amounts of personal data really raise privacy concerns. So balancing innovation with data protection becomes extremely essential. Transparency and explainability, some of these complex emerging technologies can make uphake decisions like of transparent and makes it difficult to understand their reasoning and raising concerns about accountability and fairness. And then in search of Generative AI in our lives, right? In the last two years or so. Now we're talking about deep-fakes, manipulated videos or audio, which can incredibly be convincing and blurring the lines between truth and friction and poses another major risk for spreading misinformation and propaganda and eroding trust in society. Some of the unpredictable and unreliable outputs, ensuring that the quality and reliability of Generative AI outputs can be challenging, models can produce harmful and offensive in-ector content if not carefully controlled or monitored. So these are some of the risks that we are really dealing with in today's environment. And the big question becomes, what can organizations really do, what steps they can take to mitigate these risks and really build that trust with their employees, with their customers and with various different stakeholders. What you're seeing on the right side of the slide is some of these different risk mitigation techniques, whether that's establishing a comprehensive air-respengagement framework, having the right policy and procedures in place, really thinking about the different risks within the AI lifecycle or from design to development to actual training in use case and so forth and so on. So with that, maybe there's a third path. It doesn't have to be the path of fear and path of idealism and maybe there's a third path. And if you can go to next slide please and that's really the path of being able to have the right mechanisms in place to be able to instill more trust in these machines and more trust with humans related to these machines and related to these emerging technologies. So one of those items is having a comprehensive AI risk management principles that would serve as the cornerstone of your AI journey for your organization. What you're seeing on this slide is our trustworthy AI framework that we have developed at Deloitte, that we don't not only deploy with our own internal machines and models, but we also bring this to our clients as well. There's a lot of different different frameworks out there that you can also utilize. National Institute of Science and Technology is their own framework. Some of the big tech organizations have come up to their own framework, but what you really see is that consistently across these are some of the principles that those frameworks truly hone in on whether that's the safety and security, how the AI systems can be protected from some of these risks that we can talk about. Accountability, what kind of policies that an organization needs to have in place to determine who's responsible for that decisions that are being made and derived through the use of these emerging technologies. Transprancipation explainability we talked about, how do park spins are able to really understand how data is being used and how these emerging technologies are making decisions through the use of that data. So one of the key advice that we give our clients is that really start with understanding what are those key framework components that you want to instill in your organization that's going to help you build that trust. And if you go to the next slide please, one of the things that we really honed in on the article that we published along with data IP and opportunity to build trust is some of these items that outlines what are some of the key things that organizations really think about. As more and more use cases we see related to these emerging technologies we also see an increasing number of regulations. So adhering to those regulations really understanding the organization's obligations in meeting those regulatory requirements related to use of AI and generative AI becomes extremely important and extremely challenging. Establishing controls, what are the different types of risks that you really need to control for again through the entire life cycle of these emerging technologies and being able to build your control environment that's going to be able to mitigate towards those risks it becomes extremely critical. Communication stakeholders I cannot highlight the importance this enough. We see a lot of organizations out there I think that this is just a challenge of the technology organizations or data organizations but really becomes a cross organizational challenge and communication with those stakeholders across the organization becomes extremely important and obviously the last one is the trust element. How do you excuse me? How do you ensure that you know you can still additional transparency explain ability to the process to instill that trust with your various different stakeholders that part spade in New York. So with that Connor maybe let me turn it over to you with that question. One of one of the key questions that I get in my role is really thinking about who in the organization is responsible for the story of AI generative AI of these emerging technologies. What do you see in your conversations in the organizations that you speak with who do you usually see in the organization is really responsible for the AI agenda? I'll start with saying that there's unfortunately not consistency yet in terms of how companies are addressing it but I think that that's part of that's reflected in just sort of the maturity of our organizations and what we've seen and say sort of seen anecdotally but then born out a lot in the survey that we did with Databricks last year and some other recent findings is that the companies that are being the most successful with building this trust building out and leveraging these things have somebody on the C suite whose job it is to manage their data and AI initiatives. And I think that that that role is still sort of evolving and shaping. Some companies have just a chief data officer, some companies have a chief data and analytics officer, some have a chief analytics officer, chief data officer and a chief data analytics officer. And so it's like there's definitely still sort of a we're fumbling a little bit to what that right model looks like but regardless of name the thing that is really key is that you know I in a conversation with a friend a few years ago he made a I'll say a snide but like truthful remark and that a CDO is a fake chief executive right was a fake CXO because a lot of times in that organization you know a CDO is under the CIO who was under the CIO or even further down right and so you know having a CDO or some analog to it regardless of what you're calling it is important but if that's somebody who's three or four layers down in the organization you know then you're they're going to get as much attention to this as that position sort of warrants right and so I think that that's the first in the foremost is you do you have somebody who's reporting to the CEO or maybe at most is you know one step away under a COO or you know that sort of organizational hierarchy whose job it is first you know when they wake up in the morning and they go to bed at night and then every moment in between that AI and data is the focus of what they're doing that's that's the first in the foremost one but the thing that I don't want that answer to give the impression of is that everybody else gets a pre-pass right that this isn't this if you're serious as an organization with making AI and data and and sort of like the broader you know that we're talking about here if you're serious about making this part of what you do as a company it is everybody's responsibility to be part of that and I mean everybody from the person who is sitting in a call center typing data in what they put into those fields when they're on the phone with the customer whatever will flow through to every model and every decision that comes down string from that their managers are they thinking about that are they leveraging that when they're training people are they helping them understand the importance of the data that they put in your mid managers were sort of all good initiatives either succeed or fail right like are they bought in do they understand the importance of what their teams are doing how they're contributing are they messaging down when they're hiring new salespeople are they looking for salespeople who have experience in working with data and working with you know leads scoring models or other sales like if this is again it I'm not saying that every company should make AI a core part of their strategy right like you have to decide as a company right like you can only be sort of one thing centric right some companies are customer centric something companies are product centric some companies are AI centric right so you have to decide how important this is and what level of initiative you're going to give it into it the first piece is that there's somebody whose job it is that has the like organizational wide agreement and authority to make good on it but that they're then enabled by every other person and every layer sat up down sideways to make sure that they're able to do that so yeah that's the it's the Yen in the Yang right like somebody has to have that sort of central authority but also everybody else has to be a part of it I've also seen the organizations that really experiment in any capacity with AI that forms some type of a steering committee type of a a government structure that has representation from different parts of the organization becoming very successful at this you know there should be a seed at the table for HR as much as there's a seed at the table for technology or legal or data because what you're really trying to to do is bring that collective mindset to the table to say different use cases the organization may be experimenting with the governance protocols that they may be thinking about establishing policies procedures that they're thinking about establishing should really be reviewed and thought about with someone in each of those organizations to bring that mindset to the table and in my experience those are the organizations that really do this in a right way and be able to really control for those risks as opposed to organizations that think that this is just a really to only should be on the top of the agenda for technology or data or or some of those organizations that really deal with the AI and generative AI and then the last thing I will add is I've also seen a really a proliferation of the role AI data and ethics officer you know ethics component is really being built into some of these roles whether it's with the CDO or a new role being created in organization because that governance and ethics component becomes a huge challenge for those organizations that are experimenting with AI and generative AI or any of these emerging technologies. One of the things I think is really interesting especially about this ethics angle here is that as we start in a much more explicit way than we have in the past encoding historical decisions into data and then automating them or at least like making them form or transparent be AI I think it's forcing organizations to examine their ethics in a way that they're they're have not always been I don't want to say they're never has been but not they're rarely has been something that like makes it so tangible as when you sit down and you look at an HR is always a you know a good one that you know sort of comes up these conversations when you sit down and you want to like look at how do we improve our you know DEI initiatives sitting down then actually sort of looking at the numbers looking at the stuff is a good first start that organizations haven't always been forced to sort of reckon with and once you start saying hey we think that there's an AI or a data you know product that we can build here the first thing you do is you look at what the data is telling you historically and then that sort of makes organizations come to a like a much more concrete statement about their ethics then was really historically sort of like implicit not necessarily like explicit right and so you talked a lot about the trust and so that's I think that this is a like a really natural sort of thing then it like there's one organization chooses to do what it sits down and looks at like what are my what are our we'll call our ethical decisions as an organization historically but then of course we have the EU and the new EU AI act we have the same in the came out from the White House last year and assuming the more regulation coming in the United States beyond the and you know obviously you support the financial services sector which is certainly pretty regulated in some aspects of what it does and you know likewise in healthcare and there are other regulated highly regulated industries but like how do you see that evolution of this regulatory discussion forcing more ethical decision like what does that look like to you yeah great question you know one one thing that's pretty apparent I think over the last few months here is that we are seeing an increasing number of regulations coming both at the as at the local state as well as federal level both in the US and globally so what I see in the US first of all is that we're actually probably going to see more and more at the local and state level than maybe we do at the federal level you know obviously the White House came up with the AI Bill of Rights and the Executive Order and all that stuff but what we are seeing actually is some tangible local regulations like we see in the city of New York if you are using you know models to for hiring decisions now those models needs to be audited and reviewed on on an annual basis or in the state of Colorado if you're life ensuring the state of Colorado you have certain obligations to the to the state of Colorado now that you have to comply with as well so we are seeing more tangible stuff at the maybe the more mid to local and state level than we do on the federal level in the US but if you turn the lens to global then you're seeing the EU AI Act that's very tangible in EU and has a lot of different requirements we're seeing stuff happening in Japan in Brazil in China so you know obviously I don't have a crystal wall and the type of industry that you operate in is going to is going to have a different answer for this question perhaps but what I suspect is that we're going to see an increasing number of regulations going forward at both local state and and federal level across the globe and the more of these regulations come to play there's going to be more obligations for organizations to really comply with and that kind of the regulatory change management process related to these emerging technologies is going to become more and more critical for chief compliance officers and chief risk officers and and so forth and so on so that's that's where I think where we're going but obviously who would have known that we would be here where we are today 18 months ago well and I think it goes that last point especially goes right back what you were saying around the having the steering communities and that like sort of you know we certainly see that the companies that are being the most successful are doing use cases across the organization in a wider number of parts of their right because it's bringing more people to the table sort of fostering that culture of innovation but then like wider also means harder to manage and so it isn't an interesting sort of like push and pull of you know you want to be doing it in you know in your workforce management in HR you want to be doing it in operations you want to be doing it in you know sales and go to market and like all these different parts of the business that can benefit but then like how do you balance right now the like let's move with if we get out of our skis and then regulation sort of like cuts out from underneath and it's like I get I get that sort of tension from a company but you know I think that there's you know the typically we try to sort of like think from a regulation perspective but you know there's plenty of influence from the business side there to say like how do we do this intelligently and so I think that you know we should be looking to regulation to help make it clear for the bounds of what people can and should be doing but with not at the not at the cost right now of waiting right and so not be the sort of the piece that I would you know those out there listening like you know the more you work towards it right now the better a position you'll be in when the regulations come at that local or state level because you'll know what you're doing you know what you're working on you'll know you'll have your arms wrapped around this situation so also why not a quick conversation us but you know last and a great way to start my Monday here so I really hope that everybody else has enjoyed this as much as I have. See me. All right well thank you everybody